diff --git a/app/build.gradle b/app/build.gradle
index f2b11d3..c293b5f 100644
--- a/app/build.gradle
+++ b/app/build.gradle
@@ -2,7 +2,7 @@ apply plugin: 'com.android.application'
 
 android {
     compileSdkVersion 22
-    buildToolsVersion "22.0.0"
+    buildToolsVersion "22.0.1"
 
     defaultConfig {
         applicationId "fr.pchab.androidrtc"
diff --git a/librtc/.gitignore b/librtc/.gitignore
new file mode 100644
index 0000000..796b96d
--- /dev/null
+++ b/librtc/.gitignore
@@ -0,0 +1 @@
+/build
diff --git a/librtc/build.gradle b/librtc/build.gradle
new file mode 100644
index 0000000..6a05425
--- /dev/null
+++ b/librtc/build.gradle
@@ -0,0 +1,23 @@
+apply plugin: 'com.android.library'
+
+android {
+    compileSdkVersion 22
+    buildToolsVersion "22.0.1"
+
+    defaultConfig {
+        minSdkVersion 10
+        targetSdkVersion 22
+        versionCode 1
+        versionName "1.0"
+    }
+    buildTypes {
+        release {
+            minifyEnabled false
+            proguardFiles getDefaultProguardFile('proguard-android.txt'), 'proguard-rules.pro'
+        }
+    }
+}
+
+dependencies {
+    //compile fileTree(dir: 'libs', include: ['*.jar'])
+}
diff --git a/librtc/libs/libjingle_peerconnection.jar b/librtc/libs/libjingle_peerconnection.jar
new file mode 100644
index 0000000..e080892
Binary files /dev/null and b/librtc/libs/libjingle_peerconnection.jar differ
diff --git a/librtc/proguard-rules.pro b/librtc/proguard-rules.pro
new file mode 100644
index 0000000..93d503b
--- /dev/null
+++ b/librtc/proguard-rules.pro
@@ -0,0 +1,17 @@
+# Add project specific ProGuard rules here.
+# By default, the flags in this file are appended to flags specified
+# in /home/gardev/android/sdk/tools/proguard/proguard-android.txt
+# You can edit the include path and order by changing the proguardFiles
+# directive in build.gradle.
+#
+# For more details, see
+#   http://developer.android.com/guide/developing/tools/proguard.html
+
+# Add any project specific keep options here:
+
+# If your project uses WebView with JS, uncomment the following
+# and specify the fully qualified class name to the JavaScript interface
+# class:
+#-keepclassmembers class fqcn.of.javascript.interface.for.webview {
+#   public *;
+#}
diff --git a/librtc/src/androidTest/java/com/garena/android/librtc/ApplicationTest.java b/librtc/src/androidTest/java/com/garena/android/librtc/ApplicationTest.java
new file mode 100644
index 0000000..45069b5
--- /dev/null
+++ b/librtc/src/androidTest/java/com/garena/android/librtc/ApplicationTest.java
@@ -0,0 +1,13 @@
+package com.garena.android.librtc;
+
+import android.app.Application;
+import android.test.ApplicationTestCase;
+
+/**
+ * <a href="http://d.android.com/tools/testing/testing_android.html">Testing Fundamentals</a>
+ */
+public class ApplicationTest extends ApplicationTestCase<Application> {
+    public ApplicationTest() {
+        super(Application.class);
+    }
+}
\ No newline at end of file
diff --git a/librtc/src/main/AndroidManifest.xml b/librtc/src/main/AndroidManifest.xml
new file mode 100644
index 0000000..23dea4c
--- /dev/null
+++ b/librtc/src/main/AndroidManifest.xml
@@ -0,0 +1,10 @@
+<manifest xmlns:android="http://schemas.android.com/apk/res/android"
+          package="com.garena.android.librtc">
+
+    <application android:allowBackup="true"
+                 android:label="@string/app_name"
+        >
+
+    </application>
+
+</manifest>
diff --git a/librtc/src/main/java/org/webrtc/AudioSource.java b/librtc/src/main/java/org/webrtc/AudioSource.java
new file mode 100644
index 0000000..06177a6
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/AudioSource.java
@@ -0,0 +1,38 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/**
+ * Java wrapper for a C++ AudioSourceInterface.  Used as the source for one or
+ * more {@code AudioTrack} objects.
+ */
+public class AudioSource extends MediaSource {
+  public AudioSource(long nativeSource) {
+    super(nativeSource);
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/AudioTrack.java b/librtc/src/main/java/org/webrtc/AudioTrack.java
new file mode 100644
index 0000000..3200080
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/AudioTrack.java
@@ -0,0 +1,35 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/** Java wrapper for a C++ AudioTrackInterface */
+public class AudioTrack extends MediaStreamTrack {
+  public AudioTrack(long nativeTrack) {
+    super(nativeTrack);
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/DataChannel.java b/librtc/src/main/java/org/webrtc/DataChannel.java
new file mode 100644
index 0000000..1866098
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/DataChannel.java
@@ -0,0 +1,143 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.nio.ByteBuffer;
+
+/** Java wrapper for a C++ DataChannelInterface. */
+public class DataChannel {
+  /** Java wrapper for WebIDL RTCDataChannel. */
+  public static class Init {
+    public boolean ordered = true;
+    // Optional unsigned short in WebIDL, -1 means unspecified.
+    public int maxRetransmitTimeMs = -1;
+    // Optional unsigned short in WebIDL, -1 means unspecified.
+    public int maxRetransmits = -1;
+    public String protocol = "";
+    public boolean negotiated = false;
+    // Optional unsigned short in WebIDL, -1 means unspecified.
+    public int id = -1;
+
+    public Init() {}
+
+    // Called only by native code.
+    private Init(
+        boolean ordered, int maxRetransmitTimeMs, int maxRetransmits,
+        String protocol, boolean negotiated, int id) {
+      this.ordered = ordered;
+      this.maxRetransmitTimeMs = maxRetransmitTimeMs;
+      this.maxRetransmits = maxRetransmits;
+      this.protocol = protocol;
+      this.negotiated = negotiated;
+      this.id = id;
+    }
+  }
+
+  /** Java version of C++ DataBuffer.  The atom of data in a DataChannel. */
+  public static class Buffer {
+    /** The underlying data. */
+    public final ByteBuffer data;
+
+    /**
+     * Indicates whether |data| contains UTF-8 text or "binary data"
+     * (i.e. anything else).
+     */
+    public final boolean binary;
+
+    public Buffer(ByteBuffer data, boolean binary) {
+      this.data = data;
+      this.binary = binary;
+    }
+  }
+
+  /** Java version of C++ DataChannelObserver. */
+  public interface Observer {
+    /** The data channel's bufferedAmount has changed. */
+    public void onBufferedAmountChange(long previousAmount);
+    /** The data channel state has changed. */
+    public void onStateChange();
+    /**
+     * A data buffer was successfully received.  NOTE: |buffer.data| will be
+     * freed once this function returns so callers who want to use the data
+     * asynchronously must make sure to copy it first.
+     */
+    public void onMessage(Buffer buffer);
+  }
+
+  /** Keep in sync with DataChannelInterface::DataState. */
+  public enum State { CONNECTING, OPEN, CLOSING, CLOSED };
+
+  private final long nativeDataChannel;
+  private long nativeObserver;
+
+  public DataChannel(long nativeDataChannel) {
+    this.nativeDataChannel = nativeDataChannel;
+  }
+
+  /** Register |observer|, replacing any previously-registered observer. */
+  public void registerObserver(Observer observer) {
+    if (nativeObserver != 0) {
+      unregisterObserverNative(nativeObserver);
+    }
+    nativeObserver = registerObserverNative(observer);
+  }
+  private native long registerObserverNative(Observer observer);
+
+  /** Unregister the (only) observer. */
+  public void unregisterObserver() {
+    unregisterObserverNative(nativeObserver);
+  }
+  private native void unregisterObserverNative(long nativeObserver);
+
+  public native String label();
+
+  public native State state();
+
+  /**
+   * Return the number of bytes of application data (UTF-8 text and binary data)
+   * that have been queued using SendBuffer but have not yet been transmitted
+   * to the network.
+   */
+  public native long bufferedAmount();
+
+  /** Close the channel. */
+  public native void close();
+
+  /** Send |data| to the remote peer; return success. */
+  public boolean send(Buffer buffer) {
+    // TODO(fischman): this could be cleverer about avoiding copies if the
+    // ByteBuffer is direct and/or is backed by an array.
+    byte[] data = new byte[buffer.data.remaining()];
+    buffer.data.get(data);
+    return sendNative(data, buffer.binary);
+  }
+  private native boolean sendNative(byte[] data, boolean binary);
+
+  /** Dispose of native resources attached to this channel. */
+  public native void dispose();
+};
diff --git a/librtc/src/main/java/org/webrtc/EglBase.java b/librtc/src/main/java/org/webrtc/EglBase.java
new file mode 100644
index 0000000..3e7020d
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/EglBase.java
@@ -0,0 +1,237 @@
+/*
+ * libjingle
+ * Copyright 2015 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import android.opengl.EGL14;
+import android.opengl.EGLConfig;
+import android.opengl.EGLContext;
+import android.opengl.EGLDisplay;
+import android.opengl.EGLSurface;
+import android.util.Log;
+import android.view.Surface;
+
+/**
+ * Holds EGL state and utility methods for handling an EGLContext, an EGLDisplay, and an EGLSurface.
+ */
+public final class EglBase {
+  private static final String TAG = "EglBase";
+  private static final int EGL14_SDK_VERSION = android.os.Build.VERSION_CODES.JELLY_BEAN_MR1;
+  private static final int CURRENT_SDK_VERSION = android.os.Build.VERSION.SDK_INT;
+  // Android-specific extension.
+  private static final int EGL_RECORDABLE_ANDROID = 0x3142;
+
+  private EGLContext eglContext;
+  private ConfigType configType;
+  private EGLConfig eglConfig;
+  private EGLDisplay eglDisplay;
+  private EGLSurface eglSurface = EGL14.EGL_NO_SURFACE;
+
+  public static boolean isEGL14Supported() {
+    Log.d(TAG, "SDK version: " + CURRENT_SDK_VERSION);
+    return (CURRENT_SDK_VERSION >= EGL14_SDK_VERSION);
+  }
+
+  // EGLConfig constructor type. Influences eglChooseConfig arguments.
+  public static enum ConfigType {
+    // No special parameters.
+    PLAIN,
+    // Configures with EGL_SURFACE_TYPE = EGL_PBUFFER_BIT.
+    PIXEL_BUFFER,
+    // Configures with EGL_RECORDABLE_ANDROID = 1.
+    // Discourages EGL from using pixel formats that cannot efficiently be
+    // converted to something usable by the video encoder.
+    RECORDABLE
+  }
+
+  // Create root context without any EGLSurface or parent EGLContext. This can be used for branching
+  // new contexts that share data.
+  public EglBase() {
+    this(EGL14.EGL_NO_CONTEXT, ConfigType.PLAIN);
+  }
+
+  // Create a new context with the specified config type, sharing data with sharedContext.
+  public EglBase(EGLContext sharedContext, ConfigType configType) {
+    this.configType = configType;
+    eglDisplay = getEglDisplay();
+    eglConfig = getEglConfig(eglDisplay, configType);
+    eglContext = createEglContext(sharedContext, eglDisplay, eglConfig);
+  }
+
+  // Create EGLSurface from the Android Surface.
+  public void createSurface(Surface surface) {
+    checkIsNotReleased();
+    if (configType == ConfigType.PIXEL_BUFFER) {
+      Log.w(TAG, "This EGL context is configured for PIXEL_BUFFER, but uses regular Surface");
+    }
+    if (eglSurface != EGL14.EGL_NO_SURFACE) {
+      throw new RuntimeException("Already has an EGLSurface");
+    }
+    int[] surfaceAttribs = {EGL14.EGL_NONE};
+    eglSurface = EGL14.eglCreateWindowSurface(eglDisplay, eglConfig, surface, surfaceAttribs, 0);
+    if (eglSurface == EGL14.EGL_NO_SURFACE) {
+      throw new RuntimeException("Failed to create window surface");
+    }
+  }
+
+  // Create dummy 1x1 pixel buffer surface so the context can be made current.
+  public void createDummyPbufferSurface() {
+    createPbufferSurface(1, 1);
+  }
+
+  public void createPbufferSurface(int width, int height) {
+    checkIsNotReleased();
+    if (configType != ConfigType.PIXEL_BUFFER) {
+      throw new RuntimeException(
+          "This EGL context is not configured to use a pixel buffer: " + configType);
+    }
+    if (eglSurface != EGL14.EGL_NO_SURFACE) {
+      throw new RuntimeException("Already has an EGLSurface");
+    }
+    int[] surfaceAttribs = {EGL14.EGL_WIDTH, width, EGL14.EGL_HEIGHT, height, EGL14.EGL_NONE};
+    eglSurface = EGL14.eglCreatePbufferSurface(eglDisplay, eglConfig, surfaceAttribs, 0);
+    if (eglSurface == EGL14.EGL_NO_SURFACE) {
+      throw new RuntimeException("Failed to create pixel buffer surface");
+    }
+  }
+
+  public EGLContext getContext() {
+    return eglContext;
+  }
+
+  public boolean hasSurface() {
+    return eglSurface != EGL14.EGL_NO_SURFACE;
+  }
+
+  public void releaseSurface() {
+    if (eglSurface != EGL14.EGL_NO_SURFACE) {
+      EGL14.eglDestroySurface(eglDisplay, eglSurface);
+      eglSurface = EGL14.EGL_NO_SURFACE;
+    }
+  }
+
+  private void checkIsNotReleased() {
+    if (eglDisplay == EGL14.EGL_NO_DISPLAY || eglContext == EGL14.EGL_NO_CONTEXT
+        || eglConfig == null) {
+      throw new RuntimeException("This object has been released");
+    }
+  }
+
+  public void release() {
+    checkIsNotReleased();
+    releaseSurface();
+    // Release our context.
+    EGL14.eglMakeCurrent(
+        eglDisplay, EGL14.EGL_NO_SURFACE, EGL14.EGL_NO_SURFACE, EGL14.EGL_NO_CONTEXT);
+    EGL14.eglDestroyContext(eglDisplay, eglContext);
+    EGL14.eglReleaseThread();
+    EGL14.eglTerminate(eglDisplay);
+    eglContext = EGL14.EGL_NO_CONTEXT;
+    eglDisplay = EGL14.EGL_NO_DISPLAY;
+    eglConfig = null;
+  }
+
+  public void makeCurrent() {
+    checkIsNotReleased();
+    if (eglSurface == EGL14.EGL_NO_SURFACE) {
+      throw new RuntimeException("No EGLSurface - can't make current");
+    }
+    if (!EGL14.eglMakeCurrent(eglDisplay, eglSurface, eglSurface, eglContext)) {
+      throw new RuntimeException("eglMakeCurrent failed");
+    }
+  }
+
+  public void swapBuffers() {
+    checkIsNotReleased();
+    if (eglSurface == EGL14.EGL_NO_SURFACE) {
+      throw new RuntimeException("No EGLSurface - can't swap buffers");
+    }
+    EGL14.eglSwapBuffers(eglDisplay, eglSurface);
+  }
+
+  // Return an EGLDisplay, or die trying.
+  private static EGLDisplay getEglDisplay() {
+    EGLDisplay eglDisplay = EGL14.eglGetDisplay(EGL14.EGL_DEFAULT_DISPLAY);
+    if (eglDisplay == EGL14.EGL_NO_DISPLAY) {
+      throw new RuntimeException("Unable to get EGL14 display");
+    }
+    int[] version = new int[2];
+    if (!EGL14.eglInitialize(eglDisplay, version, 0, version, 1)) {
+      throw new RuntimeException("Unable to initialize EGL14");
+    }
+    return eglDisplay;
+  }
+
+  // Return an EGLConfig, or die trying.
+  private static EGLConfig getEglConfig(EGLDisplay eglDisplay, ConfigType configType) {
+    // Always RGB888, GLES2.
+    int[] configAttributes = {
+      EGL14.EGL_RED_SIZE, 8,
+      EGL14.EGL_GREEN_SIZE, 8,
+      EGL14.EGL_BLUE_SIZE, 8,
+      EGL14.EGL_RENDERABLE_TYPE, EGL14.EGL_OPENGL_ES2_BIT,
+      EGL14.EGL_NONE, 0,  // Allocate dummy fields for specific options.
+      EGL14.EGL_NONE
+    };
+
+    // Fill in dummy fields based on configType.
+    switch (configType) {
+      case PLAIN:
+        break;
+      case PIXEL_BUFFER:
+        configAttributes[configAttributes.length - 3] = EGL14.EGL_SURFACE_TYPE;
+        configAttributes[configAttributes.length - 2] = EGL14.EGL_PBUFFER_BIT;
+        break;
+      case RECORDABLE:
+        configAttributes[configAttributes.length - 3] = EGL_RECORDABLE_ANDROID;
+        configAttributes[configAttributes.length - 2] = 1;
+        break;
+      default:
+        throw new IllegalArgumentException();
+    }
+
+    EGLConfig[] configs = new EGLConfig[1];
+    int[] numConfigs = new int[1];
+    if (!EGL14.eglChooseConfig(
+        eglDisplay, configAttributes, 0, configs, 0, configs.length, numConfigs, 0)) {
+      throw new RuntimeException("Unable to find RGB888 " + configType + " EGL config");
+    }
+    return configs[0];
+  }
+
+  // Return an EGLConfig, or die trying.
+  private static EGLContext createEglContext(
+      EGLContext sharedContext, EGLDisplay eglDisplay, EGLConfig eglConfig) {
+    int[] contextAttributes = {EGL14.EGL_CONTEXT_CLIENT_VERSION, 2, EGL14.EGL_NONE};
+    EGLContext eglContext =
+        EGL14.eglCreateContext(eglDisplay, eglConfig, sharedContext, contextAttributes, 0);
+    if (eglContext == EGL14.EGL_NO_CONTEXT) {
+      throw new RuntimeException("Failed to create EGL context");
+    }
+    return eglContext;
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/GlRectDrawer.java b/librtc/src/main/java/org/webrtc/GlRectDrawer.java
new file mode 100644
index 0000000..abd0ddf
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/GlRectDrawer.java
@@ -0,0 +1,205 @@
+/*
+ * libjingle
+ * Copyright 2015 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import android.opengl.GLES11Ext;
+import android.opengl.GLES20;
+
+import org.webrtc.GlShader;
+import org.webrtc.GlUtil;
+
+import java.nio.ByteBuffer;
+import java.nio.FloatBuffer;
+import java.util.Arrays;
+
+/**
+ * Helper class to draw a quad that covers the entire viewport. Rotation, mirror, and cropping is
+ * specified using a 4x4 texture coordinate transform matrix. The frame input can either be an OES
+ * texture or YUV textures in I420 format. The GL state must be preserved between draw calls, this
+ * is intentional to maximize performance. The function release() must be called manually to free
+ * the resources held by this object.
+ */
+public class GlRectDrawer {
+  // Simple vertex shader, used for both YUV and OES.
+  private static final String VERTEX_SHADER_STRING =
+      "varying vec2 interp_tc;\n"
+      + "attribute vec4 in_pos;\n"
+      + "attribute vec4 in_tc;\n"
+      + "\n"
+      + "uniform mat4 texMatrix;\n"
+      + "\n"
+      + "void main() {\n"
+      + "    gl_Position = in_pos;\n"
+      + "    interp_tc = (texMatrix * in_tc).xy;\n"
+      + "}\n";
+
+  private static final String YUV_FRAGMENT_SHADER_STRING =
+      "precision mediump float;\n"
+      + "varying vec2 interp_tc;\n"
+      + "\n"
+      + "uniform sampler2D y_tex;\n"
+      + "uniform sampler2D u_tex;\n"
+      + "uniform sampler2D v_tex;\n"
+      + "\n"
+      + "void main() {\n"
+      // CSC according to http://www.fourcc.org/fccyvrgb.php
+      + "  float y = texture2D(y_tex, interp_tc).r;\n"
+      + "  float u = texture2D(u_tex, interp_tc).r - 0.5;\n"
+      + "  float v = texture2D(v_tex, interp_tc).r - 0.5;\n"
+      + "  gl_FragColor = vec4(y + 1.403 * v, "
+      + "                      y - 0.344 * u - 0.714 * v, "
+      + "                      y + 1.77 * u, 1);\n"
+      + "}\n";
+
+  private static final String OES_FRAGMENT_SHADER_STRING =
+      "#extension GL_OES_EGL_image_external : require\n"
+      + "precision mediump float;\n"
+      + "varying vec2 interp_tc;\n"
+      + "\n"
+      + "uniform samplerExternalOES oes_tex;\n"
+      + "\n"
+      + "void main() {\n"
+      + "  gl_FragColor = texture2D(oes_tex, interp_tc);\n"
+      + "}\n";
+
+  private static final FloatBuffer FULL_RECTANGLE_BUF =
+      GlUtil.createFloatBuffer(new float[] {
+            -1.0f, -1.0f,  // Bottom left.
+             1.0f, -1.0f,  // Bottom right.
+            -1.0f,  1.0f,  // Top left.
+             1.0f,  1.0f,  // Top right.
+          });
+
+  private static final FloatBuffer FULL_RECTANGLE_TEX_BUF =
+      GlUtil.createFloatBuffer(new float[] {
+            0.0f, 0.0f,  // Bottom left.
+            1.0f, 0.0f,  // Bottom right.
+            0.0f, 1.0f,  // Top left.
+            1.0f, 1.0f   // Top right.
+          });
+
+  private GlShader oesShader;
+  private GlShader yuvShader;
+  private GlShader currentShader;
+  private float[] currentTexMatrix;
+  private int texMatrixLocation;
+
+  private void initGeometry(GlShader shader) {
+    shader.setVertexAttribArray("in_pos", 2, FULL_RECTANGLE_BUF);
+    shader.setVertexAttribArray("in_tc", 2, FULL_RECTANGLE_TEX_BUF);
+  }
+
+  /**
+   * Draw an OES texture frame with specified texture transformation matrix. Required resources are
+   * allocated at the first call to this function.
+   */
+  public void drawOes(int oesTextureId, float[] texMatrix) {
+    // Lazy allocation.
+    if (oesShader == null) {
+      oesShader = new GlShader(VERTEX_SHADER_STRING, OES_FRAGMENT_SHADER_STRING);
+      oesShader.useProgram();
+      initGeometry(oesShader);
+    }
+
+    // Set GLES state to OES.
+    if (currentShader != oesShader) {
+      currentShader = oesShader;
+      oesShader.useProgram();
+      GLES20.glActiveTexture(GLES20.GL_TEXTURE0);
+      currentTexMatrix = null;
+      texMatrixLocation = oesShader.getUniformLocation("texMatrix");
+    }
+
+    // updateTexImage() may be called from another thread in another EGL context, so we need to
+    // bind/unbind the texture in each draw call so that GLES understads it's a new texture.
+    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, oesTextureId);
+    drawRectangle(texMatrix);
+    GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, 0);
+  }
+
+  /**
+   * Draw a YUV frame with specified texture transformation matrix. Required resources are
+   * allocated at the first call to this function.
+   */
+  public void drawYuv(int width, int height, int[] yuvTextures, float[] texMatrix) {
+    // Lazy allocation.
+    if (yuvShader == null) {
+      yuvShader = new GlShader(VERTEX_SHADER_STRING, YUV_FRAGMENT_SHADER_STRING);
+      yuvShader.useProgram();
+
+      // Set texture samplers.
+      GLES20.glUniform1i(yuvShader.getUniformLocation("y_tex"), 0);
+      GLES20.glUniform1i(yuvShader.getUniformLocation("u_tex"), 1);
+      GLES20.glUniform1i(yuvShader.getUniformLocation("v_tex"), 2);
+      GlUtil.checkNoGLES2Error("y/u/v_tex glGetUniformLocation");
+
+      initGeometry(yuvShader);
+    }
+
+    // Set GLES state to YUV.
+    if (currentShader != yuvShader) {
+      currentShader = yuvShader;
+      yuvShader.useProgram();
+      currentTexMatrix = null;
+      texMatrixLocation = yuvShader.getUniformLocation("texMatrix");
+    }
+
+    // Bind the textures.
+    for (int i = 0; i < 3; ++i) {
+      GLES20.glActiveTexture(GLES20.GL_TEXTURE0 + i);
+      GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, yuvTextures[i]);
+    }
+
+    drawRectangle(texMatrix);
+  }
+
+  private void drawRectangle(float[] texMatrix) {
+    // Try avoid uploading the texture if possible.
+    if (!Arrays.equals(currentTexMatrix, texMatrix)) {
+      currentTexMatrix = texMatrix.clone();
+      // Copy the texture transformation matrix over.
+      GLES20.glUniformMatrix4fv(texMatrixLocation, 1, false, texMatrix, 0);
+    }
+    // Draw quad.
+    GLES20.glDrawArrays(GLES20.GL_TRIANGLE_STRIP, 0, 4);
+  }
+
+  /**
+   * Release all GLES resources. This needs to be done manually, otherwise the resources are leaked.
+   */
+  public void release() {
+    if (oesShader != null) {
+      oesShader.release();
+      oesShader = null;
+    }
+    if (yuvShader != null) {
+      yuvShader.release();
+      yuvShader = null;
+    }
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/GlShader.java b/librtc/src/main/java/org/webrtc/GlShader.java
new file mode 100644
index 0000000..3014aab
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/GlShader.java
@@ -0,0 +1,143 @@
+/*
+ * libjingle
+ * Copyright 2015 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import android.opengl.GLES20;
+import android.util.Log;
+
+import java.nio.FloatBuffer;
+
+// Helper class for handling OpenGL shaders and shader programs.
+public class GlShader {
+  private static final String TAG = "GlShader";
+
+  private static int compileShader(int shaderType, String source) {
+    int[] result = new int[] {
+        GLES20.GL_FALSE
+    };
+    int shader = GLES20.glCreateShader(shaderType);
+    GLES20.glShaderSource(shader, source);
+    GLES20.glCompileShader(shader);
+    GLES20.glGetShaderiv(shader, GLES20.GL_COMPILE_STATUS, result, 0);
+    if (result[0] != GLES20.GL_TRUE) {
+      Log.e(TAG, "Could not compile shader " + shaderType + ":" +
+          GLES20.glGetShaderInfoLog(shader));
+      throw new RuntimeException(GLES20.glGetShaderInfoLog(shader));
+    }
+    GlUtil.checkNoGLES2Error("compileShader");
+    return shader;
+  }
+
+  private int vertexShader;
+  private int fragmentShader;
+  private int program;
+
+  public GlShader(String vertexSource, String fragmentSource) {
+    vertexShader = compileShader(GLES20.GL_VERTEX_SHADER, vertexSource);
+    fragmentShader = compileShader(GLES20.GL_FRAGMENT_SHADER, fragmentSource);
+    program = GLES20.glCreateProgram();
+    if (program == 0) {
+      throw new RuntimeException("Could not create program");
+    }
+    GLES20.glAttachShader(program, vertexShader);
+    GLES20.glAttachShader(program, fragmentShader);
+    GLES20.glLinkProgram(program);
+    int[] linkStatus = new int[] {
+      GLES20.GL_FALSE
+    };
+    GLES20.glGetProgramiv(program, GLES20.GL_LINK_STATUS, linkStatus, 0);
+    if (linkStatus[0] != GLES20.GL_TRUE) {
+      Log.e(TAG, "Could not link program: " +
+          GLES20.glGetProgramInfoLog(program));
+      throw new RuntimeException(GLES20.glGetProgramInfoLog(program));
+    }
+    GlUtil.checkNoGLES2Error("Creating GlShader");
+  }
+
+  public int getAttribLocation(String label) {
+    if (program == -1) {
+      throw new RuntimeException("The program has been released");
+    }
+    int location = GLES20.glGetAttribLocation(program, label);
+    if (location < 0) {
+      throw new RuntimeException("Could not locate '" + label + "' in program");
+    }
+    return location;
+  }
+
+  /**
+   * Enable and upload a vertex array for attribute |label|. The vertex data is specified in
+   * |buffer| with |dimension| number of components per vertex.
+   */
+  public void setVertexAttribArray(String label, int dimension, FloatBuffer buffer) {
+    if (program == -1) {
+      throw new RuntimeException("The program has been released");
+    }
+    int location = getAttribLocation(label);
+    GLES20.glEnableVertexAttribArray(location);
+    GLES20.glVertexAttribPointer(location, dimension, GLES20.GL_FLOAT, false, 0, buffer);
+    GlUtil.checkNoGLES2Error("setVertexAttribArray");
+  }
+
+  public int getUniformLocation(String label) {
+    if (program == -1) {
+      throw new RuntimeException("The program has been released");
+    }
+    int location = GLES20.glGetUniformLocation(program, label);
+    if (location < 0) {
+      throw new RuntimeException("Could not locate uniform '" + label + "' in program");
+    }
+    return location;
+  }
+
+  public void useProgram() {
+    if (program == -1) {
+      throw new RuntimeException("The program has been released");
+    }
+    GLES20.glUseProgram(program);
+    GlUtil.checkNoGLES2Error("glUseProgram");
+  }
+
+  public void release() {
+    Log.d(TAG, "Deleting shader.");
+    // Flag shaders for deletion (does not delete until no longer attached to a program).
+    if (vertexShader != -1) {
+      GLES20.glDeleteShader(vertexShader);
+      vertexShader = -1;
+    }
+    if (fragmentShader != -1) {
+      GLES20.glDeleteShader(fragmentShader);
+      fragmentShader = -1;
+    }
+    // Delete program, automatically detaching any shaders from it.
+    if (program != -1) {
+      GLES20.glDeleteProgram(program);
+      program = -1;
+    }
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/GlUtil.java b/librtc/src/main/java/org/webrtc/GlUtil.java
new file mode 100644
index 0000000..196384e
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/GlUtil.java
@@ -0,0 +1,61 @@
+/*
+ * libjingle
+ * Copyright 2015 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import android.opengl.GLES20;
+import android.util.Log;
+
+import java.nio.ByteBuffer;
+import java.nio.ByteOrder;
+import java.nio.FloatBuffer;
+
+/**
+ * Some OpenGL static utility functions.
+ */
+public class GlUtil {
+  private static final String TAG = "GlUtil";
+  private GlUtil() {}
+
+  // Assert that no OpenGL ES 2.0 error has been raised.
+  public static void checkNoGLES2Error(String msg) {
+    int error = GLES20.glGetError();
+    if (error != GLES20.GL_NO_ERROR) {
+      throw new RuntimeException(msg + ": GLES20 error: " + error);
+    }
+  }
+
+  public static FloatBuffer createFloatBuffer(float[] coords) {
+    // Allocate a direct ByteBuffer, using 4 bytes per float, and copy coords into it.
+    ByteBuffer bb = ByteBuffer.allocateDirect(coords.length * 4);
+    bb.order(ByteOrder.nativeOrder());
+    FloatBuffer fb = bb.asFloatBuffer();
+    fb.put(coords);
+    fb.position(0);
+    return fb;
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/IceCandidate.java b/librtc/src/main/java/org/webrtc/IceCandidate.java
new file mode 100644
index 0000000..eb42ce4
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/IceCandidate.java
@@ -0,0 +1,48 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/**
+ * Representation of a single ICE Candidate, mirroring
+ * {@code IceCandidateInterface} in the C++ API.
+ */
+public class IceCandidate {
+  public final String sdpMid;
+  public final int sdpMLineIndex;
+  public final String sdp;
+
+  public IceCandidate(String sdpMid, int sdpMLineIndex, String sdp) {
+    this.sdpMid = sdpMid;
+    this.sdpMLineIndex = sdpMLineIndex;
+    this.sdp = sdp;
+  }
+
+  public String toString() {
+    return sdpMid + ":" + sdpMLineIndex + ":" + sdp;
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/Logging.java b/librtc/src/main/java/org/webrtc/Logging.java
new file mode 100644
index 0000000..fd49bce
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/Logging.java
@@ -0,0 +1,81 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.util.EnumSet;
+
+/** Java wrapper for WebRTC & libjingle logging. */
+public class Logging {
+  static {
+    System.loadLibrary("jingle_peerconnection_so");
+  }
+
+  // Keep in sync with webrtc/common_types.h:TraceLevel.
+  public enum TraceLevel {
+    TRACE_NONE(0x0000),
+    TRACE_STATEINFO(0x0001),
+    TRACE_WARNING(0x0002),
+    TRACE_ERROR(0x0004),
+    TRACE_CRITICAL(0x0008),
+    TRACE_APICALL(0x0010),
+    TRACE_DEFAULT(0x00ff),
+    TRACE_MODULECALL(0x0020),
+    TRACE_MEMORY(0x0100),
+    TRACE_TIMER(0x0200),
+    TRACE_STREAM(0x0400),
+    TRACE_DEBUG(0x0800),
+    TRACE_INFO(0x1000),
+    TRACE_TERSEINFO(0x2000),
+    TRACE_ALL(0xffff);
+
+    public final int level;
+    TraceLevel(int level) {
+      this.level = level;
+    }
+  };
+
+  // Keep in sync with webrtc/base/logging.h:LoggingSeverity.
+  public enum Severity {
+    LS_SENSITIVE, LS_VERBOSE, LS_INFO, LS_WARNING, LS_ERROR,
+  };
+
+
+  // Enable tracing to |path| of messages of |levels| and |severity|.
+  // On Android, use "logcat:" for |path| to send output there.
+  public static void enableTracing(
+      String path, EnumSet<TraceLevel> levels, Severity severity) {
+    int nativeLevel = 0;
+    for (TraceLevel level : levels) {
+      nativeLevel |= level.level;
+    }
+    nativeEnableTracing(path, nativeLevel, severity.ordinal());
+  }
+
+  private static native void nativeEnableTracing(
+      String path, int nativeLevels, int nativeSeverity);
+}
diff --git a/librtc/src/main/java/org/webrtc/MediaCodecVideoDecoder.java b/librtc/src/main/java/org/webrtc/MediaCodecVideoDecoder.java
new file mode 100644
index 0000000..17e98b7
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/MediaCodecVideoDecoder.java
@@ -0,0 +1,414 @@
+/*
+ * libjingle
+ * Copyright 2014 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import android.graphics.SurfaceTexture;
+import android.media.MediaCodec;
+import android.media.MediaCodecInfo;
+import android.media.MediaCodecInfo.CodecCapabilities;
+import android.media.MediaCodecList;
+import android.media.MediaFormat;
+import android.opengl.EGL14;
+import android.opengl.EGLContext;
+import android.opengl.GLES11Ext;
+import android.opengl.GLES20;
+import android.os.Build;
+import android.util.Log;
+import android.view.Surface;
+
+import java.nio.ByteBuffer;
+
+// Java-side of peerconnection_jni.cc:MediaCodecVideoDecoder.
+// This class is an implementation detail of the Java PeerConnection API.
+// MediaCodec is thread-hostile so this class must be operated on a single
+// thread.
+public class MediaCodecVideoDecoder {
+  // This class is constructed, operated, and destroyed by its C++ incarnation,
+  // so the class and its methods have non-public visibility.  The API this
+  // class exposes aims to mimic the webrtc::VideoDecoder API as closely as
+  // possibly to minimize the amount of translation work necessary.
+
+  private static final String TAG = "MediaCodecVideoDecoder";
+
+  // Tracks webrtc::VideoCodecType.
+  public enum VideoCodecType {
+    VIDEO_CODEC_VP8,
+    VIDEO_CODEC_VP9,
+    VIDEO_CODEC_H264
+  }
+
+  private static final int DEQUEUE_INPUT_TIMEOUT = 500000;  // 500 ms timeout.
+  private Thread mediaCodecThread;
+  private MediaCodec mediaCodec;
+  private ByteBuffer[] inputBuffers;
+  private ByteBuffer[] outputBuffers;
+  private static final String VP8_MIME_TYPE = "video/x-vnd.on2.vp8";
+  private static final String H264_MIME_TYPE = "video/avc";
+  // List of supported HW VP8 decoders.
+  private static final String[] supportedVp8HwCodecPrefixes =
+    {"OMX.qcom.", "OMX.Nvidia.", "OMX.Exynos.", "OMX.Intel." };
+  // List of supported HW H.264 decoders.
+  private static final String[] supportedH264HwCodecPrefixes =
+    {"OMX.qcom.", "OMX.Intel." };
+  // NV12 color format supported by QCOM codec, but not declared in MediaCodec -
+  // see /hardware/qcom/media/mm-core/inc/OMX_QCOMExtns.h
+  private static final int
+    COLOR_QCOM_FORMATYUV420PackedSemiPlanar32m = 0x7FA30C04;
+  // Allowable color formats supported by codec - in order of preference.
+  private static final int[] supportedColorList = {
+    CodecCapabilities.COLOR_FormatYUV420Planar,
+    CodecCapabilities.COLOR_FormatYUV420SemiPlanar,
+    CodecCapabilities.COLOR_QCOM_FormatYUV420SemiPlanar,
+    COLOR_QCOM_FORMATYUV420PackedSemiPlanar32m
+  };
+  private int colorFormat;
+  private int width;
+  private int height;
+  private int stride;
+  private int sliceHeight;
+  private boolean useSurface;
+  private int textureID = -1;
+  private SurfaceTexture surfaceTexture = null;
+  private Surface surface = null;
+  private EglBase eglBase;
+
+  private MediaCodecVideoDecoder() { }
+
+  // Helper struct for findVp8Decoder() below.
+  private static class DecoderProperties {
+    public DecoderProperties(String codecName, int colorFormat) {
+      this.codecName = codecName;
+      this.colorFormat = colorFormat;
+    }
+    public final String codecName; // OpenMax component name for VP8 codec.
+    public final int colorFormat;  // Color format supported by codec.
+  }
+
+  private static DecoderProperties findDecoder(
+      String mime, String[] supportedCodecPrefixes) {
+    if (Build.VERSION.SDK_INT < Build.VERSION_CODES.KITKAT) {
+      return null; // MediaCodec.setParameters is missing.
+    }
+    for (int i = 0; i < MediaCodecList.getCodecCount(); ++i) {
+      MediaCodecInfo info = MediaCodecList.getCodecInfoAt(i);
+      if (info.isEncoder()) {
+        continue;
+      }
+      String name = null;
+      for (String mimeType : info.getSupportedTypes()) {
+        if (mimeType.equals(mime)) {
+          name = info.getName();
+          break;
+        }
+      }
+      if (name == null) {
+        continue;  // No HW support in this codec; try the next one.
+      }
+      Log.v(TAG, "Found candidate decoder " + name);
+
+      // Check if this is supported decoder.
+      boolean supportedCodec = false;
+      for (String codecPrefix : supportedCodecPrefixes) {
+        if (name.startsWith(codecPrefix)) {
+          supportedCodec = true;
+          break;
+        }
+      }
+      if (!supportedCodec) {
+        continue;
+      }
+
+      // Check if codec supports either yuv420 or nv12.
+      CodecCapabilities capabilities =
+          info.getCapabilitiesForType(mime);
+      for (int colorFormat : capabilities.colorFormats) {
+        Log.v(TAG, "   Color: 0x" + Integer.toHexString(colorFormat));
+      }
+      for (int supportedColorFormat : supportedColorList) {
+        for (int codecColorFormat : capabilities.colorFormats) {
+          if (codecColorFormat == supportedColorFormat) {
+            // Found supported HW decoder.
+            Log.d(TAG, "Found target decoder " + name +
+                ". Color: 0x" + Integer.toHexString(codecColorFormat));
+            return new DecoderProperties(name, codecColorFormat);
+          }
+        }
+      }
+    }
+    return null;  // No HW decoder.
+  }
+
+  public static boolean isVp8HwSupported() {
+    return findDecoder(VP8_MIME_TYPE, supportedVp8HwCodecPrefixes) != null;
+  }
+
+  public static boolean isH264HwSupported() {
+    return findDecoder(H264_MIME_TYPE, supportedH264HwCodecPrefixes) != null;
+  }
+
+  private void checkOnMediaCodecThread() {
+    if (mediaCodecThread.getId() != Thread.currentThread().getId()) {
+      throw new RuntimeException(
+          "MediaCodecVideoDecoder previously operated on " + mediaCodecThread +
+          " but is now called on " + Thread.currentThread());
+    }
+  }
+
+  private boolean initDecode(
+      VideoCodecType type, int width, int height,
+      boolean useSurface, EGLContext sharedContext) {
+    if (mediaCodecThread != null) {
+      throw new RuntimeException("Forgot to release()?");
+    }
+    if (useSurface && sharedContext == null) {
+      throw new RuntimeException("No shared EGL context.");
+    }
+    String mime = null;
+    String[] supportedCodecPrefixes = null;
+    if (type == VideoCodecType.VIDEO_CODEC_VP8) {
+      mime = VP8_MIME_TYPE;
+      supportedCodecPrefixes = supportedVp8HwCodecPrefixes;
+    } else if (type == VideoCodecType.VIDEO_CODEC_H264) {
+      mime = H264_MIME_TYPE;
+      supportedCodecPrefixes = supportedH264HwCodecPrefixes;
+    } else {
+      throw new RuntimeException("Non supported codec " + type);
+    }
+    DecoderProperties properties = findDecoder(mime, supportedCodecPrefixes);
+    if (properties == null) {
+      throw new RuntimeException("Cannot find HW decoder for " + type);
+    }
+    Log.d(TAG, "Java initDecode: " + type + " : "+ width + " x " + height +
+        ". Color: 0x" + Integer.toHexString(properties.colorFormat) +
+        ". Use Surface: " + useSurface);
+    if (sharedContext != null) {
+      Log.d(TAG, "Decoder shared EGL Context: " + sharedContext);
+    }
+    mediaCodecThread = Thread.currentThread();
+    try {
+      Surface decodeSurface = null;
+      this.width = width;
+      this.height = height;
+      this.useSurface = useSurface;
+      stride = width;
+      sliceHeight = height;
+
+      if (useSurface) {
+        // Create shared EGL context.
+        eglBase = new EglBase(sharedContext, EglBase.ConfigType.PIXEL_BUFFER);
+        eglBase.createDummyPbufferSurface();
+        eglBase.makeCurrent();
+
+        // Create output surface
+        int[] textures = new int[1];
+        GLES20.glGenTextures(1, textures, 0);
+        GlUtil.checkNoGLES2Error("glGenTextures");
+        textureID = textures[0];
+        GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES, textureID);
+        GlUtil.checkNoGLES2Error("glBindTexture mTextureID");
+
+        GLES20.glTexParameterf(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_NEAREST);
+        GLES20.glTexParameterf(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR);
+        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
+        GlUtil.checkNoGLES2Error("glTexParameter");
+        Log.d(TAG, "Video decoder TextureID = " + textureID);
+        surfaceTexture = new SurfaceTexture(textureID);
+        surface = new Surface(surfaceTexture);
+        decodeSurface = surface;
+      }
+
+      MediaFormat format = MediaFormat.createVideoFormat(mime, width, height);
+      if (!useSurface) {
+        format.setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
+      }
+      Log.d(TAG, "  Format: " + format);
+      mediaCodec =
+          MediaCodecVideoEncoder.createByCodecName(properties.codecName);
+      if (mediaCodec == null) {
+        return false;
+      }
+      mediaCodec.configure(format, decodeSurface, null, 0);
+      mediaCodec.start();
+      colorFormat = properties.colorFormat;
+      outputBuffers = mediaCodec.getOutputBuffers();
+      inputBuffers = mediaCodec.getInputBuffers();
+      Log.d(TAG, "Input buffers: " + inputBuffers.length +
+          ". Output buffers: " + outputBuffers.length);
+      return true;
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "initDecode failed", e);
+      return false;
+    }
+  }
+
+  private void release() {
+    Log.d(TAG, "Java releaseDecoder");
+    checkOnMediaCodecThread();
+    try {
+      mediaCodec.stop();
+      mediaCodec.release();
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "release failed", e);
+    }
+    mediaCodec = null;
+    mediaCodecThread = null;
+    if (useSurface) {
+      surface.release();
+      if (textureID >= 0) {
+        int[] textures = new int[1];
+        textures[0] = textureID;
+        Log.d(TAG, "Delete video decoder TextureID " + textureID);
+        GLES20.glDeleteTextures(1, textures, 0);
+        GlUtil.checkNoGLES2Error("glDeleteTextures");
+      }
+      eglBase.release();
+      eglBase = null;
+    }
+  }
+
+  // Dequeue an input buffer and return its index, -1 if no input buffer is
+  // available, or -2 if the codec is no longer operative.
+  private int dequeueInputBuffer() {
+    checkOnMediaCodecThread();
+    try {
+      return mediaCodec.dequeueInputBuffer(DEQUEUE_INPUT_TIMEOUT);
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "dequeueIntputBuffer failed", e);
+      return -2;
+    }
+  }
+
+  private boolean queueInputBuffer(
+      int inputBufferIndex, int size, long timestampUs) {
+    checkOnMediaCodecThread();
+    try {
+      inputBuffers[inputBufferIndex].position(0);
+      inputBuffers[inputBufferIndex].limit(size);
+      mediaCodec.queueInputBuffer(inputBufferIndex, 0, size, timestampUs, 0);
+      return true;
+    }
+    catch (IllegalStateException e) {
+      Log.e(TAG, "decode failed", e);
+      return false;
+    }
+  }
+
+  // Helper struct for dequeueOutputBuffer() below.
+  private static class DecoderOutputBufferInfo {
+    public DecoderOutputBufferInfo(
+        int index, int offset, int size, long presentationTimestampUs) {
+      this.index = index;
+      this.offset = offset;
+      this.size = size;
+      this.presentationTimestampUs = presentationTimestampUs;
+    }
+
+    private final int index;
+    private final int offset;
+    private final int size;
+    private final long presentationTimestampUs;
+  }
+
+  // Dequeue and return an output buffer index, -1 if no output
+  // buffer available or -2 if error happened.
+  private DecoderOutputBufferInfo dequeueOutputBuffer(int dequeueTimeoutUs) {
+    checkOnMediaCodecThread();
+    try {
+      MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
+      int result = mediaCodec.dequeueOutputBuffer(info, dequeueTimeoutUs);
+      while (result == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED ||
+          result == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
+        if (result == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
+          outputBuffers = mediaCodec.getOutputBuffers();
+          Log.d(TAG, "Decoder output buffers changed: " + outputBuffers.length);
+        } else if (result == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
+          MediaFormat format = mediaCodec.getOutputFormat();
+          Log.d(TAG, "Decoder format changed: " + format.toString());
+          width = format.getInteger(MediaFormat.KEY_WIDTH);
+          height = format.getInteger(MediaFormat.KEY_HEIGHT);
+          if (!useSurface && format.containsKey(MediaFormat.KEY_COLOR_FORMAT)) {
+            colorFormat = format.getInteger(MediaFormat.KEY_COLOR_FORMAT);
+            Log.d(TAG, "Color: 0x" + Integer.toHexString(colorFormat));
+            // Check if new color space is supported.
+            boolean validColorFormat = false;
+            for (int supportedColorFormat : supportedColorList) {
+              if (colorFormat == supportedColorFormat) {
+                validColorFormat = true;
+                break;
+              }
+            }
+            if (!validColorFormat) {
+              Log.e(TAG, "Non supported color format");
+              return new DecoderOutputBufferInfo(-1, 0, 0, -1);
+            }
+          }
+          if (format.containsKey("stride")) {
+            stride = format.getInteger("stride");
+          }
+          if (format.containsKey("slice-height")) {
+            sliceHeight = format.getInteger("slice-height");
+          }
+          Log.d(TAG, "Frame stride and slice height: "
+              + stride + " x " + sliceHeight);
+          stride = Math.max(width, stride);
+          sliceHeight = Math.max(height, sliceHeight);
+        }
+        result = mediaCodec.dequeueOutputBuffer(info, dequeueTimeoutUs);
+      }
+      if (result >= 0) {
+        return new DecoderOutputBufferInfo(result, info.offset, info.size,
+            info.presentationTimeUs);
+      }
+      return null;
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "dequeueOutputBuffer failed", e);
+      return new DecoderOutputBufferInfo(-1, 0, 0, -1);
+    }
+  }
+
+  // Release a dequeued output buffer back to the codec for re-use.  Return
+  // false if the codec is no longer operable.
+  private boolean releaseOutputBuffer(int index, boolean render) {
+    checkOnMediaCodecThread();
+    try {
+      if (!useSurface) {
+        render = false;
+      }
+      mediaCodec.releaseOutputBuffer(index, render);
+      return true;
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "releaseOutputBuffer failed", e);
+      return false;
+    }
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/MediaCodecVideoEncoder.java b/librtc/src/main/java/org/webrtc/MediaCodecVideoEncoder.java
new file mode 100644
index 0000000..eb31cdd
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/MediaCodecVideoEncoder.java
@@ -0,0 +1,425 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+package org.webrtc;
+
+import android.media.MediaCodec;
+import android.media.MediaCodecInfo.CodecCapabilities;
+import android.media.MediaCodecInfo;
+import android.media.MediaCodecList;
+import android.media.MediaFormat;
+import android.os.Build;
+import android.os.Bundle;
+import android.util.Log;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.List;
+
+// Java-side of peerconnection_jni.cc:MediaCodecVideoEncoder.
+// This class is an implementation detail of the Java PeerConnection API.
+// MediaCodec is thread-hostile so this class must be operated on a single
+// thread.
+public class MediaCodecVideoEncoder {
+  // This class is constructed, operated, and destroyed by its C++ incarnation,
+  // so the class and its methods have non-public visibility.  The API this
+  // class exposes aims to mimic the webrtc::VideoEncoder API as closely as
+  // possibly to minimize the amount of translation work necessary.
+
+  private static final String TAG = "MediaCodecVideoEncoder";
+
+  // Tracks webrtc::VideoCodecType.
+  public enum VideoCodecType {
+    VIDEO_CODEC_VP8,
+    VIDEO_CODEC_VP9,
+    VIDEO_CODEC_H264
+  }
+
+  private static final int DEQUEUE_TIMEOUT = 0;  // Non-blocking, no wait.
+  private Thread mediaCodecThread;
+  private MediaCodec mediaCodec;
+  private ByteBuffer[] outputBuffers;
+  private static final String VP8_MIME_TYPE = "video/x-vnd.on2.vp8";
+  private static final String H264_MIME_TYPE = "video/avc";
+  // List of supported HW VP8 codecs.
+  private static final String[] supportedVp8HwCodecPrefixes =
+    {"OMX.qcom.", "OMX.Intel." };
+  // List of supported HW H.264 codecs.
+  private static final String[] supportedH264HwCodecPrefixes =
+    {"OMX.qcom." };
+  // List of devices with poor H.264 encoder quality.
+  private static final String[] H264_HW_EXCEPTION_MODELS = new String[] {
+    // HW H.264 encoder on Galaxy S4 generates 2 times lower bitrate comparing
+    // to target.
+    "SAMSUNG-SGH-I337",
+  };
+
+  // Bitrate modes - should be in sync with OMX_VIDEO_CONTROLRATETYPE defined
+  // in OMX_Video.h
+  private static final int VIDEO_ControlRateVariable = 1;
+  private static final int VIDEO_ControlRateConstant = 2;
+  // NV12 color format supported by QCOM codec, but not declared in MediaCodec -
+  // see /hardware/qcom/media/mm-core/inc/OMX_QCOMExtns.h
+  private static final int
+    COLOR_QCOM_FORMATYUV420PackedSemiPlanar32m = 0x7FA30C04;
+  // Allowable color formats supported by codec - in order of preference.
+  private static final int[] supportedColorList = {
+    CodecCapabilities.COLOR_FormatYUV420Planar,
+    CodecCapabilities.COLOR_FormatYUV420SemiPlanar,
+    CodecCapabilities.COLOR_QCOM_FormatYUV420SemiPlanar,
+    COLOR_QCOM_FORMATYUV420PackedSemiPlanar32m
+  };
+  private int colorFormat;
+  // Video encoder type.
+  private VideoCodecType type;
+  // SPS and PPS NALs (Config frame) for H.264.
+  private ByteBuffer configData = null;
+
+  private MediaCodecVideoEncoder() {}
+
+  // Helper struct for findHwEncoder() below.
+  private static class EncoderProperties {
+    public EncoderProperties(String codecName, int colorFormat) {
+      this.codecName = codecName;
+      this.colorFormat = colorFormat;
+    }
+    public final String codecName; // OpenMax component name for HW codec.
+    public final int colorFormat;  // Color format supported by codec.
+  }
+
+  private static EncoderProperties findHwEncoder(
+      String mime, String[] supportedHwCodecPrefixes) {
+    // MediaCodec.setParameters is missing for JB and below, so bitrate
+    // can not be adjusted dynamically.
+    if (Build.VERSION.SDK_INT < Build.VERSION_CODES.KITKAT) {
+      return null;
+    }
+
+    // Check if device is in H.264 exception list.
+    if (mime.equals(H264_MIME_TYPE)) {
+      List<String> exceptionModels = Arrays.asList(H264_HW_EXCEPTION_MODELS);
+      if (exceptionModels.contains(Build.MODEL)) {
+        Log.w(TAG, "Model: " + Build.MODEL +
+            " has black listed H.264 encoder.");
+        return null;
+      }
+    }
+
+    for (int i = 0; i < MediaCodecList.getCodecCount(); ++i) {
+      MediaCodecInfo info = MediaCodecList.getCodecInfoAt(i);
+      if (!info.isEncoder()) {
+        continue;
+      }
+      String name = null;
+      for (String mimeType : info.getSupportedTypes()) {
+        if (mimeType.equals(mime)) {
+          name = info.getName();
+          break;
+        }
+      }
+      if (name == null) {
+        continue;  // No HW support in this codec; try the next one.
+      }
+      Log.v(TAG, "Found candidate encoder " + name);
+
+      // Check if this is supported HW encoder.
+      boolean supportedCodec = false;
+      for (String hwCodecPrefix : supportedHwCodecPrefixes) {
+        if (name.startsWith(hwCodecPrefix)) {
+          supportedCodec = true;
+          break;
+        }
+      }
+      if (!supportedCodec) {
+        continue;
+      }
+
+      CodecCapabilities capabilities = info.getCapabilitiesForType(mime);
+      for (int colorFormat : capabilities.colorFormats) {
+        Log.v(TAG, "   Color: 0x" + Integer.toHexString(colorFormat));
+      }
+
+      // Check if codec supports either yuv420 or nv12.
+      for (int supportedColorFormat : supportedColorList) {
+        for (int codecColorFormat : capabilities.colorFormats) {
+          if (codecColorFormat == supportedColorFormat) {
+            // Found supported HW encoder.
+            Log.d(TAG, "Found target encoder for mime " + mime + " : " + name +
+                ". Color: 0x" + Integer.toHexString(codecColorFormat));
+            return new EncoderProperties(name, codecColorFormat);
+          }
+        }
+      }
+    }
+    return null;  // No HW VP8 encoder.
+  }
+
+  public static boolean isVp8HwSupported() {
+    return findHwEncoder(VP8_MIME_TYPE, supportedVp8HwCodecPrefixes) != null;
+  }
+
+  public static boolean isH264HwSupported() {
+    return findHwEncoder(H264_MIME_TYPE, supportedH264HwCodecPrefixes) != null;
+  }
+
+  private void checkOnMediaCodecThread() {
+    if (mediaCodecThread.getId() != Thread.currentThread().getId()) {
+      throw new RuntimeException(
+          "MediaCodecVideoEncoder previously operated on " + mediaCodecThread +
+          " but is now called on " + Thread.currentThread());
+    }
+  }
+
+  static MediaCodec createByCodecName(String codecName) {
+    try {
+      // In the L-SDK this call can throw IOException so in order to work in
+      // both cases catch an exception.
+      return MediaCodec.createByCodecName(codecName);
+    } catch (Exception e) {
+      return null;
+    }
+  }
+
+  // Return the array of input buffers, or null on failure.
+  private ByteBuffer[] initEncode(
+      VideoCodecType type, int width, int height, int kbps, int fps) {
+    Log.d(TAG, "Java initEncode: " + type + " : " + width + " x " + height +
+        ". @ " + kbps + " kbps. Fps: " + fps +
+        ". Color: 0x" + Integer.toHexString(colorFormat));
+    if (mediaCodecThread != null) {
+      throw new RuntimeException("Forgot to release()?");
+    }
+    this.type = type;
+    EncoderProperties properties = null;
+    String mime = null;
+    int keyFrameIntervalSec = 0;
+    if (type == VideoCodecType.VIDEO_CODEC_VP8) {
+      mime = VP8_MIME_TYPE;
+      properties = findHwEncoder(VP8_MIME_TYPE, supportedVp8HwCodecPrefixes);
+      keyFrameIntervalSec = 100;
+    } else if (type == VideoCodecType.VIDEO_CODEC_H264) {
+      mime = H264_MIME_TYPE;
+      properties = findHwEncoder(H264_MIME_TYPE, supportedH264HwCodecPrefixes);
+      keyFrameIntervalSec = 20;
+    }
+    if (properties == null) {
+      throw new RuntimeException("Can not find HW encoder for " + type);
+    }
+    mediaCodecThread = Thread.currentThread();
+    try {
+      MediaFormat format = MediaFormat.createVideoFormat(mime, width, height);
+      format.setInteger(MediaFormat.KEY_BIT_RATE, 1000 * kbps);
+      format.setInteger("bitrate-mode", VIDEO_ControlRateConstant);
+      format.setInteger(MediaFormat.KEY_COLOR_FORMAT, properties.colorFormat);
+      format.setInteger(MediaFormat.KEY_FRAME_RATE, fps);
+      format.setInteger(MediaFormat.KEY_I_FRAME_INTERVAL, keyFrameIntervalSec);
+      Log.d(TAG, "  Format: " + format);
+      mediaCodec = createByCodecName(properties.codecName);
+      if (mediaCodec == null) {
+        return null;
+      }
+      mediaCodec.configure(
+          format, null, null, MediaCodec.CONFIGURE_FLAG_ENCODE);
+      mediaCodec.start();
+      colorFormat = properties.colorFormat;
+      outputBuffers = mediaCodec.getOutputBuffers();
+      ByteBuffer[] inputBuffers = mediaCodec.getInputBuffers();
+      Log.d(TAG, "Input buffers: " + inputBuffers.length +
+          ". Output buffers: " + outputBuffers.length);
+      return inputBuffers;
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "initEncode failed", e);
+      return null;
+    }
+  }
+
+  private boolean encode(
+      boolean isKeyframe, int inputBuffer, int size,
+      long presentationTimestampUs) {
+    checkOnMediaCodecThread();
+    try {
+      if (isKeyframe) {
+        // Ideally MediaCodec would honor BUFFER_FLAG_SYNC_FRAME so we could
+        // indicate this in queueInputBuffer() below and guarantee _this_ frame
+        // be encoded as a key frame, but sadly that flag is ignored.  Instead,
+        // we request a key frame "soon".
+        Log.d(TAG, "Sync frame request");
+        Bundle b = new Bundle();
+        b.putInt(MediaCodec.PARAMETER_KEY_REQUEST_SYNC_FRAME, 0);
+        mediaCodec.setParameters(b);
+      }
+      mediaCodec.queueInputBuffer(
+          inputBuffer, 0, size, presentationTimestampUs, 0);
+      return true;
+    }
+    catch (IllegalStateException e) {
+      Log.e(TAG, "encode failed", e);
+      return false;
+    }
+  }
+
+  private void release() {
+    Log.d(TAG, "Java releaseEncoder");
+    checkOnMediaCodecThread();
+    try {
+      mediaCodec.stop();
+      mediaCodec.release();
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "release failed", e);
+    }
+    mediaCodec = null;
+    mediaCodecThread = null;
+  }
+
+  private boolean setRates(int kbps, int frameRateIgnored) {
+    // frameRate argument is ignored - HW encoder is supposed to use
+    // video frame timestamps for bit allocation.
+    checkOnMediaCodecThread();
+    Log.v(TAG, "setRates: " + kbps + " kbps. Fps: " + frameRateIgnored);
+    try {
+      Bundle params = new Bundle();
+      params.putInt(MediaCodec.PARAMETER_KEY_VIDEO_BITRATE, 1000 * kbps);
+      mediaCodec.setParameters(params);
+      return true;
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "setRates failed", e);
+      return false;
+    }
+  }
+
+  // Dequeue an input buffer and return its index, -1 if no input buffer is
+  // available, or -2 if the codec is no longer operative.
+  private int dequeueInputBuffer() {
+    checkOnMediaCodecThread();
+    try {
+      return mediaCodec.dequeueInputBuffer(DEQUEUE_TIMEOUT);
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "dequeueIntputBuffer failed", e);
+      return -2;
+    }
+  }
+
+  // Helper struct for dequeueOutputBuffer() below.
+  private static class OutputBufferInfo {
+    public OutputBufferInfo(
+        int index, ByteBuffer buffer,
+        boolean isKeyFrame, long presentationTimestampUs) {
+      this.index = index;
+      this.buffer = buffer;
+      this.isKeyFrame = isKeyFrame;
+      this.presentationTimestampUs = presentationTimestampUs;
+    }
+
+    private final int index;
+    private final ByteBuffer buffer;
+    private final boolean isKeyFrame;
+    private final long presentationTimestampUs;
+  }
+
+  // Dequeue and return an output buffer, or null if no output is ready.  Return
+  // a fake OutputBufferInfo with index -1 if the codec is no longer operable.
+  private OutputBufferInfo dequeueOutputBuffer() {
+    checkOnMediaCodecThread();
+    try {
+      MediaCodec.BufferInfo info = new MediaCodec.BufferInfo();
+      int result = mediaCodec.dequeueOutputBuffer(info, DEQUEUE_TIMEOUT);
+      // Check if this is config frame and save configuration data.
+      if (result >= 0) {
+        boolean isConfigFrame =
+            (info.flags & MediaCodec.BUFFER_FLAG_CODEC_CONFIG) != 0;
+        if (isConfigFrame) {
+          Log.d(TAG, "Config frame generated. Offset: " + info.offset +
+              ". Size: " + info.size);
+          configData = ByteBuffer.allocateDirect(info.size);
+          outputBuffers[result].position(info.offset);
+          outputBuffers[result].limit(info.offset + info.size);
+          configData.put(outputBuffers[result]);
+          // Release buffer back.
+          mediaCodec.releaseOutputBuffer(result, false);
+          // Query next output.
+          result = mediaCodec.dequeueOutputBuffer(info, DEQUEUE_TIMEOUT);
+        }
+      }
+      if (result >= 0) {
+        // MediaCodec doesn't care about Buffer position/remaining/etc so we can
+        // mess with them to get a slice and avoid having to pass extra
+        // (BufferInfo-related) parameters back to C++.
+        ByteBuffer outputBuffer = outputBuffers[result].duplicate();
+        outputBuffer.position(info.offset);
+        outputBuffer.limit(info.offset + info.size);
+        // Check key frame flag.
+        boolean isKeyFrame =
+            (info.flags & MediaCodec.BUFFER_FLAG_SYNC_FRAME) != 0;
+        if (isKeyFrame) {
+          Log.d(TAG, "Sync frame generated");
+        }
+        if (isKeyFrame && type == VideoCodecType.VIDEO_CODEC_H264) {
+          Log.d(TAG, "Appending config frame of size " + configData.capacity() +
+              " to output buffer with offset " + info.offset + ", size " +
+              info.size);
+          // For H.264 key frame append SPS and PPS NALs at the start
+          ByteBuffer keyFrameBuffer = ByteBuffer.allocateDirect(
+              configData.capacity() + info.size);
+          configData.rewind();
+          keyFrameBuffer.put(configData);
+          keyFrameBuffer.put(outputBuffer);
+          keyFrameBuffer.position(0);
+          return new OutputBufferInfo(result, keyFrameBuffer,
+              isKeyFrame, info.presentationTimeUs);
+        } else {
+          return new OutputBufferInfo(result, outputBuffer.slice(),
+              isKeyFrame, info.presentationTimeUs);
+        }
+      } else if (result == MediaCodec.INFO_OUTPUT_BUFFERS_CHANGED) {
+        outputBuffers = mediaCodec.getOutputBuffers();
+        return dequeueOutputBuffer();
+      } else if (result == MediaCodec.INFO_OUTPUT_FORMAT_CHANGED) {
+        return dequeueOutputBuffer();
+      } else if (result == MediaCodec.INFO_TRY_AGAIN_LATER) {
+        return null;
+      }
+      throw new RuntimeException("dequeueOutputBuffer: " + result);
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "dequeueOutputBuffer failed", e);
+      return new OutputBufferInfo(-1, null, false, -1);
+    }
+  }
+
+  // Release a dequeued output buffer back to the codec for re-use.  Return
+  // false if the codec is no longer operable.
+  private boolean releaseOutputBuffer(int index) {
+    checkOnMediaCodecThread();
+    try {
+      mediaCodec.releaseOutputBuffer(index, false);
+      return true;
+    } catch (IllegalStateException e) {
+      Log.e(TAG, "releaseOutputBuffer failed", e);
+      return false;
+    }
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/MediaConstraints.java b/librtc/src/main/java/org/webrtc/MediaConstraints.java
new file mode 100644
index 0000000..730df35
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/MediaConstraints.java
@@ -0,0 +1,101 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.util.LinkedList;
+import java.util.List;
+
+/**
+ * Description of media constraints for {@code MediaStream} and
+ * {@code PeerConnection}.
+ */
+public class MediaConstraints {
+  /** Simple String key/value pair. */
+  public static class KeyValuePair {
+    private final String key;
+    private final String value;
+
+    public KeyValuePair(String key, String value) {
+      this.key = key;
+      this.value = value;
+    }
+
+    public String getKey() {
+      return key;
+    }
+
+    public String getValue() {
+      return value;
+    }
+
+    public String toString() {
+      return key + ": " + value;
+    }
+
+    @Override
+    public boolean equals(Object other) {
+      if (this == other) {
+        return true;
+      }
+      if (other == null || getClass() != other.getClass()) {
+        return false;
+      }
+      KeyValuePair that = (KeyValuePair)other;
+      return key.equals(that.key) && value.equals(that.value);
+    }
+
+    @Override
+    public int hashCode() {
+      return key.hashCode() + value.hashCode();
+    }
+  }
+
+  public final List<KeyValuePair> mandatory;
+  public final List<KeyValuePair> optional;
+
+  public MediaConstraints() {
+    mandatory = new LinkedList<KeyValuePair>();
+    optional = new LinkedList<KeyValuePair>();
+  }
+
+  private static String stringifyKeyValuePairList(List<KeyValuePair> list) {
+    StringBuilder builder = new StringBuilder("[");
+    for (KeyValuePair pair : list) {
+      if (builder.length() > 1) {
+        builder.append(", ");
+      }
+      builder.append(pair.toString());
+    }
+    return builder.append("]").toString();
+  }
+
+  public String toString() {
+    return "mandatory: " + stringifyKeyValuePairList(mandatory) +
+        ", optional: " + stringifyKeyValuePairList(optional);
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/MediaSource.java b/librtc/src/main/java/org/webrtc/MediaSource.java
new file mode 100644
index 0000000..d79b462
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/MediaSource.java
@@ -0,0 +1,55 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+package org.webrtc;
+
+/** Java wrapper for a C++ MediaSourceInterface. */
+public class MediaSource {
+  /** Tracks MediaSourceInterface.SourceState */
+  public enum State {
+    INITIALIZING, LIVE, ENDED, MUTED
+  }
+
+  final long nativeSource;  // Package-protected for PeerConnectionFactory.
+
+  public MediaSource(long nativeSource) {
+    this.nativeSource = nativeSource;
+  }
+
+  public State state() {
+    return nativeState(nativeSource);
+  }
+
+  public void dispose() {
+    free(nativeSource);
+  }
+
+  private static native State nativeState(long pointer);
+
+  private static native void free(long nativeSource);
+}
diff --git a/librtc/src/main/java/org/webrtc/MediaStream.java b/librtc/src/main/java/org/webrtc/MediaStream.java
new file mode 100644
index 0000000..aa5b3e0
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/MediaStream.java
@@ -0,0 +1,115 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.util.LinkedList;
+
+/** Java wrapper for a C++ MediaStreamInterface. */
+public class MediaStream {
+  public final LinkedList<AudioTrack> audioTracks;
+  public final LinkedList<VideoTrack> videoTracks;
+  // Package-protected for PeerConnection.
+  final long nativeStream;
+
+  public MediaStream(long nativeStream) {
+    audioTracks = new LinkedList<AudioTrack>();
+    videoTracks = new LinkedList<VideoTrack>();
+    this.nativeStream = nativeStream;
+  }
+
+  public boolean addTrack(AudioTrack track) {
+    if (nativeAddAudioTrack(nativeStream, track.nativeTrack)) {
+      audioTracks.add(track);
+      return true;
+    }
+    return false;
+  }
+
+  public boolean addTrack(VideoTrack track) {
+    if (nativeAddVideoTrack(nativeStream, track.nativeTrack)) {
+      videoTracks.add(track);
+      return true;
+    }
+    return false;
+  }
+
+  public boolean removeTrack(AudioTrack track) {
+    if (nativeRemoveAudioTrack(nativeStream, track.nativeTrack)) {
+      audioTracks.remove(track);
+      return true;
+    }
+    return false;
+  }
+
+  public boolean removeTrack(VideoTrack track) {
+    if (nativeRemoveVideoTrack(nativeStream, track.nativeTrack)) {
+      videoTracks.remove(track);
+      return true;
+    }
+    return false;
+  }
+
+  public void dispose() {
+    while (!audioTracks.isEmpty()) {
+      AudioTrack track = audioTracks.getFirst();
+      removeTrack(track);
+      track.dispose();
+    }
+    while (!videoTracks.isEmpty()) {
+      VideoTrack track = videoTracks.getFirst();
+      removeTrack(track);
+      track.dispose();
+    }
+    free(nativeStream);
+  }
+
+  public String label() {
+    return nativeLabel(nativeStream);
+  }
+
+  public String toString() {
+    return "[" + label() + ":A=" + audioTracks.size() +
+        ":V=" + videoTracks.size() + "]";
+  }
+
+  private static native boolean nativeAddAudioTrack(
+      long nativeStream, long nativeAudioTrack);
+
+  private static native boolean nativeAddVideoTrack(
+      long nativeStream, long nativeVideoTrack);
+
+  private static native boolean nativeRemoveAudioTrack(
+      long nativeStream, long nativeAudioTrack);
+
+  private static native boolean nativeRemoveVideoTrack(
+      long nativeStream, long nativeVideoTrack);
+
+  private static native String nativeLabel(long nativeStream);
+
+  private static native void free(long nativeStream);
+}
diff --git a/librtc/src/main/java/org/webrtc/MediaStreamTrack.java b/librtc/src/main/java/org/webrtc/MediaStreamTrack.java
new file mode 100644
index 0000000..3965069
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/MediaStreamTrack.java
@@ -0,0 +1,86 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/** Java wrapper for a C++ MediaStreamTrackInterface. */
+public class MediaStreamTrack {
+  /** Tracks MediaStreamTrackInterface.TrackState */
+  public enum State {
+    INITIALIZING, LIVE, ENDED, FAILED
+  }
+
+  final long nativeTrack;
+
+  public MediaStreamTrack(long nativeTrack) {
+    this.nativeTrack = nativeTrack;
+  }
+
+  public String id() {
+    return nativeId(nativeTrack);
+  }
+
+  public String kind() {
+    return nativeKind(nativeTrack);
+  }
+
+  public boolean enabled() {
+    return nativeEnabled(nativeTrack);
+  }
+
+  public boolean setEnabled(boolean enable) {
+    return nativeSetEnabled(nativeTrack, enable);
+  }
+
+  public State state() {
+    return nativeState(nativeTrack);
+  }
+
+  public boolean setState(State newState) {
+    return nativeSetState(nativeTrack, newState.ordinal());
+  }
+
+  public void dispose() {
+    free(nativeTrack);
+  }
+
+  private static native String nativeId(long nativeTrack);
+
+  private static native String nativeKind(long nativeTrack);
+
+  private static native boolean nativeEnabled(long nativeTrack);
+
+  private static native boolean nativeSetEnabled(
+      long nativeTrack, boolean enabled);
+
+  private static native State nativeState(long nativeTrack);
+
+  private static native boolean nativeSetState(
+      long nativeTrack, int newState);
+
+  private static native void free(long nativeTrack);
+}
diff --git a/librtc/src/main/java/org/webrtc/PeerConnection.java b/librtc/src/main/java/org/webrtc/PeerConnection.java
new file mode 100644
index 0000000..829f0fb
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/PeerConnection.java
@@ -0,0 +1,245 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+package org.webrtc;
+
+import java.util.ArrayList;
+import java.util.LinkedList;
+import java.util.List;
+
+/**
+ * Java-land version of the PeerConnection APIs; wraps the C++ API
+ * http://www.webrtc.org/reference/native-apis, which in turn is inspired by the
+ * JS APIs: http://dev.w3.org/2011/webrtc/editor/webrtc.html and
+ * http://www.w3.org/TR/mediacapture-streams/
+ */
+public class PeerConnection {
+  static {
+    System.loadLibrary("jingle_peerconnection_so");
+  }
+
+  /** Tracks PeerConnectionInterface::IceGatheringState */
+  public enum IceGatheringState { NEW, GATHERING, COMPLETE };
+
+
+  /** Tracks PeerConnectionInterface::IceConnectionState */
+  public enum IceConnectionState {
+    NEW, CHECKING, CONNECTED, COMPLETED, FAILED, DISCONNECTED, CLOSED
+  };
+
+  /** Tracks PeerConnectionInterface::SignalingState */
+  public enum SignalingState {
+    STABLE, HAVE_LOCAL_OFFER, HAVE_LOCAL_PRANSWER, HAVE_REMOTE_OFFER,
+    HAVE_REMOTE_PRANSWER, CLOSED
+  };
+
+  /** Java version of PeerConnectionObserver. */
+  public static interface Observer {
+    /** Triggered when the SignalingState changes. */
+    public void onSignalingChange(SignalingState newState);
+
+    /** Triggered when the IceConnectionState changes. */
+    public void onIceConnectionChange(IceConnectionState newState);
+
+    /** Triggered when the ICE connection receiving status changes. */
+    public void onIceConnectionReceivingChange(boolean receiving);
+
+    /** Triggered when the IceGatheringState changes. */
+    public void onIceGatheringChange(IceGatheringState newState);
+
+    /** Triggered when a new ICE candidate has been found. */
+    public void onIceCandidate(IceCandidate candidate);
+
+    /** Triggered when media is received on a new stream from remote peer. */
+    public void onAddStream(MediaStream stream);
+
+    /** Triggered when a remote peer close a stream. */
+    public void onRemoveStream(MediaStream stream);
+
+    /** Triggered when a remote peer opens a DataChannel. */
+    public void onDataChannel(DataChannel dataChannel);
+
+    /** Triggered when renegotiation is necessary. */
+    public void onRenegotiationNeeded();
+  }
+
+  /** Java version of PeerConnectionInterface.IceServer. */
+  public static class IceServer {
+    public final String uri;
+    public final String username;
+    public final String password;
+
+    /** Convenience constructor for STUN servers. */
+    public IceServer(String uri) {
+      this(uri, "", "");
+    }
+
+    public IceServer(String uri, String username, String password) {
+      this.uri = uri;
+      this.username = username;
+      this.password = password;
+    }
+
+    public String toString() {
+      return uri + "[" + username + ":" + password + "]";
+    }
+  }
+
+  /** Java version of PeerConnectionInterface.IceTransportsType */
+  public enum IceTransportsType {
+    NONE, RELAY, NOHOST, ALL
+  };
+
+  /** Java version of PeerConnectionInterface.BundlePolicy */
+  public enum BundlePolicy {
+    BALANCED, MAXBUNDLE, MAXCOMPAT
+  };
+
+  /** Java version of PeerConnectionInterface.RtcpMuxPolicy */
+  public enum RtcpMuxPolicy {
+    NEGOTIATE, REQUIRE
+  };
+  /** Java version of PeerConnectionInterface.TcpCandidatePolicy */
+  public enum TcpCandidatePolicy {
+    ENABLED, DISABLED
+  };
+
+  /** Java version of PeerConnectionInterface.RTCConfiguration */
+  public static class RTCConfiguration {
+    public IceTransportsType iceTransportsType;
+    public List<IceServer> iceServers;
+    public BundlePolicy bundlePolicy;
+    public RtcpMuxPolicy rtcpMuxPolicy;
+    public TcpCandidatePolicy tcpCandidatePolicy;
+    public int audioJitterBufferMaxPackets;
+    public boolean audioJitterBufferFastAccelerate;
+
+    public RTCConfiguration(List<IceServer> iceServers) {
+      iceTransportsType = IceTransportsType.ALL;
+      bundlePolicy = BundlePolicy.BALANCED;
+      rtcpMuxPolicy = RtcpMuxPolicy.NEGOTIATE;
+      tcpCandidatePolicy = TcpCandidatePolicy.ENABLED;
+      this.iceServers = iceServers;
+      audioJitterBufferMaxPackets = 50;
+      audioJitterBufferFastAccelerate = false;
+    }
+  };
+
+  private final List<MediaStream> localStreams;
+  private final long nativePeerConnection;
+  private final long nativeObserver;
+
+  PeerConnection(long nativePeerConnection, long nativeObserver) {
+    this.nativePeerConnection = nativePeerConnection;
+    this.nativeObserver = nativeObserver;
+    localStreams = new LinkedList<MediaStream>();
+  }
+
+  // JsepInterface.
+  public native SessionDescription getLocalDescription();
+
+  public native SessionDescription getRemoteDescription();
+
+  public native DataChannel createDataChannel(
+      String label, DataChannel.Init init);
+
+  public native void createOffer(
+      SdpObserver observer, MediaConstraints constraints);
+
+  public native void createAnswer(
+      SdpObserver observer, MediaConstraints constraints);
+
+  public native void setLocalDescription(
+      SdpObserver observer, SessionDescription sdp);
+
+  public native void setRemoteDescription(
+      SdpObserver observer, SessionDescription sdp);
+
+  public native void setIceConnectionReceivingTimeout(int timeoutMs);
+
+  public native boolean updateIce(
+      List<IceServer> iceServers, MediaConstraints constraints);
+
+  public boolean addIceCandidate(IceCandidate candidate) {
+    return nativeAddIceCandidate(
+        candidate.sdpMid, candidate.sdpMLineIndex, candidate.sdp);
+  }
+
+  public boolean addStream(MediaStream stream) {
+    boolean ret = nativeAddLocalStream(stream.nativeStream);
+    if (!ret) {
+      return false;
+    }
+    localStreams.add(stream);
+    return true;
+  }
+
+  public void removeStream(MediaStream stream) {
+    nativeRemoveLocalStream(stream.nativeStream);
+    localStreams.remove(stream);
+  }
+
+  public boolean getStats(StatsObserver observer, MediaStreamTrack track) {
+    return nativeGetStats(observer, (track == null) ? 0 : track.nativeTrack);
+  }
+
+  // TODO(fischman): add support for DTMF-related methods once that API
+  // stabilizes.
+  public native SignalingState signalingState();
+
+  public native IceConnectionState iceConnectionState();
+
+  public native IceGatheringState iceGatheringState();
+
+  public native void close();
+
+  public void dispose() {
+    close();
+    for (MediaStream stream : localStreams) {
+      nativeRemoveLocalStream(stream.nativeStream);
+      stream.dispose();
+    }
+    localStreams.clear();
+    freePeerConnection(nativePeerConnection);
+    freeObserver(nativeObserver);
+  }
+
+  private static native void freePeerConnection(long nativePeerConnection);
+
+  private static native void freeObserver(long nativeObserver);
+
+  private native boolean nativeAddIceCandidate(
+      String sdpMid, int sdpMLineIndex, String iceCandidateSdp);
+
+  private native boolean nativeAddLocalStream(long nativeStream);
+
+  private native void nativeRemoveLocalStream(long nativeStream);
+
+  private native boolean nativeGetStats(
+      StatsObserver observer, long nativeTrack);
+}
diff --git a/librtc/src/main/java/org/webrtc/PeerConnectionFactory.java b/librtc/src/main/java/org/webrtc/PeerConnectionFactory.java
new file mode 100644
index 0000000..9c32f37
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/PeerConnectionFactory.java
@@ -0,0 +1,166 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+package org.webrtc;
+
+import java.util.List;
+
+/**
+ * Java wrapper for a C++ PeerConnectionFactoryInterface.  Main entry point to
+ * the PeerConnection API for clients.
+ */
+public class PeerConnectionFactory {
+  static {
+    System.loadLibrary("jingle_peerconnection_so");
+  }
+
+  private final long nativeFactory;
+
+  public static class Options {
+    // Keep in sync with webrtc/base/network.h!
+    static final int ADAPTER_TYPE_UNKNOWN = 0;
+    static final int ADAPTER_TYPE_ETHERNET = 1 << 0;
+    static final int ADAPTER_TYPE_WIFI = 1 << 1;
+    static final int ADAPTER_TYPE_CELLULAR = 1 << 2;
+    static final int ADAPTER_TYPE_VPN = 1 << 3;
+    static final int ADAPTER_TYPE_LOOPBACK = 1 << 4;
+
+    public int networkIgnoreMask;
+    public boolean disableEncryption;
+  }
+
+  // |context| is an android.content.Context object, but we keep it untyped here
+  // to allow building on non-Android platforms.
+  // Callers may specify either |initializeAudio| or |initializeVideo| as false
+  // to skip initializing the respective engine (and avoid the need for the
+  // respective permissions).
+  // |renderEGLContext| can be provided to suport HW video decoding to
+  // texture and will be used to create a shared EGL context on video
+  // decoding thread.
+  public static native boolean initializeAndroidGlobals(
+      Object context, boolean initializeAudio, boolean initializeVideo,
+      boolean vp8HwAcceleration, Object renderEGLContext);
+
+  // Field trial initialization. Must be called before PeerConnectionFactory
+  // is created.
+  public static native void initializeFieldTrials(String fieldTrialsInitString);
+
+  public PeerConnectionFactory() {
+    nativeFactory = nativeCreatePeerConnectionFactory();
+    if (nativeFactory == 0) {
+      throw new RuntimeException("Failed to initialize PeerConnectionFactory!");
+    }
+  }
+
+  public PeerConnection createPeerConnection(
+      PeerConnection.RTCConfiguration rtcConfig,
+      MediaConstraints constraints,
+      PeerConnection.Observer observer) {
+    long nativeObserver = nativeCreateObserver(observer);
+    if (nativeObserver == 0) {
+      return null;
+    }
+    long nativePeerConnection = nativeCreatePeerConnection(
+        nativeFactory, rtcConfig, constraints, nativeObserver);
+    if (nativePeerConnection == 0) {
+      return null;
+    }
+    return new PeerConnection(nativePeerConnection, nativeObserver);
+  }
+
+  public PeerConnection createPeerConnection(
+      List<PeerConnection.IceServer> iceServers,
+      MediaConstraints constraints,
+      PeerConnection.Observer observer) {
+    PeerConnection.RTCConfiguration rtcConfig =
+        new PeerConnection.RTCConfiguration(iceServers);
+    return createPeerConnection(rtcConfig, constraints, observer);
+  }
+
+  public MediaStream createLocalMediaStream(String label) {
+    return new MediaStream(
+        nativeCreateLocalMediaStream(nativeFactory, label));
+  }
+
+  public VideoSource createVideoSource(
+      VideoCapturer capturer, MediaConstraints constraints) {
+    return new VideoSource(nativeCreateVideoSource(
+        nativeFactory, capturer.takeNativeVideoCapturer(), constraints));
+  }
+
+  public VideoTrack createVideoTrack(String id, VideoSource source) {
+    return new VideoTrack(nativeCreateVideoTrack(
+        nativeFactory, id, source.nativeSource));
+  }
+
+  public AudioSource createAudioSource(MediaConstraints constraints) {
+    return new AudioSource(nativeCreateAudioSource(nativeFactory, constraints));
+  }
+
+  public AudioTrack createAudioTrack(String id, AudioSource source) {
+    return new AudioTrack(nativeCreateAudioTrack(
+        nativeFactory, id, source.nativeSource));
+  }
+
+  public void setOptions(Options options) {
+    nativeSetOptions(nativeFactory, options);
+  }
+
+  public void dispose() {
+    freeFactory(nativeFactory);
+  }
+
+  public native void nativeSetOptions(long nativeFactory, Options options);
+
+  private static native long nativeCreatePeerConnectionFactory();
+
+  private static native long nativeCreateObserver(
+      PeerConnection.Observer observer);
+
+  private static native long nativeCreatePeerConnection(
+      long nativeFactory, PeerConnection.RTCConfiguration rtcConfig,
+      MediaConstraints constraints, long nativeObserver);
+
+  private static native long nativeCreateLocalMediaStream(
+      long nativeFactory, String label);
+
+  private static native long nativeCreateVideoSource(
+      long nativeFactory, long nativeVideoCapturer,
+      MediaConstraints constraints);
+
+  private static native long nativeCreateVideoTrack(
+      long nativeFactory, String id, long nativeVideoSource);
+
+  private static native long nativeCreateAudioSource(
+      long nativeFactory, MediaConstraints constraints);
+
+  private static native long nativeCreateAudioTrack(
+      long nativeFactory, String id, long nativeSource);
+
+  private static native void freeFactory(long nativeFactory);
+}
diff --git a/librtc/src/main/java/org/webrtc/SdpObserver.java b/librtc/src/main/java/org/webrtc/SdpObserver.java
new file mode 100644
index 0000000..779bf1b
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/SdpObserver.java
@@ -0,0 +1,43 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/** Interface for observing SDP-related events. */
+public interface SdpObserver {
+  /** Called on success of Create{Offer,Answer}(). */
+  public void onCreateSuccess(SessionDescription sdp);
+
+  /** Called on success of Set{Local,Remote}Description(). */
+  public void onSetSuccess();
+
+  /** Called on error of Create{Offer,Answer}(). */
+  public void onCreateFailure(String error);
+
+  /** Called on error of Set{Local,Remote}Description(). */
+  public void onSetFailure(String error);
+}
diff --git a/librtc/src/main/java/org/webrtc/SessionDescription.java b/librtc/src/main/java/org/webrtc/SessionDescription.java
new file mode 100644
index 0000000..c3dfcd4
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/SessionDescription.java
@@ -0,0 +1,57 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+package org.webrtc;
+
+/**
+ * Description of an RFC 4566 Session.
+ * SDPs are passed as serialized Strings in Java-land and are materialized
+ * to SessionDescriptionInterface as appropriate in the JNI layer.
+ */
+public class SessionDescription {
+  /** Java-land enum version of SessionDescriptionInterface's type() string. */
+  public static enum Type {
+    OFFER, PRANSWER, ANSWER;
+
+    public String canonicalForm() {
+      return name().toLowerCase();
+    }
+
+    public static Type fromCanonicalForm(String canonical) {
+      return Type.valueOf(Type.class, canonical.toUpperCase());
+    }
+  }
+
+  public final Type type;
+  public final String description;
+
+  public SessionDescription(Type type, String description) {
+    this.type = type;
+    this.description = description;
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/StatsObserver.java b/librtc/src/main/java/org/webrtc/StatsObserver.java
new file mode 100644
index 0000000..99223ad
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/StatsObserver.java
@@ -0,0 +1,34 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/** Interface for observing Stats reports (see webrtc::StatsObservers). */
+public interface StatsObserver {
+  /** Called when the reports are ready.*/
+  public void onComplete(StatsReport[] reports);
+}
diff --git a/librtc/src/main/java/org/webrtc/StatsReport.java b/librtc/src/main/java/org/webrtc/StatsReport.java
new file mode 100644
index 0000000..6e32543
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/StatsReport.java
@@ -0,0 +1,72 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/** Java version of webrtc::StatsReport. */
+public class StatsReport {
+
+  /** Java version of webrtc::StatsReport::Value. */
+  public static class Value {
+    public final String name;
+    public final String value;
+
+    public Value(String name, String value) {
+      this.name = name;
+      this.value = value;
+    }
+
+    public String toString() {
+      StringBuilder builder = new StringBuilder();
+      builder.append("[").append(name).append(": ").append(value).append("]");
+      return builder.toString();
+    }
+  }
+
+  public final String id;
+  public final String type;
+  // Time since 1970-01-01T00:00:00Z in milliseconds.
+  public final double timestamp;
+  public final Value[] values;
+
+  public StatsReport(String id, String type, double timestamp, Value[] values) {
+    this.id = id;
+    this.type = type;
+    this.timestamp = timestamp;
+    this.values = values;
+  }
+
+  public String toString() {
+    StringBuilder builder = new StringBuilder();
+    builder.append("id: ").append(id).append(", type: ").append(type)
+        .append(", timestamp: ").append(timestamp).append(", values: ");
+    for (int i = 0; i < values.length; ++i) {
+      builder.append(values[i].toString()).append(", ");
+    }
+    return builder.toString();
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/VideoCapturer.java b/librtc/src/main/java/org/webrtc/VideoCapturer.java
new file mode 100644
index 0000000..158cc34
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/VideoCapturer.java
@@ -0,0 +1,70 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+/** Java version of cricket::VideoCapturer. */
+public class VideoCapturer {
+  private long nativeVideoCapturer;
+
+  protected VideoCapturer() {
+  }
+
+  public static VideoCapturer create(String deviceName) {
+    Object capturer = nativeCreateVideoCapturer(deviceName);
+    if (capturer != null)
+      return (VideoCapturer) (capturer);
+    return null;
+  }
+
+  // Sets |nativeCapturer| to be owned by VideoCapturer.
+  protected void setNativeCapturer(long nativeCapturer) {
+    this.nativeVideoCapturer = nativeCapturer;
+  }
+
+  // Package-visible for PeerConnectionFactory.
+  long takeNativeVideoCapturer() {
+    if (nativeVideoCapturer == 0) {
+      throw new RuntimeException("Capturer can only be taken once!");
+    }
+    long ret = nativeVideoCapturer;
+    nativeVideoCapturer = 0;
+    return ret;
+  }
+
+  public void dispose() {
+    // No-op iff this capturer is owned by a source (see comment on
+    // PeerConnectionFactoryInterface::CreateVideoSource()).
+    if (nativeVideoCapturer != 0) {
+      free(nativeVideoCapturer);
+    }
+  }
+
+  private static native Object nativeCreateVideoCapturer(String deviceName);
+
+  private static native void free(long nativeVideoCapturer);
+}
diff --git a/librtc/src/main/java/org/webrtc/VideoCapturerAndroid.java b/librtc/src/main/java/org/webrtc/VideoCapturerAndroid.java
new file mode 100644
index 0000000..cfce19e
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/VideoCapturerAndroid.java
@@ -0,0 +1,1013 @@
+/*
+ * libjingle
+ * Copyright 2015 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import static java.lang.Math.abs;
+import static java.lang.Math.ceil;
+
+import android.content.Context;
+import android.graphics.ImageFormat;
+import android.graphics.SurfaceTexture;
+import android.hardware.Camera;
+import android.hardware.Camera.PreviewCallback;
+import android.opengl.GLES11Ext;
+import android.opengl.GLES20;
+import android.os.Handler;
+import android.os.Looper;
+import android.os.SystemClock;
+import android.util.Log;
+import android.view.Surface;
+import android.view.WindowManager;
+
+import org.json.JSONArray;
+import org.json.JSONException;
+import org.json.JSONObject;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.IdentityHashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.Exchanger;
+import java.util.concurrent.TimeUnit;
+
+// Android specific implementation of VideoCapturer.
+// An instance of this class can be created by an application using
+// VideoCapturerAndroid.create();
+// This class extends VideoCapturer with a method to easily switch between the
+// front and back camera. It also provides methods for enumerating valid device
+// names.
+//
+// Threading notes: this class is called from C++ code, and from Camera
+// Java callbacks.  Since these calls happen on different threads,
+// the entry points to this class are all synchronized.  This shouldn't present
+// a performance bottleneck because only onPreviewFrame() is called more than
+// once (and is called serially on a single thread), so the lock should be
+// uncontended.  Note that each of these synchronized methods must check
+// |camera| for null to account for having possibly waited for stopCapture() to
+// complete.
+@SuppressWarnings("deprecation")
+public class VideoCapturerAndroid extends VideoCapturer implements PreviewCallback {
+  private final static String TAG = "VideoCapturerAndroid";
+  private final static int CAMERA_OBSERVER_PERIOD_MS = 5000;
+
+  private Camera camera;  // Only non-null while capturing.
+  private CameraThread cameraThread;
+  private Handler cameraThreadHandler;
+  private Context applicationContext;
+  private int id;
+  private Camera.CameraInfo info;
+  private SurfaceTexture cameraSurfaceTexture;
+  private int[] cameraGlTextures = null;
+  private final FramePool videoBuffers = new FramePool();
+  // Remember the requested format in case we want to switch cameras.
+  private int requestedWidth;
+  private int requestedHeight;
+  private int requestedFramerate;
+  // The capture format will be the closest supported format to the requested format.
+  private CaptureFormat captureFormat;
+  private int cameraFramesCount;
+  private int captureBuffersCount;
+  private volatile boolean pendingCameraSwitch;
+  private CapturerObserver frameObserver = null;
+  private CameraErrorHandler errorHandler = null;
+  // List of formats supported by all cameras. This list is filled once in order
+  // to be able to switch cameras.
+  private static List<List<CaptureFormat>> supportedFormats;
+
+  // Camera error callback.
+  private final Camera.ErrorCallback cameraErrorCallback =
+      new Camera.ErrorCallback() {
+    @Override
+    public void onError(int error, Camera camera) {
+      String errorMessage;
+      if (error == Camera.CAMERA_ERROR_SERVER_DIED) {
+        errorMessage = "Camera server died!";
+      } else {
+        errorMessage = "Camera error: " + error;
+      }
+      Log.e(TAG, errorMessage);
+      if (errorHandler != null) {
+        errorHandler.onCameraError(errorMessage);
+      }
+    }
+  };
+
+  // Camera observer - monitors camera framerate and amount of available
+  // camera buffers. Observer is excecuted on camera thread.
+  private final Runnable cameraObserver = new Runnable() {
+    @Override
+    public void run() {
+      int cameraFps = (cameraFramesCount * 1000 + CAMERA_OBSERVER_PERIOD_MS / 2)
+          / CAMERA_OBSERVER_PERIOD_MS;
+      double averageCaptureBuffersCount = 0;
+      if (cameraFramesCount > 0) {
+        averageCaptureBuffersCount =
+            (double)captureBuffersCount / cameraFramesCount;
+      }
+      Log.d(TAG, "Camera fps: " + cameraFps + ". CaptureBuffers: " +
+          String.format("%.1f", averageCaptureBuffersCount) +
+          ". Pending buffers: " + videoBuffers.pendingFramesTimeStamps());
+      if (cameraFramesCount == 0) {
+        Log.e(TAG, "Camera freezed.");
+        if (errorHandler != null) {
+          errorHandler.onCameraError("Camera failure.");
+        }
+      } else {
+        cameraFramesCount = 0;
+        captureBuffersCount = 0;
+        if (cameraThreadHandler != null) {
+          cameraThreadHandler.postDelayed(this, CAMERA_OBSERVER_PERIOD_MS);
+        }
+      }
+    }
+  };
+
+  // Camera error handler - invoked when camera stops receiving frames
+  // or any camera exception happens on camera thread.
+  public static interface CameraErrorHandler {
+    public void onCameraError(String errorDescription);
+  }
+
+  // Returns device names that can be used to create a new VideoCapturerAndroid.
+  public static String[] getDeviceNames() {
+    String[] names = new String[Camera.getNumberOfCameras()];
+    for (int i = 0; i < Camera.getNumberOfCameras(); ++i) {
+      names[i] = getDeviceName(i);
+    }
+    return names;
+  }
+
+  // Returns number of cameras on device.
+  public static int getDeviceCount() {
+    return Camera.getNumberOfCameras();
+  }
+
+  // Returns the name of the camera with camera index. Returns null if the
+  // camera can not be used.
+  public static String getDeviceName(int index) {
+    Camera.CameraInfo info = new Camera.CameraInfo();
+    try {
+      Camera.getCameraInfo(index, info);
+    } catch (Exception e) {
+      Log.e(TAG, "getCameraInfo failed on index " + index,e);
+      return null;
+    }
+
+    String facing =
+        (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT) ? "front" : "back";
+    return "Camera " + index + ", Facing " + facing
+        + ", Orientation " + info.orientation;
+  }
+
+  // Returns the name of the front facing camera. Returns null if the
+  // camera can not be used or does not exist.
+  public static String getNameOfFrontFacingDevice() {
+    for (int i = 0; i < Camera.getNumberOfCameras(); ++i) {
+      Camera.CameraInfo info = new Camera.CameraInfo();
+      try {
+        Camera.getCameraInfo(i, info);
+        if (info.facing == Camera.CameraInfo.CAMERA_FACING_FRONT)
+          return getDeviceName(i);
+      } catch (Exception e) {
+        Log.e(TAG, "getCameraInfo failed on index " + i, e);
+      }
+    }
+    return null;
+  }
+
+  // Returns the name of the back facing camera. Returns null if the
+  // camera can not be used or does not exist.
+  public static String getNameOfBackFacingDevice() {
+    for (int i = 0; i < Camera.getNumberOfCameras(); ++i) {
+      Camera.CameraInfo info = new Camera.CameraInfo();
+      try {
+        Camera.getCameraInfo(i, info);
+        if (info.facing == Camera.CameraInfo.CAMERA_FACING_BACK)
+          return getDeviceName(i);
+      } catch (Exception e) {
+        Log.e(TAG, "getCameraInfo failed on index " + i, e);
+      }
+    }
+    return null;
+  }
+
+  public static VideoCapturerAndroid create(String name,
+      CameraErrorHandler errorHandler) {
+    VideoCapturer capturer = VideoCapturer.create(name);
+    if (capturer != null) {
+      VideoCapturerAndroid capturerAndroid = (VideoCapturerAndroid) capturer;
+      capturerAndroid.errorHandler = errorHandler;
+      return capturerAndroid;
+    }
+    return null;
+  }
+
+  // Switch camera to the next valid camera id. This can only be called while
+  // the camera is running.
+  // Returns true on success. False if the next camera does not support the
+  // current resolution.
+  public synchronized boolean switchCamera(final Runnable switchDoneEvent) {
+    if (Camera.getNumberOfCameras() < 2 )
+      return false;
+
+    if (cameraThread == null) {
+      Log.e(TAG, "Camera has not been started");
+      return false;
+    }
+    if (pendingCameraSwitch) {
+      // Do not handle multiple camera switch request to avoid blocking
+      // camera thread by handling too many switch request from a queue.
+      Log.w(TAG, "Ignoring camera switch request.");
+      return false;
+    }
+
+    pendingCameraSwitch = true;
+    id = (id + 1) % Camera.getNumberOfCameras();
+    cameraThreadHandler.post(new Runnable() {
+      @Override public void run() {
+        switchCameraOnCameraThread(switchDoneEvent);
+      }
+    });
+    return true;
+  }
+
+  // Requests a new output format from the video capturer. Captured frames
+  // by the camera will be scaled/or dropped by the video capturer.
+  public synchronized void onOutputFormatRequest(
+      final int width, final int height, final int fps) {
+    if (cameraThreadHandler == null) {
+      Log.e(TAG, "Calling onOutputFormatRequest() for already stopped camera.");
+      return;
+    }
+    cameraThreadHandler.post(new Runnable() {
+      @Override public void run() {
+        onOutputFormatRequestOnCameraThread(width, height, fps);
+      }
+    });
+  }
+
+  // Reconfigure the camera to capture in a new format. This should only be called while the camera
+  // is running.
+  public synchronized void changeCaptureFormat(
+      final int width, final int height, final int framerate) {
+    if (cameraThreadHandler == null) {
+      Log.e(TAG, "Calling changeCaptureFormat() for already stopped camera.");
+      return;
+    }
+    cameraThreadHandler.post(new Runnable() {
+      @Override public void run() {
+        startPreviewOnCameraThread(width, height, framerate);
+      }
+    });
+  }
+
+  public synchronized List<CaptureFormat> getSupportedFormats() {
+    return supportedFormats.get(id);
+  }
+
+  private VideoCapturerAndroid() {
+    Log.d(TAG, "VideoCapturerAndroid");
+  }
+
+  // Called by native code.
+  // Enumerates resolution and frame rates for all cameras to be able to switch
+  // cameras. Initializes local variables for the camera named |deviceName| and
+  // starts a thread to be used for capturing.
+  // If deviceName is empty, the first available device is used in order to be
+  // compatible with the generic VideoCapturer class.
+  synchronized boolean init(String deviceName) {
+    Log.d(TAG, "init: " + deviceName);
+    if (deviceName == null || !initStatics())
+      return false;
+
+    boolean foundDevice = false;
+    if (deviceName.isEmpty()) {
+      this.id = 0;
+      foundDevice = true;
+    } else {
+      for (int i = 0; i < Camera.getNumberOfCameras(); ++i) {
+        String existing_device = getDeviceName(i);
+        if (existing_device != null && deviceName.equals(existing_device)) {
+          this.id = i;
+          foundDevice = true;
+        }
+      }
+    }
+    return foundDevice;
+  }
+
+  private static boolean initStatics() {
+    if (supportedFormats != null)
+      return true;
+    try {
+      Log.d(TAG, "Get supported formats.");
+      supportedFormats =
+          new ArrayList<List<CaptureFormat>>(Camera.getNumberOfCameras());
+      // Start requesting supported formats from camera with the highest index
+      // (back camera) first. If it fails then likely camera is in bad state.
+      for (int i = Camera.getNumberOfCameras() - 1; i >= 0; i--) {
+        ArrayList<CaptureFormat> supportedFormat = getSupportedFormats(i);
+        if (supportedFormat.size() == 0) {
+          Log.e(TAG, "Fail to get supported formats for camera " + i);
+          supportedFormats = null;
+          return false;
+        }
+        supportedFormats.add(supportedFormat);
+      }
+      // Reverse the list since it is filled in reverse order.
+      Collections.reverse(supportedFormats);
+      Log.d(TAG, "Get supported formats done.");
+      return true;
+    } catch (Exception e) {
+      supportedFormats = null;
+      Log.e(TAG, "InitStatics failed",e);
+    }
+    return false;
+  }
+
+  String getSupportedFormatsAsJson() throws JSONException {
+    return getSupportedFormatsAsJson(id);
+  }
+
+  public static class CaptureFormat {
+    public final int width;
+    public final int height;
+    public final int maxFramerate;
+    public final int minFramerate;
+    // TODO(hbos): If VideoCapturerAndroid.startCapture is updated to support
+    // other image formats then this needs to be updated and
+    // VideoCapturerAndroid.getSupportedFormats need to return CaptureFormats of
+    // all imageFormats.
+    public final int imageFormat = ImageFormat.YV12;
+
+    public CaptureFormat(int width, int height, int minFramerate,
+        int maxFramerate) {
+      this.width = width;
+      this.height = height;
+      this.minFramerate = minFramerate;
+      this.maxFramerate = maxFramerate;
+    }
+
+    // Calculates the frame size of this capture format.
+    public int frameSize() {
+      return frameSize(width, height, imageFormat);
+    }
+
+    // Calculates the frame size of the specified image format. Currently only
+    // supporting ImageFormat.YV12. The YV12's stride is the closest rounded up
+    // multiple of 16 of the width and width and height are always even.
+    // Android guarantees this:
+    // http://developer.android.com/reference/android/hardware/Camera.Parameters.html#setPreviewFormat%28int%29
+    public static int frameSize(int width, int height, int imageFormat) {
+      if (imageFormat != ImageFormat.YV12) {
+        throw new UnsupportedOperationException("Don't know how to calculate "
+            + "the frame size of non-YV12 image formats.");
+      }
+      int yStride = roundUp(width, 16);
+      int uvStride = roundUp(yStride / 2, 16);
+      int ySize = yStride * height;
+      int uvSize = uvStride * height / 2;
+      return ySize + uvSize * 2;
+    }
+
+    // Rounds up |x| to the closest value that is a multiple of |alignment|.
+    private static int roundUp(int x, int alignment) {
+      return (int)ceil(x / (double)alignment) * alignment;
+    }
+
+    @Override
+    public String toString() {
+      return width + "x" + height + "@[" + minFramerate + ":" + maxFramerate + "]";
+    }
+
+    @Override
+    public boolean equals(Object that) {
+      if (!(that instanceof CaptureFormat)) {
+        return false;
+      }
+      final CaptureFormat c = (CaptureFormat) that;
+      return width == c.width && height == c.height && maxFramerate == c.maxFramerate
+          && minFramerate == c.minFramerate;
+    }
+  }
+
+  private static String getSupportedFormatsAsJson(int id) throws JSONException {
+    List<CaptureFormat> formats = supportedFormats.get(id);
+    JSONArray json_formats = new JSONArray();
+    for (CaptureFormat format : formats) {
+      JSONObject json_format = new JSONObject();
+      json_format.put("width", format.width);
+      json_format.put("height", format.height);
+      json_format.put("framerate", (format.maxFramerate + 999) / 1000);
+      json_formats.put(json_format);
+    }
+    Log.d(TAG, "Supported formats for camera " + id + ": "
+        +  json_formats.toString(2));
+    return json_formats.toString();
+  }
+
+  // Returns a list of CaptureFormat for the camera with index id.
+  static ArrayList<CaptureFormat> getSupportedFormats(int id) {
+    ArrayList<CaptureFormat> formatList = new ArrayList<CaptureFormat>();
+
+    Camera camera;
+    try {
+      Log.d(TAG, "Opening camera " + id);
+      camera = Camera.open(id);
+    } catch (Exception e) {
+      Log.e(TAG, "Open camera failed on id " + id, e);
+      return formatList;
+    }
+
+    try {
+      Camera.Parameters parameters;
+      parameters = camera.getParameters();
+      // getSupportedPreviewFpsRange returns a sorted list.
+      List<int[]> listFpsRange = parameters.getSupportedPreviewFpsRange();
+      int[] range = {0, 0};
+      if (listFpsRange != null)
+        range = listFpsRange.get(listFpsRange.size() -1);
+
+      List<Camera.Size> supportedSizes = parameters.getSupportedPreviewSizes();
+      for (Camera.Size size : supportedSizes) {
+        formatList.add(new CaptureFormat(size.width, size.height,
+            range[Camera.Parameters.PREVIEW_FPS_MIN_INDEX],
+            range[Camera.Parameters.PREVIEW_FPS_MAX_INDEX]));
+      }
+    } catch (Exception e) {
+      Log.e(TAG, "getSupportedFormats failed on id " + id, e);
+    }
+    camera.release();
+    camera = null;
+    return formatList;
+  }
+
+  private class CameraThread extends Thread {
+    private Exchanger<Handler> handlerExchanger;
+    public CameraThread(Exchanger<Handler> handlerExchanger) {
+      this.handlerExchanger = handlerExchanger;
+    }
+
+    @Override public void run() {
+      Looper.prepare();
+      exchange(handlerExchanger, new Handler());
+      Looper.loop();
+    }
+  }
+
+  // Called by native code.  Returns true if capturer is started.
+  //
+  // Note that this actually opens the camera, and Camera callbacks run on the
+  // thread that calls open(), so this is done on the CameraThread.  Since the
+  // API needs a synchronous success return value we wait for the result.
+  synchronized void startCapture(
+      final int width, final int height, final int framerate,
+      final Context applicationContext, final CapturerObserver frameObserver) {
+    Log.d(TAG, "startCapture requested: " + width + "x" + height
+        + "@" + framerate);
+    if (applicationContext == null) {
+      throw new RuntimeException("applicationContext not set.");
+    }
+    if (frameObserver == null) {
+      throw new RuntimeException("frameObserver not set.");
+    }
+    if (cameraThreadHandler != null) {
+      throw new RuntimeException("Camera has already been started.");
+    }
+
+    Exchanger<Handler> handlerExchanger = new Exchanger<Handler>();
+    cameraThread = new CameraThread(handlerExchanger);
+    cameraThread.start();
+    cameraThreadHandler = exchange(handlerExchanger, null);
+    cameraThreadHandler.post(new Runnable() {
+      @Override public void run() {
+        startCaptureOnCameraThread(width, height, framerate, frameObserver,
+            applicationContext);
+      }
+    });
+  }
+
+  private void startCaptureOnCameraThread(
+      int width, int height, int framerate, CapturerObserver frameObserver,
+      Context applicationContext) {
+    Throwable error = null;
+    this.applicationContext = applicationContext;
+    this.frameObserver = frameObserver;
+    try {
+      Log.d(TAG, "Opening camera " + id);
+      camera = Camera.open(id);
+      info = new Camera.CameraInfo();
+      Camera.getCameraInfo(id, info);
+      // No local renderer (we only care about onPreviewFrame() buffers, not a
+      // directly-displayed UI element).  Camera won't capture without
+      // setPreview{Texture,Display}, so we create a SurfaceTexture and hand
+      // it over to Camera, but never listen for frame-ready callbacks,
+      // and never call updateTexImage on it.
+      try {
+        cameraSurfaceTexture = null;
+
+        cameraGlTextures = new int[1];
+        // Generate one texture pointer and bind it as an external texture.
+        GLES20.glGenTextures(1, cameraGlTextures, 0);
+        GLES20.glBindTexture(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            cameraGlTextures[0]);
+        GLES20.glTexParameterf(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_LINEAR);
+        GLES20.glTexParameterf(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR);
+        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameteri(GLES11Ext.GL_TEXTURE_EXTERNAL_OES,
+            GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
+
+        cameraSurfaceTexture = new SurfaceTexture(cameraGlTextures[0]);
+        cameraSurfaceTexture.setOnFrameAvailableListener(null);
+
+        camera.setPreviewTexture(cameraSurfaceTexture);
+      } catch (IOException e) {
+        Log.e(TAG, "setPreviewTexture failed", error);
+        throw new RuntimeException(e);
+      }
+
+      Log.d(TAG, "Camera orientation: " + info.orientation +
+          " .Device orientation: " + getDeviceOrientation());
+      camera.setErrorCallback(cameraErrorCallback);
+      startPreviewOnCameraThread(width, height, framerate);
+      frameObserver.OnCapturerStarted(true);
+
+      // Start camera observer.
+      cameraFramesCount = 0;
+      captureBuffersCount = 0;
+      cameraThreadHandler.postDelayed(cameraObserver, CAMERA_OBSERVER_PERIOD_MS);
+      return;
+    } catch (RuntimeException e) {
+      error = e;
+    }
+    Log.e(TAG, "startCapture failed", error);
+    stopCaptureOnCameraThread();
+    cameraThreadHandler = null;
+    frameObserver.OnCapturerStarted(false);
+    if (errorHandler != null) {
+      errorHandler.onCameraError("Camera can not be started.");
+    }
+    return;
+  }
+
+  // (Re)start preview with the closest supported format to |width| x |height| @ |framerate|.
+  private void startPreviewOnCameraThread(int width, int height, int framerate) {
+    Log.d(TAG, "startPreviewOnCameraThread requested: " + width + "x" + height + "@" + framerate);
+    if (camera == null) {
+      Log.e(TAG, "Calling startPreviewOnCameraThread on stopped camera.");
+      return;
+    }
+
+    requestedWidth = width;
+    requestedHeight = height;
+    requestedFramerate = framerate;
+
+    // Find closest supported format for |width| x |height| @ |framerate|.
+    final Camera.Parameters parameters = camera.getParameters();
+    final int[] range = getFramerateRange(parameters, framerate * 1000);
+    final Camera.Size previewSize =
+        getClosestSupportedSize(parameters.getSupportedPreviewSizes(), width, height);
+    final CaptureFormat captureFormat = new CaptureFormat(
+        previewSize.width, previewSize.height,
+        range[Camera.Parameters.PREVIEW_FPS_MIN_INDEX],
+        range[Camera.Parameters.PREVIEW_FPS_MAX_INDEX]);
+
+    // Check if we are already using this capture format, then we don't need to do anything.
+    if (captureFormat.equals(this.captureFormat)) {
+      return;
+    }
+
+    // Update camera parameters.
+    Log.d(TAG, "isVideoStabilizationSupported: " +
+        parameters.isVideoStabilizationSupported());
+    if (parameters.isVideoStabilizationSupported()) {
+      parameters.setVideoStabilization(true);
+    }
+    // Note: setRecordingHint(true) actually decrease frame rate on N5.
+    // parameters.setRecordingHint(true);
+    if (captureFormat.maxFramerate > 0) {
+      parameters.setPreviewFpsRange(captureFormat.minFramerate, captureFormat.maxFramerate);
+    }
+    parameters.setPreviewSize(captureFormat.width, captureFormat.height);
+    parameters.setPreviewFormat(captureFormat.imageFormat);
+    // Picture size is for taking pictures and not for preview/video, but we need to set it anyway
+    // as a workaround for an aspect ratio problem on Nexus 7.
+    final Camera.Size pictureSize =
+        getClosestSupportedSize(parameters.getSupportedPictureSizes(), width, height);
+    parameters.setPictureSize(pictureSize.width, pictureSize.height);
+
+    // Temporarily stop preview if it's already running.
+    if (this.captureFormat != null) {
+      camera.stopPreview();
+      // Calling |setPreviewCallbackWithBuffer| with null should clear the internal camera buffer
+      // queue, but sometimes we receive a frame with the old resolution after this call anyway.
+      camera.setPreviewCallbackWithBuffer(null);
+    }
+
+    // (Re)start preview.
+    Log.d(TAG, "Start capturing: " + captureFormat);
+    this.captureFormat = captureFormat;
+    camera.setParameters(parameters);
+    videoBuffers.queueCameraBuffers(captureFormat.frameSize(), camera);
+    camera.setPreviewCallbackWithBuffer(this);
+    camera.startPreview();
+  }
+
+  // Called by native code.  Returns true when camera is known to be stopped.
+  synchronized void stopCapture() throws InterruptedException {
+    if (cameraThreadHandler == null) {
+      Log.e(TAG, "Calling stopCapture() for already stopped camera.");
+      return;
+    }
+    Log.d(TAG, "stopCapture");
+    cameraThreadHandler.post(new Runnable() {
+        @Override public void run() {
+          stopCaptureOnCameraThread();
+        }
+    });
+    cameraThread.join();
+    cameraThreadHandler = null;
+    Log.d(TAG, "stopCapture done");
+  }
+
+  private void stopCaptureOnCameraThread() {
+    doStopCaptureOnCameraThread();
+    Looper.myLooper().quit();
+    return;
+  }
+
+  private void doStopCaptureOnCameraThread() {
+    Log.d(TAG, "stopCaptureOnCameraThread");
+    if (camera == null) {
+      return;
+    }
+    try {
+      cameraThreadHandler.removeCallbacks(cameraObserver);
+      Log.d(TAG, "Stop preview.");
+      camera.stopPreview();
+      camera.setPreviewCallbackWithBuffer(null);
+      videoBuffers.stopReturnBuffersToCamera();
+      captureFormat = null;
+
+      camera.setPreviewTexture(null);
+      cameraSurfaceTexture = null;
+      if (cameraGlTextures != null) {
+        GLES20.glDeleteTextures(1, cameraGlTextures, 0);
+        cameraGlTextures = null;
+      }
+
+      Log.d(TAG, "Release camera.");
+      camera.release();
+      camera = null;
+    } catch (IOException e) {
+      Log.e(TAG, "Failed to stop camera", e);
+    }
+  }
+
+  private void switchCameraOnCameraThread(Runnable switchDoneEvent) {
+    Log.d(TAG, "switchCameraOnCameraThread");
+
+    doStopCaptureOnCameraThread();
+    startCaptureOnCameraThread(requestedWidth, requestedHeight, requestedFramerate, frameObserver,
+        applicationContext);
+    pendingCameraSwitch = false;
+    Log.d(TAG, "switchCameraOnCameraThread done");
+    if (switchDoneEvent != null) {
+      switchDoneEvent.run();
+    }
+  }
+
+  private void onOutputFormatRequestOnCameraThread(
+      int width, int height, int fps) {
+    if (camera == null) {
+      return;
+    }
+    Log.d(TAG, "onOutputFormatRequestOnCameraThread: " + width + "x" + height +
+        "@" + fps);
+    frameObserver.OnOutputFormatRequest(width, height, fps);
+  }
+
+  synchronized void returnBuffer(final long timeStamp) {
+    if (cameraThreadHandler == null) {
+      // The camera has been stopped.
+      videoBuffers.returnBuffer(timeStamp);
+      return;
+    }
+    cameraThreadHandler.post(new Runnable() {
+      @Override public void run() {
+        videoBuffers.returnBuffer(timeStamp);
+      }
+    });
+  }
+
+  private int getDeviceOrientation() {
+    int orientation = 0;
+
+    WindowManager wm = (WindowManager) applicationContext.getSystemService(
+        Context.WINDOW_SERVICE);
+    switch(wm.getDefaultDisplay().getRotation()) {
+      case Surface.ROTATION_90:
+        orientation = 90;
+        break;
+      case Surface.ROTATION_180:
+        orientation = 180;
+        break;
+      case Surface.ROTATION_270:
+        orientation = 270;
+        break;
+      case Surface.ROTATION_0:
+      default:
+        orientation = 0;
+        break;
+    }
+    return orientation;
+  }
+
+  // Helper class for finding the closest supported format for the two functions below.
+  private static abstract class ClosestComparator<T> implements Comparator<T> {
+    // Difference between supported and requested parameter.
+    abstract int diff(T supportedParameter);
+
+    @Override
+    public int compare(T t1, T t2) {
+      return diff(t1) - diff(t2);
+    }
+  }
+
+  private static int[] getFramerateRange(Camera.Parameters parameters, final int framerate) {
+    List<int[]> listFpsRange = parameters.getSupportedPreviewFpsRange();
+    if (listFpsRange.isEmpty()) {
+      Log.w(TAG, "No supported preview fps range");
+      return new int[]{0, 0};
+    }
+    return Collections.min(listFpsRange,
+        new ClosestComparator<int[]>() {
+          @Override int diff(int[] range) {
+            return abs(framerate - range[Camera.Parameters.PREVIEW_FPS_MIN_INDEX])
+                + abs(framerate - range[Camera.Parameters.PREVIEW_FPS_MAX_INDEX]);
+          }
+     });
+  }
+
+  private static Camera.Size getClosestSupportedSize(
+      List<Camera.Size> supportedSizes, final int requestedWidth, final int requestedHeight) {
+    return Collections.min(supportedSizes,
+        new ClosestComparator<Camera.Size>() {
+          @Override int diff(Camera.Size size) {
+            return abs(requestedWidth - size.width) + abs(requestedHeight - size.height);
+          }
+     });
+  }
+
+  // Called on cameraThread so must not "synchronized".
+  @Override
+  public void onPreviewFrame(byte[] data, Camera callbackCamera) {
+    if (Thread.currentThread() != cameraThread) {
+      throw new RuntimeException("Camera callback not on camera thread?!?");
+    }
+    if (camera == null) {
+      return;
+    }
+    if (camera != callbackCamera) {
+      throw new RuntimeException("Unexpected camera in callback!");
+    }
+
+    final long captureTimeNs =
+        TimeUnit.MILLISECONDS.toNanos(SystemClock.elapsedRealtime());
+
+    captureBuffersCount += videoBuffers.numCaptureBuffersAvailable();
+    int rotation = getDeviceOrientation();
+    if (info.facing == Camera.CameraInfo.CAMERA_FACING_BACK) {
+      rotation = 360 - rotation;
+    }
+    rotation = (info.orientation + rotation) % 360;
+    // Mark the frame owning |data| as used.
+    // Note that since data is directBuffer,
+    // data.length >= videoBuffers.frameSize.
+    if (videoBuffers.reserveByteBuffer(data, captureTimeNs)) {
+      cameraFramesCount++;
+      frameObserver.OnFrameCaptured(data, videoBuffers.frameSize, captureFormat.width,
+          captureFormat.height, rotation, captureTimeNs);
+    } else {
+      Log.w(TAG, "reserveByteBuffer failed - dropping frame.");
+    }
+  }
+
+  // runCameraThreadUntilIdle make sure all posted messages to the cameraThread
+  // is processed before returning. It does that by itself posting a message to
+  // to the message queue and waits until is has been processed.
+  // It is used in tests.
+  void runCameraThreadUntilIdle() {
+    if (cameraThreadHandler == null)
+      return;
+    final Exchanger<Boolean> result = new Exchanger<Boolean>();
+    cameraThreadHandler.post(new Runnable() {
+      @Override public void run() {
+        exchange(result, true); // |true| is a dummy here.
+      }
+    });
+    exchange(result, false);  // |false| is a dummy value here.
+    return;
+  }
+
+  // Exchanges |value| with |exchanger|, converting InterruptedExceptions to
+  // RuntimeExceptions (since we expect never to see these).
+  private static <T> T exchange(Exchanger<T> exchanger, T value) {
+    try {
+      return exchanger.exchange(value);
+    } catch (InterruptedException e) {
+      throw new RuntimeException(e);
+    }
+  }
+
+  // Class used for allocating and bookkeeping video frames. All buffers are
+  // direct allocated so that they can be directly used from native code.
+  private static class FramePool {
+    // Arbitrary queue depth.  Higher number means more memory allocated & held,
+    // lower number means more sensitivity to processing time in the client (and
+    // potentially stalling the capturer if it runs out of buffers to write to).
+    private static final int numCaptureBuffers = 3;
+    // This container tracks the buffers added as camera callback buffers. It is needed for finding
+    // the corresponding ByteBuffer given a byte[].
+    private final Map<byte[], ByteBuffer> queuedBuffers = new IdentityHashMap<byte[], ByteBuffer>();
+    // This container tracks the frames that have been sent but not returned. It is needed for
+    // keeping the buffers alive and for finding the corresponding ByteBuffer given a timestamp.
+    private final Map<Long, ByteBuffer> pendingBuffers = new HashMap<Long, ByteBuffer>();
+    private int frameSize = 0;
+    private Camera camera;
+
+    int numCaptureBuffersAvailable() {
+      return queuedBuffers.size();
+    }
+
+    // Discards previous queued buffers and adds new callback buffers to camera.
+    void queueCameraBuffers(int frameSize, Camera camera) {
+      this.camera = camera;
+      this.frameSize = frameSize;
+
+      queuedBuffers.clear();
+      for (int i = 0; i < numCaptureBuffers; ++i) {
+        final ByteBuffer buffer = ByteBuffer.allocateDirect(frameSize);
+        camera.addCallbackBuffer(buffer.array());
+        queuedBuffers.put(buffer.array(), buffer);
+      }
+      Log.d(TAG, "queueCameraBuffers enqueued " + numCaptureBuffers
+          + " buffers of size " + frameSize + ".");
+    }
+
+    String pendingFramesTimeStamps() {
+      List<Long> timeStampsMs = new ArrayList<Long>();
+      for (Long timeStampNs : pendingBuffers.keySet()) {
+        timeStampsMs.add(TimeUnit.NANOSECONDS.toMillis(timeStampNs));
+      }
+      return timeStampsMs.toString();
+    }
+
+    void stopReturnBuffersToCamera() {
+      this.camera = null;
+      queuedBuffers.clear();
+      // Frames in |pendingBuffers| need to be kept alive until they are returned.
+      Log.d(TAG, "stopReturnBuffersToCamera called."
+            + (pendingBuffers.isEmpty() ?
+                   " All buffers have been returned."
+                   : " Pending buffers: " + pendingFramesTimeStamps() + "."));
+    }
+
+    boolean reserveByteBuffer(byte[] data, long timeStamp) {
+      final ByteBuffer buffer = queuedBuffers.remove(data);
+      if (buffer == null) {
+        // Frames might be posted to |onPreviewFrame| with the previous format while changing
+        // capture format in |startPreviewOnCameraThread|. Drop these old frames.
+        Log.w(TAG, "Received callback buffer from previous configuration with length: "
+            + (data == null ? "null" : data.length));
+        return false;
+      }
+      if (buffer.capacity() != frameSize) {
+        throw new IllegalStateException("Callback buffer has unexpected frame size");
+      }
+      if (pendingBuffers.containsKey(timeStamp)) {
+        Log.e(TAG, "Timestamp already present in pending buffers - they need to be unique");
+        return false;
+      }
+      pendingBuffers.put(timeStamp, buffer);
+      if (queuedBuffers.isEmpty()) {
+        Log.v(TAG, "Camera is running out of capture buffers."
+            + " Pending buffers: " + pendingFramesTimeStamps());
+      }
+      return true;
+    }
+
+    void returnBuffer(long timeStamp) {
+      final ByteBuffer returnedFrame = pendingBuffers.remove(timeStamp);
+      if (returnedFrame == null) {
+        throw new RuntimeException("unknown data buffer with time stamp "
+            + timeStamp + "returned?!?");
+      }
+
+      if (camera != null && returnedFrame.capacity() == frameSize) {
+        camera.addCallbackBuffer(returnedFrame.array());
+        if (queuedBuffers.isEmpty()) {
+          Log.v(TAG, "Frame returned when camera is running out of capture"
+              + " buffers for TS " + TimeUnit.NANOSECONDS.toMillis(timeStamp));
+        }
+        queuedBuffers.put(returnedFrame.array(), returnedFrame);
+        return;
+      }
+
+      if (returnedFrame.capacity() != frameSize) {
+        Log.d(TAG, "returnBuffer with time stamp "
+            + TimeUnit.NANOSECONDS.toMillis(timeStamp)
+            + " called with old frame size, " + returnedFrame.capacity() + ".");
+        // Since this frame has the wrong size, don't requeue it. Frames with the correct size are
+        // created in queueCameraBuffers so this must be an old buffer.
+        return;
+      }
+
+      Log.d(TAG, "returnBuffer with time stamp "
+          + TimeUnit.NANOSECONDS.toMillis(timeStamp)
+          + " called after camera has been stopped.");
+    }
+  }
+
+  // Interface used for providing callbacks to an observer.
+  interface CapturerObserver {
+    // Notify if the camera have been started successfully or not.
+    // Called on a Java thread owned by VideoCapturerAndroid.
+    abstract void OnCapturerStarted(boolean success);
+
+    // Delivers a captured frame. Called on a Java thread owned by
+    // VideoCapturerAndroid.
+    abstract void OnFrameCaptured(byte[] data, int length, int width, int height, int rotation, long timeStamp);
+
+    // Requests an output format from the video capturer. Captured frames
+    // by the camera will be scaled/or dropped by the video capturer.
+    // Called on a Java thread owned by VideoCapturerAndroid.
+    abstract void OnOutputFormatRequest(int width, int height, int fps);
+  }
+
+  // An implementation of CapturerObserver that forwards all calls from
+  // Java to the C layer.
+  static class NativeObserver implements CapturerObserver {
+    private final long nativeCapturer;
+
+    public NativeObserver(long nativeCapturer) {
+      this.nativeCapturer = nativeCapturer;
+    }
+
+    @Override
+    public void OnCapturerStarted(boolean success) {
+      nativeCapturerStarted(nativeCapturer, success);
+    }
+
+    @Override
+    public void OnFrameCaptured(byte[] data, int length, int width, int height,
+        int rotation, long timeStamp) {
+      nativeOnFrameCaptured(nativeCapturer, data, length, width, height, rotation, timeStamp);
+    }
+
+    @Override
+    public void OnOutputFormatRequest(int width, int height, int fps) {
+      nativeOnOutputFormatRequest(nativeCapturer, width, height, fps);
+    }
+
+    private native void nativeCapturerStarted(long nativeCapturer,
+        boolean success);
+    private native void nativeOnFrameCaptured(long nativeCapturer,
+        byte[] data, int length, int width, int height, int rotation, long timeStamp);
+    private native void nativeOnOutputFormatRequest(long nativeCapturer,
+        int width, int height, int fps);
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/VideoRenderer.java b/librtc/src/main/java/org/webrtc/VideoRenderer.java
new file mode 100644
index 0000000..596d377
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/VideoRenderer.java
@@ -0,0 +1,220 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.nio.ByteBuffer;
+
+/**
+ * Java version of VideoRendererInterface.  In addition to allowing clients to
+ * define their own rendering behavior (by passing in a Callbacks object), this
+ * class also provides a createGui() method for creating a GUI-rendering window
+ * on various platforms.
+ */
+public class VideoRenderer {
+
+  /** Java version of cricket::VideoFrame. */
+  public static class I420Frame {
+    public final int width;
+    public final int height;
+    public final int[] yuvStrides;
+    public final ByteBuffer[] yuvPlanes;
+    public final boolean yuvFrame;
+    public Object textureObject;
+    public int textureId;
+
+    // rotationDegree is the degree that the frame must be rotated clockwisely
+    // to be rendered correctly.
+    public int rotationDegree;
+
+    /**
+     * Construct a frame of the given dimensions with the specified planar
+     * data.  If |yuvPlanes| is null, new planes of the appropriate sizes are
+     * allocated.
+     */
+    public I420Frame(
+        int width, int height, int rotationDegree,
+        int[] yuvStrides, ByteBuffer[] yuvPlanes) {
+      this.width = width;
+      this.height = height;
+      this.yuvStrides = yuvStrides;
+      if (yuvPlanes == null) {
+        yuvPlanes = new ByteBuffer[3];
+        yuvPlanes[0] = ByteBuffer.allocateDirect(yuvStrides[0] * height);
+        yuvPlanes[1] = ByteBuffer.allocateDirect(yuvStrides[1] * height / 2);
+        yuvPlanes[2] = ByteBuffer.allocateDirect(yuvStrides[2] * height / 2);
+      }
+      this.yuvPlanes = yuvPlanes;
+      this.yuvFrame = true;
+      this.rotationDegree = rotationDegree;
+      if (rotationDegree % 90 != 0) {
+        throw new IllegalArgumentException("Rotation degree not multiple of 90: " + rotationDegree);
+      }
+    }
+
+    /**
+     * Construct a texture frame of the given dimensions with data in SurfaceTexture
+     */
+    public I420Frame(
+        int width, int height, int rotationDegree,
+        Object textureObject, int textureId) {
+      this.width = width;
+      this.height = height;
+      this.yuvStrides = null;
+      this.yuvPlanes = null;
+      this.textureObject = textureObject;
+      this.textureId = textureId;
+      this.yuvFrame = false;
+      this.rotationDegree = rotationDegree;
+      if (rotationDegree % 90 != 0) {
+        throw new IllegalArgumentException("Rotation degree not multiple of 90: " + rotationDegree);
+      }
+    }
+
+    public int rotatedWidth() {
+      return (rotationDegree % 180 == 0) ? width : height;
+    }
+
+    public int rotatedHeight() {
+      return (rotationDegree % 180 == 0) ? height : width;
+    }
+
+    /**
+     * Copy the planes out of |source| into |this| and return |this|.  Calling
+     * this with mismatched frame dimensions or frame type is a programming
+     * error and will likely crash.
+     */
+    public I420Frame copyFrom(I420Frame source) {
+      if (source.yuvFrame && yuvFrame) {
+        if (width != source.width || height != source.height) {
+          throw new RuntimeException("Mismatched dimensions!  Source: " +
+              source.toString() + ", destination: " + toString());
+        }
+        nativeCopyPlane(source.yuvPlanes[0], width, height,
+            source.yuvStrides[0], yuvPlanes[0], yuvStrides[0]);
+        nativeCopyPlane(source.yuvPlanes[1], width / 2, height / 2,
+            source.yuvStrides[1], yuvPlanes[1], yuvStrides[1]);
+        nativeCopyPlane(source.yuvPlanes[2], width / 2, height / 2,
+            source.yuvStrides[2], yuvPlanes[2], yuvStrides[2]);
+        rotationDegree = source.rotationDegree;
+        return this;
+      } else if (!source.yuvFrame && !yuvFrame) {
+        textureObject = source.textureObject;
+        textureId = source.textureId;
+        rotationDegree = source.rotationDegree;
+        return this;
+      } else {
+        throw new RuntimeException("Mismatched frame types!  Source: " +
+            source.toString() + ", destination: " + toString());
+      }
+    }
+
+    public I420Frame copyFrom(byte[] yuvData, int rotationDegree) {
+      if (yuvData.length < width * height * 3 / 2) {
+        throw new RuntimeException("Wrong arrays size: " + yuvData.length);
+      }
+      if (!yuvFrame) {
+        throw new RuntimeException("Can not feed yuv data to texture frame");
+      }
+      int planeSize = width * height;
+      ByteBuffer[] planes = new ByteBuffer[3];
+      planes[0] = ByteBuffer.wrap(yuvData, 0, planeSize);
+      planes[1] = ByteBuffer.wrap(yuvData, planeSize, planeSize / 4);
+      planes[2] = ByteBuffer.wrap(yuvData, planeSize + planeSize / 4,
+          planeSize / 4);
+      for (int i = 0; i < 3; i++) {
+        yuvPlanes[i].position(0);
+        yuvPlanes[i].put(planes[i]);
+        yuvPlanes[i].position(0);
+        yuvPlanes[i].limit(yuvPlanes[i].capacity());
+      }
+      this.rotationDegree = rotationDegree;
+      return this;
+    }
+
+    @Override
+    public String toString() {
+      return width + "x" + height + ":" + yuvStrides[0] + ":" + yuvStrides[1] +
+          ":" + yuvStrides[2];
+    }
+  }
+
+  // Helper native function to do a video frame plane copying.
+  private static native void nativeCopyPlane(ByteBuffer src, int width,
+      int height, int srcStride, ByteBuffer dst, int dstStride);
+
+  /** The real meat of VideoRendererInterface. */
+  public static interface Callbacks {
+    // |frame| might have pending rotation and implementation of Callbacks
+    // should handle that by applying rotation during rendering.
+    public void renderFrame(I420Frame frame);
+    // TODO(guoweis): Remove this once chrome code base is updated.
+    public boolean canApplyRotation();
+  }
+
+  // |this| either wraps a native (GUI) renderer or a client-supplied Callbacks
+  // (Java) implementation; this is indicated by |isWrappedVideoRenderer|.
+  long nativeVideoRenderer;
+  private final boolean isWrappedVideoRenderer;
+
+  public static VideoRenderer createGui(int x, int y) {
+    long nativeVideoRenderer = nativeCreateGuiVideoRenderer(x, y);
+    if (nativeVideoRenderer == 0) {
+      return null;
+    }
+    return new VideoRenderer(nativeVideoRenderer);
+  }
+
+  public VideoRenderer(Callbacks callbacks) {
+    nativeVideoRenderer = nativeWrapVideoRenderer(callbacks);
+    isWrappedVideoRenderer = true;
+  }
+
+  private VideoRenderer(long nativeVideoRenderer) {
+    this.nativeVideoRenderer = nativeVideoRenderer;
+    isWrappedVideoRenderer = false;
+  }
+
+  public void dispose() {
+    if (nativeVideoRenderer == 0) {
+      // Already disposed.
+      return;
+    }
+    if (!isWrappedVideoRenderer) {
+      freeGuiVideoRenderer(nativeVideoRenderer);
+    } else {
+      freeWrappedVideoRenderer(nativeVideoRenderer);
+    }
+    nativeVideoRenderer = 0;
+  }
+
+  private static native long nativeCreateGuiVideoRenderer(int x, int y);
+  private static native long nativeWrapVideoRenderer(Callbacks callbacks);
+
+  private static native void freeGuiVideoRenderer(long nativeVideoRenderer);
+  private static native void freeWrappedVideoRenderer(long nativeVideoRenderer);
+}
diff --git a/librtc/src/main/java/org/webrtc/VideoRendererGui.java b/librtc/src/main/java/org/webrtc/VideoRendererGui.java
new file mode 100644
index 0000000..8246e8b
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/VideoRendererGui.java
@@ -0,0 +1,727 @@
+/*
+ * libjingle
+ * Copyright 2014 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.util.ArrayList;
+import java.util.concurrent.CountDownLatch;
+import java.util.concurrent.LinkedBlockingQueue;
+
+import javax.microedition.khronos.egl.EGLConfig;
+import javax.microedition.khronos.opengles.GL10;
+
+import android.annotation.SuppressLint;
+import android.graphics.Point;
+import android.graphics.Rect;
+import android.graphics.SurfaceTexture;
+import android.opengl.EGL14;
+import android.opengl.EGLContext;
+import android.opengl.GLES20;
+import android.opengl.GLSurfaceView;
+import android.opengl.Matrix;
+import android.util.Log;
+
+import org.webrtc.VideoRenderer.I420Frame;
+
+/**
+ * Efficiently renders YUV frames using the GPU for CSC.
+ * Clients will want first to call setView() to pass GLSurfaceView
+ * and then for each video stream either create instance of VideoRenderer using
+ * createGui() call or VideoRenderer.Callbacks interface using create() call.
+ * Only one instance of the class can be created.
+ */
+public class VideoRendererGui implements GLSurfaceView.Renderer {
+  // |instance|, |instance.surface|, |eglContext|, and |eglContextReady| are synchronized on
+  // |VideoRendererGui.class|.
+  private static VideoRendererGui instance = null;
+  private static Runnable eglContextReady = null;
+  private static final String TAG = "VideoRendererGui";
+  private GLSurfaceView surface;
+  private static EGLContext eglContext = null;
+  // Indicates if SurfaceView.Renderer.onSurfaceCreated was called.
+  // If true then for every newly created yuv image renderer createTexture()
+  // should be called. The variable is accessed on multiple threads and
+  // all accesses are synchronized on yuvImageRenderers' object lock.
+  private boolean onSurfaceCreatedCalled;
+  private int screenWidth;
+  private int screenHeight;
+  // List of yuv renderers.
+  private final ArrayList<YuvImageRenderer> yuvImageRenderers;
+  // |drawer| is synchronized on |yuvImageRenderers|.
+  private GlRectDrawer drawer;
+  // The minimum fraction of the frame content that will be shown for |SCALE_ASPECT_BALANCED|.
+  // This limits excessive cropping when adjusting display size.
+  private static float BALANCED_VISIBLE_FRACTION = 0.56f;
+  // Types of video scaling:
+  // SCALE_ASPECT_FIT - video frame is scaled to fit the size of the view by
+  //    maintaining the aspect ratio (black borders may be displayed).
+  // SCALE_ASPECT_FILL - video frame is scaled to fill the size of the view by
+  //    maintaining the aspect ratio. Some portion of the video frame may be
+  //    clipped.
+  // SCALE_ASPECT_BALANCED - Compromise between FIT and FILL. Video frame will fill as much as
+  // possible of the view while maintaining aspect ratio, under the constraint that at least
+  // |BALANCED_VISIBLE_FRACTION| of the frame content will be shown.
+  public static enum ScalingType
+      { SCALE_ASPECT_FIT, SCALE_ASPECT_FILL, SCALE_ASPECT_BALANCED }
+  private static final int EGL14_SDK_VERSION =
+      android.os.Build.VERSION_CODES.JELLY_BEAN_MR1;
+  // Current SDK version.
+  private static final int CURRENT_SDK_VERSION =
+      android.os.Build.VERSION.SDK_INT;
+
+  private VideoRendererGui(GLSurfaceView surface) {
+    this.surface = surface;
+    // Create an OpenGL ES 2.0 context.
+    surface.setPreserveEGLContextOnPause(true);
+    surface.setEGLContextClientVersion(2);
+    surface.setRenderer(this);
+    surface.setRenderMode(GLSurfaceView.RENDERMODE_WHEN_DIRTY);
+
+    yuvImageRenderers = new ArrayList<YuvImageRenderer>();
+  }
+
+  public static synchronized void dispose() {
+    if (instance == null){
+      return;
+    }
+    synchronized (instance.yuvImageRenderers) {
+      for (YuvImageRenderer yuvImageRenderer : instance.yuvImageRenderers) {
+        yuvImageRenderer.release();
+      }
+      instance.yuvImageRenderers.clear();
+      if (instance.drawer != null) {
+        instance.drawer.release();
+      }
+    }
+    instance.surface = null;
+    instance.eglContext = null;
+    instance.eglContextReady = null;
+    instance = null;
+  }
+
+  /**
+   * Class used to display stream of YUV420 frames at particular location
+   * on a screen. New video frames are sent to display using renderFrame()
+   * call.
+   */
+  private static class YuvImageRenderer implements VideoRenderer.Callbacks {
+    // |surface| is synchronized on |this|.
+    private GLSurfaceView surface;
+    private int id;
+    private int[] yuvTextures = { -1, -1, -1 };
+    private int oesTexture = -1;
+
+    // Render frame queue - accessed by two threads. renderFrame() call does
+    // an offer (writing I420Frame to render) and early-returns (recording
+    // a dropped frame) if that queue is full. draw() call does a peek(),
+    // copies frame to texture and then removes it from a queue using poll().
+    private final LinkedBlockingQueue<I420Frame> frameToRenderQueue;
+    // Local copy of incoming video frame. Synchronized on |frameToRenderQueue|.
+    private I420Frame yuvFrameToRender;
+    private I420Frame textureFrameToRender;
+    // Type of video frame used for recent frame rendering.
+    private static enum RendererType { RENDERER_YUV, RENDERER_TEXTURE };
+    private RendererType rendererType;
+    private ScalingType scalingType;
+    private boolean mirror;
+    private RendererEvents rendererEvents;
+    // Flag if renderFrame() was ever called.
+    boolean seenFrame;
+    // Total number of video frames received in renderFrame() call.
+    private int framesReceived;
+    // Number of video frames dropped by renderFrame() because previous
+    // frame has not been rendered yet.
+    private int framesDropped;
+    // Number of rendered video frames.
+    private int framesRendered;
+    // Time in ns when the first video frame was rendered.
+    private long startTimeNs = -1;
+    // Time in ns spent in draw() function.
+    private long drawTimeNs;
+    // Time in ns spent in renderFrame() function - including copying frame
+    // data to rendering planes.
+    private long copyTimeNs;
+    // The allowed view area in percentage of screen size.
+    private final Rect layoutInPercentage;
+    // The actual view area in pixels. It is a centered subrectangle of the rectangle defined by
+    // |layoutInPercentage|.
+    private final Rect displayLayout = new Rect();
+    // Cached texture transformation matrix, calculated from current layout parameters.
+    private final float[] texMatrix = new float[16];
+    // Flag if texture vertices or coordinates update is needed.
+    private boolean updateTextureProperties;
+    // Texture properties update lock.
+    private final Object updateTextureLock = new Object();
+    // Viewport dimensions.
+    private int screenWidth;
+    private int screenHeight;
+    // Video dimension.
+    private int videoWidth;
+    private int videoHeight;
+
+    // This is the degree that the frame should be rotated clockwisely to have
+    // it rendered up right.
+    private int rotationDegree;
+
+    private YuvImageRenderer(
+        GLSurfaceView surface, int id,
+        int x, int y, int width, int height,
+        ScalingType scalingType, boolean mirror) {
+      Log.d(TAG, "YuvImageRenderer.Create id: " + id);
+      this.surface = surface;
+      this.id = id;
+      this.scalingType = scalingType;
+      this.mirror = mirror;
+      frameToRenderQueue = new LinkedBlockingQueue<I420Frame>(1);
+      layoutInPercentage = new Rect(x, y, Math.min(100, x + width), Math.min(100, y + height));
+      updateTextureProperties = false;
+      rotationDegree = 0;
+    }
+
+    private synchronized void release() {
+      surface = null;
+      synchronized (frameToRenderQueue) {
+        frameToRenderQueue.clear();
+        yuvFrameToRender = null;
+        textureFrameToRender = null;
+      }
+    }
+
+    private void createTextures() {
+      Log.d(TAG, "  YuvImageRenderer.createTextures " + id + " on GL thread:" +
+          Thread.currentThread().getId());
+
+      // Generate 3 texture ids for Y/U/V and place them into |yuvTextures|.
+      GLES20.glGenTextures(3, yuvTextures, 0);
+      for (int i = 0; i < 3; i++)  {
+        GLES20.glActiveTexture(GLES20.GL_TEXTURE0 + i);
+        GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, yuvTextures[i]);
+        GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D,
+            GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_LINEAR);
+        GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D,
+            GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR);
+        GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D,
+            GLES20.GL_TEXTURE_WRAP_S, GLES20.GL_CLAMP_TO_EDGE);
+        GLES20.glTexParameterf(GLES20.GL_TEXTURE_2D,
+            GLES20.GL_TEXTURE_WRAP_T, GLES20.GL_CLAMP_TO_EDGE);
+      }
+      GlUtil.checkNoGLES2Error("y/u/v glGenTextures");
+    }
+
+    private static float convertScalingTypeToVisibleFraction(ScalingType scalingType) {
+      switch (scalingType) {
+        case SCALE_ASPECT_FIT:
+          return 1.0f;
+        case SCALE_ASPECT_FILL:
+          return 0.0f;
+        case SCALE_ASPECT_BALANCED:
+          return BALANCED_VISIBLE_FRACTION;
+        default:
+          throw new IllegalArgumentException();
+      }
+    }
+
+    private static Point getDisplaySize(float minVisibleFraction, float videoAspectRatio,
+        int maxDisplayWidth, int maxDisplayHeight) {
+      // If there is no constraint on the amount of cropping, fill the allowed display area.
+      if (minVisibleFraction == 0) {
+        return new Point(maxDisplayWidth, maxDisplayHeight);
+      }
+      // Each dimension is constrained on max display size and how much we are allowed to crop.
+      final int width = Math.min(maxDisplayWidth,
+          (int) (maxDisplayHeight / minVisibleFraction * videoAspectRatio));
+      final int height = Math.min(maxDisplayHeight,
+          (int) (maxDisplayWidth / minVisibleFraction / videoAspectRatio));
+      return new Point(width, height);
+    }
+
+    private void checkAdjustTextureCoords() {
+      synchronized(updateTextureLock) {
+        if (!updateTextureProperties) {
+          return;
+        }
+        // Initialize to maximum allowed area. Round to integer coordinates inwards the layout
+        // bounding box (ceil left/top and floor right/bottom) to not break constraints.
+        displayLayout.set(
+            (screenWidth * layoutInPercentage.left + 99) / 100,
+            (screenHeight * layoutInPercentage.top + 99) / 100,
+            (screenWidth * layoutInPercentage.right) / 100,
+            (screenHeight * layoutInPercentage.bottom) / 100);
+        Log.d(TAG, "ID: "  + id + ". AdjustTextureCoords. Allowed display size: "
+            + displayLayout.width() + " x " + displayLayout.height() + ". Video: " + videoWidth
+            + " x " + videoHeight + ". Rotation: " + rotationDegree + ". Mirror: " + mirror);
+        final float videoAspectRatio = (rotationDegree % 180 == 0)
+            ? (float) videoWidth / videoHeight
+            : (float) videoHeight / videoWidth;
+        // Adjust display size based on |scalingType|.
+        final float minVisibleFraction = convertScalingTypeToVisibleFraction(scalingType);
+        final Point displaySize = getDisplaySize(minVisibleFraction, videoAspectRatio,
+            displayLayout.width(), displayLayout.height());
+        displayLayout.inset((displayLayout.width() - displaySize.x) / 2,
+                            (displayLayout.height() - displaySize.y) / 2);
+        Log.d(TAG, "  Adjusted display size: " + displayLayout.width() + " x "
+            + displayLayout.height());
+        // The matrix stack is using post-multiplication, which means that matrix operations:
+        // A; B; C; will end up as A * B * C. When you apply this to a vertex, it will result in:
+        // v' = A * B * C * v, i.e. the last matrix operation is the first thing that affects the
+        // vertex. This is the opposite of what you might expect.
+        Matrix.setIdentityM(texMatrix, 0);
+        // Move coordinates back to [0,1]x[0,1].
+        Matrix.translateM(texMatrix, 0, 0.5f, 0.5f, 0.0f);
+        // Rotate frame clockwise in the XY-plane (around the Z-axis).
+        Matrix.rotateM(texMatrix, 0, -rotationDegree, 0, 0, 1);
+        // Scale one dimension until video and display size have same aspect ratio.
+        final float displayAspectRatio = (float) displayLayout.width() / displayLayout.height();
+        if (displayAspectRatio > videoAspectRatio) {
+            Matrix.scaleM(texMatrix, 0, 1, videoAspectRatio / displayAspectRatio, 1);
+        } else {
+            Matrix.scaleM(texMatrix, 0, displayAspectRatio / videoAspectRatio, 1, 1);
+        }
+        // TODO(magjed): We currently ignore the texture transform matrix from the SurfaceTexture.
+        // It contains a vertical flip that is hardcoded here instead.
+        Matrix.scaleM(texMatrix, 0, 1, -1, 1);
+        // Apply optional horizontal flip.
+        if (mirror) {
+          Matrix.scaleM(texMatrix, 0, -1, 1, 1);
+        }
+        // Center coordinates around origin.
+        Matrix.translateM(texMatrix, 0, -0.5f, -0.5f, 0.0f);
+        updateTextureProperties = false;
+        Log.d(TAG, "  AdjustTextureCoords done");
+      }
+    }
+
+    private void draw(GlRectDrawer drawer) {
+      if (!seenFrame) {
+        // No frame received yet - nothing to render.
+        return;
+      }
+      long now = System.nanoTime();
+
+      // OpenGL defaults to lower left origin.
+      GLES20.glViewport(displayLayout.left, screenHeight - displayLayout.bottom,
+                        displayLayout.width(), displayLayout.height());
+
+      I420Frame frameFromQueue;
+      synchronized (frameToRenderQueue) {
+        // Check if texture vertices/coordinates adjustment is required when
+        // screen orientation changes or video frame size changes.
+        checkAdjustTextureCoords();
+
+        frameFromQueue = frameToRenderQueue.peek();
+        if (frameFromQueue != null && startTimeNs == -1) {
+          startTimeNs = now;
+        }
+
+        if (frameFromQueue != null) {
+          if (frameFromQueue.yuvFrame) {
+            // YUV textures rendering. Upload YUV data as textures.
+            for (int i = 0; i < 3; ++i) {
+              GLES20.glActiveTexture(GLES20.GL_TEXTURE0 + i);
+              GLES20.glBindTexture(GLES20.GL_TEXTURE_2D, yuvTextures[i]);
+              int w = (i == 0) ? frameFromQueue.width : frameFromQueue.width / 2;
+              int h = (i == 0) ? frameFromQueue.height : frameFromQueue.height / 2;
+              GLES20.glTexImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_LUMINANCE,
+                  w, h, 0, GLES20.GL_LUMINANCE, GLES20.GL_UNSIGNED_BYTE,
+                  frameFromQueue.yuvPlanes[i]);
+            }
+          } else {
+            // External texture rendering. Copy texture id and update texture image to latest.
+            // TODO(magjed): We should not make an unmanaged copy of texture id. Also, this is not
+            // the best place to call updateTexImage.
+            oesTexture = frameFromQueue.textureId;
+            if (frameFromQueue.textureObject instanceof SurfaceTexture) {
+              SurfaceTexture surfaceTexture =
+                  (SurfaceTexture) frameFromQueue.textureObject;
+              surfaceTexture.updateTexImage();
+            }
+          }
+
+          frameToRenderQueue.poll();
+        }
+      }
+
+      if (rendererType == RendererType.RENDERER_YUV) {
+        drawer.drawYuv(videoWidth, videoHeight, yuvTextures, texMatrix);
+      } else {
+        drawer.drawOes(oesTexture, texMatrix);
+      }
+
+      if (frameFromQueue != null) {
+        framesRendered++;
+        drawTimeNs += (System.nanoTime() - now);
+        if ((framesRendered % 300) == 0) {
+          logStatistics();
+        }
+      }
+    }
+
+    private void logStatistics() {
+      long timeSinceFirstFrameNs = System.nanoTime() - startTimeNs;
+      Log.d(TAG, "ID: " + id + ". Type: " + rendererType +
+          ". Frames received: " + framesReceived +
+          ". Dropped: " + framesDropped + ". Rendered: " + framesRendered);
+      if (framesReceived > 0 && framesRendered > 0) {
+        Log.d(TAG, "Duration: " + (int)(timeSinceFirstFrameNs / 1e6) +
+            " ms. FPS: " + (float)framesRendered * 1e9 / timeSinceFirstFrameNs);
+        Log.d(TAG, "Draw time: " +
+            (int) (drawTimeNs / (1000 * framesRendered)) + " us. Copy time: " +
+            (int) (copyTimeNs / (1000 * framesReceived)) + " us");
+      }
+    }
+
+    public void setScreenSize(final int screenWidth, final int screenHeight) {
+      synchronized(updateTextureLock) {
+        if (screenWidth == this.screenWidth && screenHeight == this.screenHeight) {
+          return;
+        }
+        Log.d(TAG, "ID: " + id + ". YuvImageRenderer.setScreenSize: " +
+            screenWidth + " x " + screenHeight);
+        this.screenWidth = screenWidth;
+        this.screenHeight = screenHeight;
+        updateTextureProperties = true;
+      }
+    }
+
+    public void setPosition(int x, int y, int width, int height,
+        ScalingType scalingType, boolean mirror) {
+      final Rect layoutInPercentage =
+          new Rect(x, y, Math.min(100, x + width), Math.min(100, y + height));
+      synchronized(updateTextureLock) {
+        if (layoutInPercentage.equals(this.layoutInPercentage) && scalingType == this.scalingType
+            && mirror == this.mirror) {
+          return;
+        }
+        Log.d(TAG, "ID: " + id + ". YuvImageRenderer.setPosition: (" + x + ", " + y +
+            ") " +  width + " x " + height + ". Scaling: " + scalingType +
+            ". Mirror: " + mirror);
+        this.layoutInPercentage.set(layoutInPercentage);
+        this.scalingType = scalingType;
+        this.mirror = mirror;
+        updateTextureProperties = true;
+      }
+    }
+
+    private void setSize(final int videoWidth, final int videoHeight, final int rotation) {
+      if (videoWidth == this.videoWidth && videoHeight == this.videoHeight
+          && rotation == rotationDegree) {
+        return;
+      }
+      if (rendererEvents != null) {
+        Log.d(TAG, "ID: " + id +
+            ". Reporting frame resolution changed to " + videoWidth + " x " + videoHeight);
+        rendererEvents.onFrameResolutionChanged(videoWidth, videoHeight, rotation);
+      }
+
+      // Frame re-allocation need to be synchronized with copying
+      // frame to textures in draw() function to avoid re-allocating
+      // the frame while it is being copied.
+      synchronized (frameToRenderQueue) {
+        Log.d(TAG, "ID: " + id + ". YuvImageRenderer.setSize: " +
+            videoWidth + " x " + videoHeight + " rotation " + rotation);
+
+        this.videoWidth = videoWidth;
+        this.videoHeight = videoHeight;
+        rotationDegree = rotation;
+        int[] strides = { videoWidth, videoWidth / 2, videoWidth / 2  };
+
+        // Clear rendering queue.
+        frameToRenderQueue.poll();
+        // Re-allocate / allocate the frame.
+        yuvFrameToRender = new I420Frame(videoWidth, videoHeight, rotationDegree,
+                                         strides, null);
+        textureFrameToRender = new I420Frame(videoWidth, videoHeight, rotationDegree,
+                                             null, -1);
+        updateTextureProperties = true;
+        Log.d(TAG, "  YuvImageRenderer.setSize done.");
+      }
+    }
+
+    @Override
+    public synchronized void renderFrame(I420Frame frame) {
+      if (surface == null) {
+        // This object has been released.
+        return;
+      }
+      if (!seenFrame && rendererEvents != null) {
+        Log.d(TAG, "ID: " + id + ". Reporting first rendered frame.");
+        rendererEvents.onFirstFrameRendered();
+      }
+      setSize(frame.width, frame.height, frame.rotationDegree);
+      long now = System.nanoTime();
+      framesReceived++;
+      synchronized (frameToRenderQueue) {
+        // Skip rendering of this frame if setSize() was not called.
+        if (yuvFrameToRender == null || textureFrameToRender == null) {
+          framesDropped++;
+          return;
+        }
+        // Check input frame parameters.
+        if (frame.yuvFrame) {
+          if (frame.yuvStrides[0] < frame.width ||
+              frame.yuvStrides[1] < frame.width / 2 ||
+              frame.yuvStrides[2] < frame.width / 2) {
+            Log.e(TAG, "Incorrect strides " + frame.yuvStrides[0] + ", " +
+                frame.yuvStrides[1] + ", " + frame.yuvStrides[2]);
+            return;
+          }
+          // Check incoming frame dimensions.
+          if (frame.width != yuvFrameToRender.width ||
+              frame.height != yuvFrameToRender.height) {
+            throw new RuntimeException("Wrong frame size " +
+                frame.width + " x " + frame.height);
+          }
+        }
+
+        if (frameToRenderQueue.size() > 0) {
+          // Skip rendering of this frame if previous frame was not rendered yet.
+          framesDropped++;
+          return;
+        }
+
+        // Create a local copy of the frame.
+        if (frame.yuvFrame) {
+          yuvFrameToRender.copyFrom(frame);
+          rendererType = RendererType.RENDERER_YUV;
+          frameToRenderQueue.offer(yuvFrameToRender);
+        } else {
+          textureFrameToRender.copyFrom(frame);
+          rendererType = RendererType.RENDERER_TEXTURE;
+          frameToRenderQueue.offer(textureFrameToRender);
+        }
+      }
+      copyTimeNs += (System.nanoTime() - now);
+      seenFrame = true;
+
+      // Request rendering.
+      surface.requestRender();
+    }
+
+    // TODO(guoweis): Remove this once chrome code base is updated.
+    @Override
+    public boolean canApplyRotation() {
+      return true;
+    }
+  }
+
+  /** Interface for reporting rendering events. */
+  public static interface RendererEvents {
+    /**
+     * Callback fired once first frame is rendered.
+     */
+    public void onFirstFrameRendered();
+
+    /**
+     * Callback fired when rendered frame resolution or rotation has changed.
+     */
+    public void onFrameResolutionChanged(int videoWidth, int videoHeight, int rotation);
+  }
+
+  /** Passes GLSurfaceView to video renderer. */
+  public static synchronized void setView(GLSurfaceView surface,
+      Runnable eglContextReadyCallback) {
+    Log.d(TAG, "VideoRendererGui.setView");
+    instance = new VideoRendererGui(surface);
+    eglContextReady = eglContextReadyCallback;
+  }
+
+  public static synchronized EGLContext getEGLContext() {
+    return eglContext;
+  }
+
+  /**
+   * Creates VideoRenderer with top left corner at (x, y) and resolution
+   * (width, height). All parameters are in percentage of screen resolution.
+   */
+  public static VideoRenderer createGui(int x, int y, int width, int height,
+      ScalingType scalingType, boolean mirror) throws Exception {
+    YuvImageRenderer javaGuiRenderer = create(
+        x, y, width, height, scalingType, mirror);
+    return new VideoRenderer(javaGuiRenderer);
+  }
+
+  public static VideoRenderer.Callbacks createGuiRenderer(
+      int x, int y, int width, int height,
+      ScalingType scalingType, boolean mirror) {
+    return create(x, y, width, height, scalingType, mirror);
+  }
+
+  /**
+   * Creates VideoRenderer.Callbacks with top left corner at (x, y) and
+   * resolution (width, height). All parameters are in percentage of
+   * screen resolution.
+   */
+  public static synchronized YuvImageRenderer create(int x, int y, int width, int height,
+      ScalingType scalingType, boolean mirror) {
+    // Check display region parameters.
+    if (x < 0 || x > 100 || y < 0 || y > 100 ||
+        width < 0 || width > 100 || height < 0 || height > 100 ||
+        x + width > 100 || y + height > 100) {
+      throw new RuntimeException("Incorrect window parameters.");
+    }
+
+    if (instance == null) {
+      throw new RuntimeException(
+          "Attempt to create yuv renderer before setting GLSurfaceView");
+    }
+    final YuvImageRenderer yuvImageRenderer = new YuvImageRenderer(
+        instance.surface, instance.yuvImageRenderers.size(),
+        x, y, width, height, scalingType, mirror);
+    synchronized (instance.yuvImageRenderers) {
+      if (instance.onSurfaceCreatedCalled) {
+        // onSurfaceCreated has already been called for VideoRendererGui -
+        // need to create texture for new image and add image to the
+        // rendering list.
+        final CountDownLatch countDownLatch = new CountDownLatch(1);
+        instance.surface.queueEvent(new Runnable() {
+          public void run() {
+            yuvImageRenderer.createTextures();
+            yuvImageRenderer.setScreenSize(
+                instance.screenWidth, instance.screenHeight);
+            countDownLatch.countDown();
+          }
+        });
+        // Wait for task completion.
+        try {
+          countDownLatch.await();
+        } catch (InterruptedException e) {
+          throw new RuntimeException(e);
+        }
+      }
+      // Add yuv renderer to rendering list.
+      instance.yuvImageRenderers.add(yuvImageRenderer);
+    }
+    return yuvImageRenderer;
+  }
+
+  public static synchronized void update(
+      VideoRenderer.Callbacks renderer,
+      int x, int y, int width, int height, ScalingType scalingType, boolean mirror) {
+    Log.d(TAG, "VideoRendererGui.update");
+    if (instance == null) {
+      throw new RuntimeException(
+          "Attempt to update yuv renderer before setting GLSurfaceView");
+    }
+    synchronized (instance.yuvImageRenderers) {
+      for (YuvImageRenderer yuvImageRenderer : instance.yuvImageRenderers) {
+        if (yuvImageRenderer == renderer) {
+          yuvImageRenderer.setPosition(x, y, width, height, scalingType, mirror);
+        }
+      }
+    }
+  }
+
+  public static synchronized void setRendererEvents(
+      VideoRenderer.Callbacks renderer, RendererEvents rendererEvents) {
+    Log.d(TAG, "VideoRendererGui.setRendererEvents");
+    if (instance == null) {
+      throw new RuntimeException(
+          "Attempt to set renderer events before setting GLSurfaceView");
+    }
+    synchronized (instance.yuvImageRenderers) {
+      for (YuvImageRenderer yuvImageRenderer : instance.yuvImageRenderers) {
+        if (yuvImageRenderer == renderer) {
+          yuvImageRenderer.rendererEvents = rendererEvents;
+        }
+      }
+    }
+  }
+
+  public static synchronized void remove(VideoRenderer.Callbacks renderer) {
+    Log.d(TAG, "VideoRendererGui.remove");
+    if (instance == null) {
+      throw new RuntimeException(
+          "Attempt to remove yuv renderer before setting GLSurfaceView");
+    }
+    synchronized (instance.yuvImageRenderers) {
+      final int index = instance.yuvImageRenderers.indexOf(renderer);
+      if (index == -1) {
+        Log.w(TAG, "Couldn't remove renderer (not present in current list)");
+      } else {
+        instance.yuvImageRenderers.remove(index).release();
+      }
+    }
+  }
+
+  @SuppressLint("NewApi")
+  @Override
+  public void onSurfaceCreated(GL10 unused, EGLConfig config) {
+    Log.d(TAG, "VideoRendererGui.onSurfaceCreated");
+    // Store render EGL context.
+    if (CURRENT_SDK_VERSION >= EGL14_SDK_VERSION) {
+      synchronized (VideoRendererGui.class) {
+        eglContext = EGL14.eglGetCurrentContext();
+        Log.d(TAG, "VideoRendererGui EGL Context: " + eglContext);
+      }
+    }
+
+    synchronized (yuvImageRenderers) {
+      // Create drawer for YUV/OES frames.
+      drawer = new GlRectDrawer();
+      // Create textures for all images.
+      for (YuvImageRenderer yuvImageRenderer : yuvImageRenderers) {
+        yuvImageRenderer.createTextures();
+      }
+      onSurfaceCreatedCalled = true;
+    }
+    GlUtil.checkNoGLES2Error("onSurfaceCreated done");
+    GLES20.glPixelStorei(GLES20.GL_UNPACK_ALIGNMENT, 1);
+    GLES20.glClearColor(0.15f, 0.15f, 0.15f, 1.0f);
+
+    // Fire EGL context ready event.
+    synchronized (VideoRendererGui.class) {
+      if (eglContextReady != null) {
+        eglContextReady.run();
+      }
+    }
+  }
+
+  @Override
+  public void onSurfaceChanged(GL10 unused, int width, int height) {
+    Log.d(TAG, "VideoRendererGui.onSurfaceChanged: " +
+        width + " x " + height + "  ");
+    screenWidth = width;
+    screenHeight = height;
+    synchronized (yuvImageRenderers) {
+      for (YuvImageRenderer yuvImageRenderer : yuvImageRenderers) {
+        yuvImageRenderer.setScreenSize(screenWidth, screenHeight);
+      }
+    }
+  }
+
+  @Override
+  public void onDrawFrame(GL10 unused) {
+    GLES20.glViewport(0, 0, screenWidth, screenHeight);
+    GLES20.glClear(GLES20.GL_COLOR_BUFFER_BIT);
+    synchronized (yuvImageRenderers) {
+      for (YuvImageRenderer yuvImageRenderer : yuvImageRenderers) {
+        yuvImageRenderer.draw(drawer);
+      }
+    }
+  }
+
+}
diff --git a/librtc/src/main/java/org/webrtc/VideoSource.java b/librtc/src/main/java/org/webrtc/VideoSource.java
new file mode 100644
index 0000000..7151748
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/VideoSource.java
@@ -0,0 +1,63 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+
+package org.webrtc;
+
+/**
+ * Java version of VideoSourceInterface, extended with stop/restart
+ * functionality to allow explicit control of the camera device on android,
+ * where there is no support for multiple open capture devices and the cost of
+ * holding a camera open (even if MediaStreamTrack.setEnabled(false) is muting
+ * its output to the encoder) can be too high to bear.
+ */
+public class VideoSource extends MediaSource {
+
+  public VideoSource(long nativeSource) {
+    super(nativeSource);
+  }
+
+  // Stop capture feeding this source.
+  public void stop() {
+    stop(nativeSource);
+  }
+
+  // Restart capture feeding this source.  stop() must have been called since
+  // the last call to restart() (if any).  Note that this isn't "start()";
+  // sources are started by default at birth.
+  public void restart() {
+    restart(nativeSource);
+  }
+
+  @Override
+  public void dispose() {
+    super.dispose();
+  }
+
+  private static native void stop(long nativeSource);
+  private static native void restart(long nativeSource);
+}
diff --git a/librtc/src/main/java/org/webrtc/VideoTrack.java b/librtc/src/main/java/org/webrtc/VideoTrack.java
new file mode 100644
index 0000000..7333a90
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/VideoTrack.java
@@ -0,0 +1,68 @@
+/*
+ * libjingle
+ * Copyright 2013 Google Inc.
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions are met:
+ *
+ *  1. Redistributions of source code must retain the above copyright notice,
+ *     this list of conditions and the following disclaimer.
+ *  2. Redistributions in binary form must reproduce the above copyright notice,
+ *     this list of conditions and the following disclaimer in the documentation
+ *     and/or other materials provided with the distribution.
+ *  3. The name of the author may not be used to endorse or promote products
+ *     derived from this software without specific prior written permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR IMPLIED
+ * WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF
+ * MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO
+ * EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
+ * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
+ * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
+ * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
+ * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
+ * ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+package org.webrtc;
+
+import java.util.LinkedList;
+
+/** Java version of VideoTrackInterface. */
+public class VideoTrack extends MediaStreamTrack {
+  private final LinkedList<VideoRenderer> renderers =
+      new LinkedList<VideoRenderer>();
+
+  public VideoTrack(long nativeTrack) {
+    super(nativeTrack);
+  }
+
+  public void addRenderer(VideoRenderer renderer) {
+    renderers.add(renderer);
+    nativeAddRenderer(nativeTrack, renderer.nativeVideoRenderer);
+  }
+
+  public void removeRenderer(VideoRenderer renderer) {
+    if (!renderers.remove(renderer)) {
+      return;
+    }
+    nativeRemoveRenderer(nativeTrack, renderer.nativeVideoRenderer);
+    renderer.dispose();
+  }
+
+  public void dispose() {
+    while (!renderers.isEmpty()) {
+      removeRenderer(renderers.getFirst());
+    }
+    super.dispose();
+  }
+
+  private static native void free(long nativeTrack);
+
+  private static native void nativeAddRenderer(
+      long nativeTrack, long nativeRenderer);
+
+  private static native void nativeRemoveRenderer(
+      long nativeTrack, long nativeRenderer);
+}
diff --git a/librtc/src/main/java/org/webrtc/videoengine/ViEAndroidGLES20.java b/librtc/src/main/java/org/webrtc/videoengine/ViEAndroidGLES20.java
new file mode 100644
index 0000000..c3471d5
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/videoengine/ViEAndroidGLES20.java
@@ -0,0 +1,370 @@
+/*
+ *  Copyright (c) 2012 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.videoengine;
+
+import java.util.concurrent.locks.ReentrantLock;
+
+import javax.microedition.khronos.egl.EGL10;
+import javax.microedition.khronos.egl.EGLConfig;
+import javax.microedition.khronos.egl.EGLContext;
+import javax.microedition.khronos.egl.EGLDisplay;
+import javax.microedition.khronos.opengles.GL10;
+
+import android.app.ActivityManager;
+import android.content.Context;
+import android.content.pm.ConfigurationInfo;
+import android.graphics.PixelFormat;
+import android.opengl.GLSurfaceView;
+import android.util.Log;
+
+public class ViEAndroidGLES20 extends GLSurfaceView
+        implements GLSurfaceView.Renderer {
+    private static String TAG = "WEBRTC-JR";
+    private static final boolean DEBUG = false;
+    // True if onSurfaceCreated has been called.
+    private boolean surfaceCreated = false;
+    private boolean openGLCreated = false;
+    // True if NativeFunctionsRegistered has been called.
+    private boolean nativeFunctionsRegisted = false;
+    private ReentrantLock nativeFunctionLock = new ReentrantLock();
+    // Address of Native object that will do the drawing.
+    private long nativeObject = 0;
+    private int viewWidth = 0;
+    private int viewHeight = 0;
+
+    public static boolean UseOpenGL2(Object renderWindow) {
+        return ViEAndroidGLES20.class.isInstance(renderWindow);
+    }
+
+    public ViEAndroidGLES20(Context context) {
+        super(context);
+        init(false, 0, 0);
+    }
+
+    public ViEAndroidGLES20(Context context, boolean translucent,
+            int depth, int stencil) {
+        super(context);
+        init(translucent, depth, stencil);
+    }
+
+    private void init(boolean translucent, int depth, int stencil) {
+
+        // By default, GLSurfaceView() creates a RGB_565 opaque surface.
+        // If we want a translucent one, we should change the surface's
+        // format here, using PixelFormat.TRANSLUCENT for GL Surfaces
+        // is interpreted as any 32-bit surface with alpha by SurfaceFlinger.
+        if (translucent) {
+            this.getHolder().setFormat(PixelFormat.TRANSLUCENT);
+        }
+
+        // Setup the context factory for 2.0 rendering.
+        // See ContextFactory class definition below
+        setEGLContextFactory(new ContextFactory());
+
+        // We need to choose an EGLConfig that matches the format of
+        // our surface exactly. This is going to be done in our
+        // custom config chooser. See ConfigChooser class definition
+        // below.
+        setEGLConfigChooser( translucent ?
+                             new ConfigChooser(8, 8, 8, 8, depth, stencil) :
+                             new ConfigChooser(5, 6, 5, 0, depth, stencil) );
+
+        // Set the renderer responsible for frame rendering
+        this.setRenderer(this);
+        this.setRenderMode(GLSurfaceView.RENDERMODE_WHEN_DIRTY);
+    }
+
+    private static class ContextFactory implements GLSurfaceView.EGLContextFactory {
+        private static int EGL_CONTEXT_CLIENT_VERSION = 0x3098;
+        public EGLContext createContext(EGL10 egl, EGLDisplay display, EGLConfig eglConfig) {
+            Log.w(TAG, "creating OpenGL ES 2.0 context");
+            checkEglError("Before eglCreateContext", egl);
+            int[] attrib_list = {EGL_CONTEXT_CLIENT_VERSION, 2, EGL10.EGL_NONE };
+            EGLContext context = egl.eglCreateContext(display, eglConfig,
+                    EGL10.EGL_NO_CONTEXT, attrib_list);
+            checkEglError("After eglCreateContext", egl);
+            return context;
+        }
+
+        public void destroyContext(EGL10 egl, EGLDisplay display, EGLContext context) {
+            egl.eglDestroyContext(display, context);
+        }
+    }
+
+    private static void checkEglError(String prompt, EGL10 egl) {
+        int error;
+        while ((error = egl.eglGetError()) != EGL10.EGL_SUCCESS) {
+            Log.e(TAG, String.format("%s: EGL error: 0x%x", prompt, error));
+        }
+    }
+
+    private static class ConfigChooser implements GLSurfaceView.EGLConfigChooser {
+
+        public ConfigChooser(int r, int g, int b, int a, int depth, int stencil) {
+            mRedSize = r;
+            mGreenSize = g;
+            mBlueSize = b;
+            mAlphaSize = a;
+            mDepthSize = depth;
+            mStencilSize = stencil;
+        }
+
+        // This EGL config specification is used to specify 2.0 rendering.
+        // We use a minimum size of 4 bits for red/green/blue, but will
+        // perform actual matching in chooseConfig() below.
+        private static int EGL_OPENGL_ES2_BIT = 4;
+        private static int[] s_configAttribs2 =
+        {
+            EGL10.EGL_RED_SIZE, 4,
+            EGL10.EGL_GREEN_SIZE, 4,
+            EGL10.EGL_BLUE_SIZE, 4,
+            EGL10.EGL_RENDERABLE_TYPE, EGL_OPENGL_ES2_BIT,
+            EGL10.EGL_NONE
+        };
+
+        public EGLConfig chooseConfig(EGL10 egl, EGLDisplay display) {
+
+            // Get the number of minimally matching EGL configurations
+            int[] num_config = new int[1];
+            egl.eglChooseConfig(display, s_configAttribs2, null, 0, num_config);
+
+            int numConfigs = num_config[0];
+
+            if (numConfigs <= 0) {
+                throw new IllegalArgumentException("No configs match configSpec");
+            }
+
+            // Allocate then read the array of minimally matching EGL configs
+            EGLConfig[] configs = new EGLConfig[numConfigs];
+            egl.eglChooseConfig(display, s_configAttribs2, configs, numConfigs, num_config);
+
+            if (DEBUG) {
+                printConfigs(egl, display, configs);
+            }
+            // Now return the "best" one
+            return chooseConfig(egl, display, configs);
+        }
+
+        public EGLConfig chooseConfig(EGL10 egl, EGLDisplay display,
+                EGLConfig[] configs) {
+            for(EGLConfig config : configs) {
+                int d = findConfigAttrib(egl, display, config,
+                        EGL10.EGL_DEPTH_SIZE, 0);
+                int s = findConfigAttrib(egl, display, config,
+                        EGL10.EGL_STENCIL_SIZE, 0);
+
+                // We need at least mDepthSize and mStencilSize bits
+                if (d < mDepthSize || s < mStencilSize)
+                    continue;
+
+                // We want an *exact* match for red/green/blue/alpha
+                int r = findConfigAttrib(egl, display, config,
+                        EGL10.EGL_RED_SIZE, 0);
+                int g = findConfigAttrib(egl, display, config,
+                            EGL10.EGL_GREEN_SIZE, 0);
+                int b = findConfigAttrib(egl, display, config,
+                            EGL10.EGL_BLUE_SIZE, 0);
+                int a = findConfigAttrib(egl, display, config,
+                        EGL10.EGL_ALPHA_SIZE, 0);
+
+                if (r == mRedSize && g == mGreenSize && b == mBlueSize && a == mAlphaSize)
+                    return config;
+            }
+            return null;
+        }
+
+        private int findConfigAttrib(EGL10 egl, EGLDisplay display,
+                EGLConfig config, int attribute, int defaultValue) {
+
+            if (egl.eglGetConfigAttrib(display, config, attribute, mValue)) {
+                return mValue[0];
+            }
+            return defaultValue;
+        }
+
+        private void printConfigs(EGL10 egl, EGLDisplay display,
+            EGLConfig[] configs) {
+            int numConfigs = configs.length;
+            Log.w(TAG, String.format("%d configurations", numConfigs));
+            for (int i = 0; i < numConfigs; i++) {
+                Log.w(TAG, String.format("Configuration %d:\n", i));
+                printConfig(egl, display, configs[i]);
+            }
+        }
+
+        private void printConfig(EGL10 egl, EGLDisplay display,
+                EGLConfig config) {
+            int[] attributes = {
+                    EGL10.EGL_BUFFER_SIZE,
+                    EGL10.EGL_ALPHA_SIZE,
+                    EGL10.EGL_BLUE_SIZE,
+                    EGL10.EGL_GREEN_SIZE,
+                    EGL10.EGL_RED_SIZE,
+                    EGL10.EGL_DEPTH_SIZE,
+                    EGL10.EGL_STENCIL_SIZE,
+                    EGL10.EGL_CONFIG_CAVEAT,
+                    EGL10.EGL_CONFIG_ID,
+                    EGL10.EGL_LEVEL,
+                    EGL10.EGL_MAX_PBUFFER_HEIGHT,
+                    EGL10.EGL_MAX_PBUFFER_PIXELS,
+                    EGL10.EGL_MAX_PBUFFER_WIDTH,
+                    EGL10.EGL_NATIVE_RENDERABLE,
+                    EGL10.EGL_NATIVE_VISUAL_ID,
+                    EGL10.EGL_NATIVE_VISUAL_TYPE,
+                    0x3030, // EGL10.EGL_PRESERVED_RESOURCES,
+                    EGL10.EGL_SAMPLES,
+                    EGL10.EGL_SAMPLE_BUFFERS,
+                    EGL10.EGL_SURFACE_TYPE,
+                    EGL10.EGL_TRANSPARENT_TYPE,
+                    EGL10.EGL_TRANSPARENT_RED_VALUE,
+                    EGL10.EGL_TRANSPARENT_GREEN_VALUE,
+                    EGL10.EGL_TRANSPARENT_BLUE_VALUE,
+                    0x3039, // EGL10.EGL_BIND_TO_TEXTURE_RGB,
+                    0x303A, // EGL10.EGL_BIND_TO_TEXTURE_RGBA,
+                    0x303B, // EGL10.EGL_MIN_SWAP_INTERVAL,
+                    0x303C, // EGL10.EGL_MAX_SWAP_INTERVAL,
+                    EGL10.EGL_LUMINANCE_SIZE,
+                    EGL10.EGL_ALPHA_MASK_SIZE,
+                    EGL10.EGL_COLOR_BUFFER_TYPE,
+                    EGL10.EGL_RENDERABLE_TYPE,
+                    0x3042 // EGL10.EGL_CONFORMANT
+            };
+            String[] names = {
+                    "EGL_BUFFER_SIZE",
+                    "EGL_ALPHA_SIZE",
+                    "EGL_BLUE_SIZE",
+                    "EGL_GREEN_SIZE",
+                    "EGL_RED_SIZE",
+                    "EGL_DEPTH_SIZE",
+                    "EGL_STENCIL_SIZE",
+                    "EGL_CONFIG_CAVEAT",
+                    "EGL_CONFIG_ID",
+                    "EGL_LEVEL",
+                    "EGL_MAX_PBUFFER_HEIGHT",
+                    "EGL_MAX_PBUFFER_PIXELS",
+                    "EGL_MAX_PBUFFER_WIDTH",
+                    "EGL_NATIVE_RENDERABLE",
+                    "EGL_NATIVE_VISUAL_ID",
+                    "EGL_NATIVE_VISUAL_TYPE",
+                    "EGL_PRESERVED_RESOURCES",
+                    "EGL_SAMPLES",
+                    "EGL_SAMPLE_BUFFERS",
+                    "EGL_SURFACE_TYPE",
+                    "EGL_TRANSPARENT_TYPE",
+                    "EGL_TRANSPARENT_RED_VALUE",
+                    "EGL_TRANSPARENT_GREEN_VALUE",
+                    "EGL_TRANSPARENT_BLUE_VALUE",
+                    "EGL_BIND_TO_TEXTURE_RGB",
+                    "EGL_BIND_TO_TEXTURE_RGBA",
+                    "EGL_MIN_SWAP_INTERVAL",
+                    "EGL_MAX_SWAP_INTERVAL",
+                    "EGL_LUMINANCE_SIZE",
+                    "EGL_ALPHA_MASK_SIZE",
+                    "EGL_COLOR_BUFFER_TYPE",
+                    "EGL_RENDERABLE_TYPE",
+                    "EGL_CONFORMANT"
+            };
+            int[] value = new int[1];
+            for (int i = 0; i < attributes.length; i++) {
+                int attribute = attributes[i];
+                String name = names[i];
+                if (egl.eglGetConfigAttrib(display, config, attribute, value)) {
+                    Log.w(TAG, String.format("  %s: %d\n", name, value[0]));
+                } else {
+                    // Log.w(TAG, String.format("  %s: failed\n", name));
+                    while (egl.eglGetError() != EGL10.EGL_SUCCESS);
+                }
+            }
+        }
+
+        // Subclasses can adjust these values:
+        protected int mRedSize;
+        protected int mGreenSize;
+        protected int mBlueSize;
+        protected int mAlphaSize;
+        protected int mDepthSize;
+        protected int mStencilSize;
+        private int[] mValue = new int[1];
+    }
+
+    // IsSupported
+    // Return true if this device support Open GL ES 2.0 rendering.
+    public static boolean IsSupported(Context context) {
+        ActivityManager am =
+                (ActivityManager) context.getSystemService(Context.ACTIVITY_SERVICE);
+        ConfigurationInfo info = am.getDeviceConfigurationInfo();
+        if(info.reqGlEsVersion >= 0x20000) {
+            // Open GL ES 2.0 is supported.
+            return true;
+        }
+        return false;
+    }
+
+    public void onDrawFrame(GL10 gl) {
+        nativeFunctionLock.lock();
+        if(!nativeFunctionsRegisted || !surfaceCreated) {
+            nativeFunctionLock.unlock();
+            return;
+        }
+
+        if(!openGLCreated) {
+            if(0 != CreateOpenGLNative(nativeObject, viewWidth, viewHeight)) {
+                return; // Failed to create OpenGL
+            }
+            openGLCreated = true; // Created OpenGL successfully
+        }
+        DrawNative(nativeObject); // Draw the new frame
+        nativeFunctionLock.unlock();
+    }
+
+    public void onSurfaceChanged(GL10 gl, int width, int height) {
+        surfaceCreated = true;
+        viewWidth = width;
+        viewHeight = height;
+
+        nativeFunctionLock.lock();
+        if(nativeFunctionsRegisted) {
+            if(CreateOpenGLNative(nativeObject,width,height) == 0)
+                openGLCreated = true;
+        }
+        nativeFunctionLock.unlock();
+    }
+
+    public void onSurfaceCreated(GL10 gl, EGLConfig config) {
+    }
+
+    public void RegisterNativeObject(long nativeObject) {
+        nativeFunctionLock.lock();
+        this.nativeObject = nativeObject;
+        nativeFunctionsRegisted = true;
+        nativeFunctionLock.unlock();
+    }
+
+    public void DeRegisterNativeObject() {
+        nativeFunctionLock.lock();
+        nativeFunctionsRegisted = false;
+        openGLCreated = false;
+        this.nativeObject = 0;
+        nativeFunctionLock.unlock();
+    }
+
+    public void ReDraw() {
+        if(surfaceCreated) {
+            // Request the renderer to redraw using the render thread context.
+            this.requestRender();
+        }
+    }
+
+    private native int CreateOpenGLNative(long nativeObject,
+            int width, int height);
+    private native void DrawNative(long nativeObject);
+
+}
diff --git a/librtc/src/main/java/org/webrtc/videoengine/ViERenderer.java b/librtc/src/main/java/org/webrtc/videoengine/ViERenderer.java
new file mode 100644
index 0000000..50b1a59
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/videoengine/ViERenderer.java
@@ -0,0 +1,29 @@
+/*
+ *  Copyright (c) 2012 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.videoengine;
+
+import android.content.Context;
+import android.view.SurfaceHolder;
+import android.view.SurfaceView;
+
+public class ViERenderer {
+    public static SurfaceView CreateRenderer(Context context) {
+        return CreateRenderer(context, false);
+    }
+
+    public static SurfaceView CreateRenderer(Context context,
+            boolean useOpenGLES2) {
+        if(useOpenGLES2 == true && ViEAndroidGLES20.IsSupported(context))
+            return new ViEAndroidGLES20(context);
+        else
+            return new SurfaceView(context);
+    }
+}
diff --git a/librtc/src/main/java/org/webrtc/videoengine/ViESurfaceRenderer.java b/librtc/src/main/java/org/webrtc/videoengine/ViESurfaceRenderer.java
new file mode 100644
index 0000000..1fda021
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/videoengine/ViESurfaceRenderer.java
@@ -0,0 +1,184 @@
+/*
+ *  Copyright (c) 2012 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.videoengine;
+
+// The following four imports are needed saveBitmapToJPEG which
+// is for debug only
+import java.io.ByteArrayOutputStream;
+import java.io.FileNotFoundException;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import android.graphics.Bitmap;
+import android.graphics.Canvas;
+import android.graphics.Rect;
+import android.util.Log;
+import android.view.SurfaceHolder;
+import android.view.SurfaceView;
+import android.view.SurfaceHolder.Callback;
+
+public class ViESurfaceRenderer implements Callback {
+
+    private final static String TAG = "WEBRTC";
+
+    // the bitmap used for drawing.
+    private Bitmap bitmap = null;
+    private ByteBuffer byteBuffer = null;
+    private SurfaceHolder surfaceHolder;
+    // Rect of the source bitmap to draw
+    private Rect srcRect = new Rect();
+    // Rect of the destination canvas to draw to
+    private Rect dstRect = new Rect();
+    private float dstTopScale = 0;
+    private float dstBottomScale = 1;
+    private float dstLeftScale = 0;
+    private float dstRightScale = 1;
+
+    public ViESurfaceRenderer(SurfaceView view) {
+        surfaceHolder = view.getHolder();
+        if(surfaceHolder == null)
+            return;
+        surfaceHolder.addCallback(this);
+    }
+
+    // surfaceChanged and surfaceCreated share this function
+    private void changeDestRect(int dstWidth, int dstHeight) {
+        dstRect.right = (int)(dstRect.left + dstRightScale * dstWidth);
+        dstRect.bottom = (int)(dstRect.top + dstBottomScale * dstHeight);
+    }
+
+    public void surfaceChanged(SurfaceHolder holder, int format,
+            int in_width, int in_height) {
+        Log.d(TAG, "ViESurfaceRender::surfaceChanged");
+
+        changeDestRect(in_width, in_height);
+
+        Log.d(TAG, "ViESurfaceRender::surfaceChanged" +
+                " in_width:" + in_width + " in_height:" + in_height +
+                " srcRect.left:" + srcRect.left +
+                " srcRect.top:" + srcRect.top +
+                " srcRect.right:" + srcRect.right +
+                " srcRect.bottom:" + srcRect.bottom +
+                " dstRect.left:" + dstRect.left +
+                " dstRect.top:" + dstRect.top +
+                " dstRect.right:" + dstRect.right +
+                " dstRect.bottom:" + dstRect.bottom);
+    }
+
+    public void surfaceCreated(SurfaceHolder holder) {
+        Canvas canvas = surfaceHolder.lockCanvas();
+        if(canvas != null) {
+            Rect dst = surfaceHolder.getSurfaceFrame();
+            if(dst != null) {
+                changeDestRect(dst.right - dst.left, dst.bottom - dst.top);
+                Log.d(TAG, "ViESurfaceRender::surfaceCreated" +
+                        " dst.left:" + dst.left +
+                        " dst.top:" + dst.top +
+                        " dst.right:" + dst.right +
+                        " dst.bottom:" + dst.bottom +
+                        " srcRect.left:" + srcRect.left +
+                        " srcRect.top:" + srcRect.top +
+                        " srcRect.right:" + srcRect.right +
+                        " srcRect.bottom:" + srcRect.bottom +
+                        " dstRect.left:" + dstRect.left +
+                        " dstRect.top:" + dstRect.top +
+                        " dstRect.right:" + dstRect.right +
+                        " dstRect.bottom:" + dstRect.bottom);
+            }
+            surfaceHolder.unlockCanvasAndPost(canvas);
+        }
+    }
+
+    public void surfaceDestroyed(SurfaceHolder holder) {
+        Log.d(TAG, "ViESurfaceRenderer::surfaceDestroyed");
+        bitmap = null;
+        byteBuffer = null;
+    }
+
+    public Bitmap CreateBitmap(int width, int height) {
+        Log.d(TAG, "CreateByteBitmap " + width + ":" + height);
+        if (bitmap == null) {
+            try {
+                android.os.Process.setThreadPriority(
+                    android.os.Process.THREAD_PRIORITY_DISPLAY);
+            }
+            catch (Exception e) {
+            }
+        }
+        bitmap = Bitmap.createBitmap(width, height, Bitmap.Config.RGB_565);
+        srcRect.left = 0;
+        srcRect.top = 0;
+        srcRect.bottom = height;
+        srcRect.right = width;
+        return bitmap;
+    }
+
+    public ByteBuffer CreateByteBuffer(int width, int height) {
+        Log.d(TAG, "CreateByteBuffer " + width + ":" + height);
+        if (bitmap == null) {
+            bitmap = CreateBitmap(width, height);
+            byteBuffer = ByteBuffer.allocateDirect(width * height * 2);
+        }
+        return byteBuffer;
+    }
+
+    public void SetCoordinates(float left, float top,
+            float right, float bottom) {
+        Log.d(TAG, "SetCoordinates " + left + "," + top + ":" +
+                right + "," + bottom);
+        dstLeftScale = left;
+        dstTopScale = top;
+        dstRightScale = right;
+        dstBottomScale = bottom;
+    }
+
+    // It saves bitmap data to a JPEG picture, this function is for debug only.
+    private void saveBitmapToJPEG(int width, int height) {
+        ByteArrayOutputStream byteOutStream = new ByteArrayOutputStream();
+        bitmap.compress(Bitmap.CompressFormat.JPEG, 100, byteOutStream);
+
+        try{
+            FileOutputStream output = new FileOutputStream(String.format(
+                "/sdcard/render_%d.jpg", System.currentTimeMillis()));
+            output.write(byteOutStream.toByteArray());
+            output.flush();
+            output.close();
+        }
+        catch (FileNotFoundException e) {
+        }
+        catch (IOException e) {
+        }
+    }
+
+    public void DrawByteBuffer() {
+        if(byteBuffer == null)
+            return;
+        byteBuffer.rewind();
+        bitmap.copyPixelsFromBuffer(byteBuffer);
+        DrawBitmap();
+    }
+
+    public void DrawBitmap() {
+        if(bitmap == null)
+            return;
+
+        Canvas canvas = surfaceHolder.lockCanvas();
+        if(canvas != null) {
+            // The follow line is for debug only
+            // saveBitmapToJPEG(srcRect.right - srcRect.left,
+            //                  srcRect.bottom - srcRect.top);
+            canvas.drawBitmap(bitmap, srcRect, dstRect, null);
+            surfaceHolder.unlockCanvasAndPost(canvas);
+        }
+    }
+
+}
diff --git a/librtc/src/main/java/org/webrtc/voiceengine/BuildInfo.java b/librtc/src/main/java/org/webrtc/voiceengine/BuildInfo.java
new file mode 100644
index 0000000..9f025c4
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/voiceengine/BuildInfo.java
@@ -0,0 +1,52 @@
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.voiceengine;
+
+import android.os.Build;
+import android.util.Log;
+
+public final class BuildInfo {
+  public static String getDevice() {
+    return Build.DEVICE;
+  }
+
+  public static String getDeviceModel() {
+    return Build.MODEL;
+  }
+
+  public static String getProduct() {
+    return Build.PRODUCT;
+  }
+
+  public static String getBrand() {
+    return Build.BRAND;
+  }
+
+  public static String getDeviceManufacturer() {
+    return Build.MANUFACTURER;
+  }
+
+  public static String getAndroidBuildId() {
+    return Build.ID;
+  }
+
+  public static String getBuildType() {
+    return Build.TYPE;
+  }
+
+  public static String getBuildRelease() {
+    return Build.VERSION.RELEASE;
+  }
+
+  public static String getSdkVersion() {
+    return Integer.toString(Build.VERSION.SDK_INT);
+  }
+}
diff --git a/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioManager.java b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioManager.java
new file mode 100644
index 0000000..9396c49
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioManager.java
@@ -0,0 +1,258 @@
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.voiceengine;
+
+import android.content.Context;
+import android.content.pm.PackageManager;
+import android.media.AudioFormat;
+import android.media.AudioManager;
+import android.media.AudioRecord;
+import android.media.AudioTrack;
+import android.os.Build;
+import android.util.Log;
+
+import java.lang.Math;
+
+// WebRtcAudioManager handles tasks that uses android.media.AudioManager.
+// At construction, storeAudioParameters() is called and it retrieves
+// fundamental audio parameters like native sample rate and number of channels.
+// The result is then provided to the caller by nativeCacheAudioParameters().
+// It is also possible to call init() to set up the audio environment for best
+// possible "VoIP performance". All settings done in init() are reverted by
+// dispose(). This class can also be used without calling init() if the user
+// prefers to set up the audio environment separately. However, it is
+// recommended to always use AudioManager.MODE_IN_COMMUNICATION.
+// This class also adds support for output volume control of the
+// STREAM_VOICE_CALL-type stream.
+class WebRtcAudioManager {
+  private static final boolean DEBUG = false;
+
+  private static final String TAG = "WebRtcAudioManager";
+
+  // Default audio data format is PCM 16 bit per sample.
+  // Guaranteed to be supported by all devices.
+  private static final int BITS_PER_SAMPLE = 16;
+
+   // Use 44.1kHz as the default sampling rate.
+  private static final int SAMPLE_RATE_HZ = 44100;
+
+  // TODO(henrika): add stereo support for playout.
+  private static final int CHANNELS = 1;
+
+  // List of possible audio modes.
+  private static final String[] AUDIO_MODES = new String[] {
+      "MODE_NORMAL",
+      "MODE_RINGTONE",
+      "MODE_IN_CALL",
+      "MODE_IN_COMMUNICATION",
+  };
+
+  private static final int DEFAULT_FRAME_PER_BUFFER = 256;
+
+  private final long nativeAudioManager;
+  private final Context context;
+  private final AudioManager audioManager;
+
+  private boolean initialized = false;
+  private int nativeSampleRate;
+  private int nativeChannels;
+
+  private boolean hardwareAEC;
+  private boolean lowLatencyOutput;
+  private int sampleRate;
+  private int channels;
+  private int outputBufferSize;
+  private int inputBufferSize;
+
+  WebRtcAudioManager(Context context, long nativeAudioManager) {
+    Logd("ctor" + WebRtcAudioUtils.getThreadInfo());
+    this.context = context;
+    this.nativeAudioManager = nativeAudioManager;
+    audioManager = (AudioManager) context.getSystemService(
+        Context.AUDIO_SERVICE);
+    if (DEBUG) {
+      WebRtcAudioUtils.logDeviceInfo(TAG);
+    }
+    storeAudioParameters();
+    nativeCacheAudioParameters(
+        sampleRate, channels, hardwareAEC, lowLatencyOutput, outputBufferSize,
+        inputBufferSize, nativeAudioManager);
+  }
+
+  private boolean init() {
+    Logd("init" + WebRtcAudioUtils.getThreadInfo());
+    if (initialized) {
+      return true;
+    }
+    Logd("audio mode is: " + AUDIO_MODES[audioManager.getMode()]);
+    initialized = true;
+    return true;
+  }
+
+  private void dispose() {
+    Logd("dispose" + WebRtcAudioUtils.getThreadInfo());
+    if (!initialized) {
+      return;
+    }
+  }
+
+  private boolean isCommunicationModeEnabled() {
+    return (audioManager.getMode() == AudioManager.MODE_IN_COMMUNICATION);
+  }
+
+   private boolean isDeviceBlacklistedForOpenSLESUsage() {
+    boolean blacklisted =
+        WebRtcAudioUtils.deviceIsBlacklistedForOpenSLESUsage();
+    if (blacklisted) {
+      // TODO(henrika): enable again for all devices once issue in b/21485703
+      // has been resolved.
+      Loge(Build.MODEL + " is blacklisted for OpenSL ES usage!");
+    }
+    return blacklisted;
+  }
+
+  private void storeAudioParameters() {
+    // Only mono is supported currently (in both directions).
+    // TODO(henrika): add support for stereo playout.
+    channels = CHANNELS;
+    sampleRate = getNativeOutputSampleRate();
+    hardwareAEC = isAcousticEchoCancelerSupported();
+    lowLatencyOutput = isLowLatencyOutputSupported();
+    outputBufferSize = lowLatencyOutput ?
+        getLowLatencyOutputFramesPerBuffer() :
+        getMinOutputFrameSize(sampleRate, channels);
+    // TODO(henrika): add support for low-latency input.
+    inputBufferSize = getMinInputFrameSize(sampleRate, channels);
+  }
+
+  // Gets the current earpiece state.
+  private boolean hasEarpiece() {
+    return context.getPackageManager().hasSystemFeature(
+        PackageManager.FEATURE_TELEPHONY);
+  }
+
+  // Returns true if low-latency audio output is supported.
+  private boolean isLowLatencyOutputSupported() {
+    return isOpenSLESSupported() &&
+        context.getPackageManager().hasSystemFeature(
+            PackageManager.FEATURE_AUDIO_LOW_LATENCY);
+  }
+
+  // Returns true if low-latency audio input is supported.
+  public boolean isLowLatencyInputSupported() {
+    // TODO(henrika): investigate if some sort of device list is needed here
+    // as well. The NDK doc states that: "As of API level 21, lower latency
+    // audio input is supported on select devices. To take advantage of this
+    // feature, first confirm that lower latency output is available".
+    return WebRtcAudioUtils.runningOnLollipopOrHigher() &&
+        isLowLatencyOutputSupported();
+  }
+
+  // Returns the native output sample rate for this device's output stream.
+  private int getNativeOutputSampleRate() {
+    // Override this if we're running on an old emulator image which only
+    // supports 8 kHz and doesn't support PROPERTY_OUTPUT_SAMPLE_RATE.
+    if (WebRtcAudioUtils.runningOnEmulator()) {
+      Logd("Running on old emulator, overriding sampling rate to 8 kHz.");
+      return 8000;
+    }
+    if (!WebRtcAudioUtils.runningOnJellyBeanMR1OrHigher()) {
+      return SAMPLE_RATE_HZ;
+    }
+    String sampleRateString = audioManager.getProperty(
+        AudioManager.PROPERTY_OUTPUT_SAMPLE_RATE);
+    return (sampleRateString == null) ?
+        SAMPLE_RATE_HZ : Integer.parseInt(sampleRateString);
+  }
+
+  // Returns the native output buffer size for low-latency output streams.
+  private int getLowLatencyOutputFramesPerBuffer() {
+    assertTrue(isLowLatencyOutputSupported());
+    if (!WebRtcAudioUtils.runningOnJellyBeanMR1OrHigher()) {
+      return DEFAULT_FRAME_PER_BUFFER;
+    }
+    String framesPerBuffer = audioManager.getProperty(
+        AudioManager.PROPERTY_OUTPUT_FRAMES_PER_BUFFER);
+    return framesPerBuffer == null ?
+        DEFAULT_FRAME_PER_BUFFER : Integer.parseInt(framesPerBuffer);
+  }
+
+  // Returns true if the device supports Acoustic Echo Canceler (AEC).
+  // Also takes blacklisting into account.
+  private static boolean isAcousticEchoCancelerSupported() {
+    if (WebRtcAudioUtils.deviceIsBlacklistedForHwAecUsage()) {
+      Logd(Build.MODEL + " is blacklisted for HW AEC usage!");
+      return false;
+    }
+    return WebRtcAudioUtils.isAcousticEchoCancelerSupported();
+  }
+
+  // Returns the minimum output buffer size for Java based audio (AudioTrack).
+  // This size can also be used for OpenSL ES implementations on devices that
+  // lacks support of low-latency output.
+  private static int getMinOutputFrameSize(int sampleRateInHz, int numChannels) {
+    final int bytesPerFrame = numChannels * (BITS_PER_SAMPLE / 8);
+    final int channelConfig;
+    if (numChannels == 1) {
+      channelConfig = AudioFormat.CHANNEL_OUT_MONO;
+    } else if (numChannels == 2) {
+      channelConfig = AudioFormat.CHANNEL_OUT_STEREO;
+    } else {
+      return -1;
+    }
+    return AudioTrack.getMinBufferSize(
+        sampleRateInHz, channelConfig, AudioFormat.ENCODING_PCM_16BIT) /
+        bytesPerFrame;
+  }
+
+  // Returns the native input buffer size for input streams.
+  private int getLowLatencyInputFramesPerBuffer() {
+    assertTrue(isLowLatencyInputSupported());
+    return getLowLatencyOutputFramesPerBuffer();
+  }
+
+  // Returns the minimum input buffer size for Java based audio (AudioRecord).
+  // This size can calso be used for OpenSL ES implementations on devices that
+  // lacks support of low-latency input.
+  private static int getMinInputFrameSize(int sampleRateInHz, int numChannels) {
+    final int bytesPerFrame = numChannels * (BITS_PER_SAMPLE / 8);
+    assertTrue(numChannels == CHANNELS);
+    return AudioRecord.getMinBufferSize(sampleRateInHz,
+        AudioFormat.CHANNEL_IN_MONO, AudioFormat.ENCODING_PCM_16BIT) /
+        bytesPerFrame;
+  }
+
+  // Returns true if OpenSL ES audio is supported.
+  private static boolean isOpenSLESSupported() {
+    // Check for API level 9 or higher, to confirm use of OpenSL ES.
+    return WebRtcAudioUtils.runningOnGingerBreadOrHigher();
+  }
+
+  // Helper method which throws an exception  when an assertion has failed.
+  private static void assertTrue(boolean condition) {
+    if (!condition) {
+      throw new AssertionError("Expected condition to be true");
+    }
+  }
+
+  private static void Logd(String msg) {
+    Log.d(TAG, msg);
+  }
+
+  private static void Loge(String msg) {
+    Log.e(TAG, msg);
+  }
+
+  private native void nativeCacheAudioParameters(
+    int sampleRate, int channels, boolean hardwareAEC, boolean lowLatencyOutput,
+    int outputBufferSize, int inputBufferSize,
+    long nativeAudioManager);
+}
diff --git a/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioRecord.java b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioRecord.java
new file mode 100644
index 0000000..f81bab3
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioRecord.java
@@ -0,0 +1,282 @@
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.voiceengine;
+
+import java.lang.System;
+import java.nio.ByteBuffer;
+import java.util.concurrent.TimeUnit;
+
+import android.content.Context;
+import android.media.AudioFormat;
+import android.media.audiofx.AcousticEchoCanceler;
+import android.media.audiofx.AudioEffect;
+import android.media.audiofx.AudioEffect.Descriptor;
+import android.media.AudioRecord;
+import android.media.MediaRecorder.AudioSource;
+import android.os.Build;
+import android.os.Process;
+import android.os.SystemClock;
+import android.util.Log;
+
+class  WebRtcAudioRecord {
+  private static final boolean DEBUG = false;
+
+  private static final String TAG = "WebRtcAudioRecord";
+
+  // Default audio data format is PCM 16 bit per sample.
+  // Guaranteed to be supported by all devices.
+  private static final int BITS_PER_SAMPLE = 16;
+
+  // Requested size of each recorded buffer provided to the client.
+  private static final int CALLBACK_BUFFER_SIZE_MS = 10;
+
+  // Average number of callbacks per second.
+  private static final int BUFFERS_PER_SECOND = 1000 / CALLBACK_BUFFER_SIZE_MS;
+
+  private final long nativeAudioRecord;
+  private final Context context;
+
+  private ByteBuffer byteBuffer;
+
+  private AudioRecord audioRecord = null;
+  private AudioRecordThread audioThread = null;
+
+  private AcousticEchoCanceler aec = null;
+  private boolean useBuiltInAEC = false;
+
+  /**
+   * Audio thread which keeps calling ByteBuffer.read() waiting for audio
+   * to be recorded. Feeds recorded data to the native counterpart as a
+   * periodic sequence of callbacks using DataIsRecorded().
+   * This thread uses a Process.THREAD_PRIORITY_URGENT_AUDIO priority.
+   */
+  private class AudioRecordThread extends Thread {
+    private volatile boolean keepAlive = true;
+
+    public AudioRecordThread(String name) {
+      super(name);
+    }
+
+    @Override
+    public void run() {
+      Process.setThreadPriority(Process.THREAD_PRIORITY_URGENT_AUDIO);
+      Logd("AudioRecordThread" + WebRtcAudioUtils.getThreadInfo());
+      assertTrue(audioRecord.getRecordingState()
+          == AudioRecord.RECORDSTATE_RECORDING);
+
+      long lastTime = System.nanoTime();
+      while (keepAlive) {
+        int bytesRead = audioRecord.read(byteBuffer, byteBuffer.capacity());
+        if (bytesRead == byteBuffer.capacity()) {
+          nativeDataIsRecorded(bytesRead, nativeAudioRecord);
+        } else {
+          Loge("AudioRecord.read failed: " + bytesRead);
+          if (bytesRead == AudioRecord.ERROR_INVALID_OPERATION) {
+            keepAlive = false;
+          }
+        }
+        if (DEBUG) {
+          long nowTime = System.nanoTime();
+          long durationInMs =
+              TimeUnit.NANOSECONDS.toMillis((nowTime - lastTime));
+          lastTime = nowTime;
+          Logd("bytesRead[" + durationInMs + "] " + bytesRead);
+        }
+      }
+
+      try {
+        audioRecord.stop();
+      } catch (IllegalStateException e) {
+        Loge("AudioRecord.stop failed: " + e.getMessage());
+      }
+    }
+
+    public void joinThread() {
+      keepAlive = false;
+      while (isAlive()) {
+        try {
+          join();
+        } catch (InterruptedException e) {
+          // Ignore.
+        }
+      }
+    }
+  }
+
+  WebRtcAudioRecord(Context context, long nativeAudioRecord) {
+    Logd("ctor" + WebRtcAudioUtils.getThreadInfo());
+    this.context = context;
+    this.nativeAudioRecord = nativeAudioRecord;
+    if (DEBUG) {
+      WebRtcAudioUtils.logDeviceInfo(TAG);
+    }
+  }
+
+  private boolean EnableBuiltInAEC(boolean enable) {
+    Logd("EnableBuiltInAEC(" + enable + ')');
+    assertTrue(WebRtcAudioUtils.isAcousticEchoCancelerApproved());
+    // Store the AEC state.
+    useBuiltInAEC = enable;
+    // Set AEC state if AEC has already been created.
+    if (aec != null) {
+      int ret = aec.setEnabled(enable);
+      if (ret != AudioEffect.SUCCESS) {
+        Loge("AcousticEchoCanceler.setEnabled failed");
+        return false;
+      }
+      Logd("AcousticEchoCanceler.getEnabled: " + aec.getEnabled());
+    }
+    return true;
+  }
+
+  private int InitRecording(int sampleRate, int channels) {
+    Logd("InitRecording(sampleRate=" + sampleRate + ", channels=" +
+        channels + ")");
+    if (!WebRtcAudioUtils.hasPermission(
+        context, android.Manifest.permission.RECORD_AUDIO)) {
+      Loge("RECORD_AUDIO permission is missing");
+      return -1;
+    }
+    final int bytesPerFrame = channels * (BITS_PER_SAMPLE / 8);
+    final int framesPerBuffer = sampleRate / BUFFERS_PER_SECOND;
+    byteBuffer = ByteBuffer.allocateDirect(bytesPerFrame * framesPerBuffer);
+    Logd("byteBuffer.capacity: " + byteBuffer.capacity());
+    // Rather than passing the ByteBuffer with every callback (requiring
+    // the potentially expensive GetDirectBufferAddress) we simply have the
+    // the native class cache the address to the memory once.
+    nativeCacheDirectBufferAddress(byteBuffer, nativeAudioRecord);
+
+    // Get the minimum buffer size required for the successful creation of
+    // an AudioRecord object, in byte units.
+    // Note that this size doesn't guarantee a smooth recording under load.
+    // TODO(henrika): Do we need to make this larger to avoid underruns?
+    int minBufferSize = AudioRecord.getMinBufferSize(
+          sampleRate,
+          AudioFormat.CHANNEL_IN_MONO,
+          AudioFormat.ENCODING_PCM_16BIT);
+    Logd("AudioRecord.getMinBufferSize: " + minBufferSize);
+
+    if (aec != null) {
+      aec.release();
+      aec = null;
+    }
+    assertTrue(audioRecord == null);
+
+    int bufferSizeInBytes = Math.max(byteBuffer.capacity(), minBufferSize);
+    Logd("bufferSizeInBytes: " + bufferSizeInBytes);
+    try {
+      audioRecord = new AudioRecord(AudioSource.VOICE_COMMUNICATION,
+                                    sampleRate,
+                                    AudioFormat.CHANNEL_IN_MONO,
+                                    AudioFormat.ENCODING_PCM_16BIT,
+                                    bufferSizeInBytes);
+
+    } catch (IllegalArgumentException e) {
+      Logd(e.getMessage());
+      return -1;
+    }
+    assertTrue(audioRecord.getState() == AudioRecord.STATE_INITIALIZED);
+
+    Logd("AudioRecord " +
+          "session ID: " + audioRecord.getAudioSessionId() + ", " +
+          "audio format: " + audioRecord.getAudioFormat() + ", " +
+          "channels: " + audioRecord.getChannelCount() + ", " +
+          "sample rate: " + audioRecord.getSampleRate());
+    Logd("AcousticEchoCanceler.isAvailable: " + builtInAECIsAvailable());
+    if (!builtInAECIsAvailable()) {
+      return framesPerBuffer;
+    }
+    if (WebRtcAudioUtils.deviceIsBlacklistedForHwAecUsage()) {
+      // Just in case, ensure that no attempt has been done to enable the
+      // HW AEC on a blacklisted device.
+      assertTrue(!useBuiltInAEC);
+    }
+    // We create an AEC also for blacklisted devices since it is possible that
+    // HW EAC is enabled by default. Hence, the AEC object is needed to be
+    // able to check the current state and to disable the AEC if enabled.
+    aec = AcousticEchoCanceler.create(audioRecord.getAudioSessionId());
+    if (aec == null) {
+      Loge("AcousticEchoCanceler.create failed");
+      return -1;
+    }
+    int ret = aec.setEnabled(useBuiltInAEC);
+    if (ret != AudioEffect.SUCCESS) {
+      Loge("AcousticEchoCanceler.setEnabled failed");
+      return -1;
+    }
+    Descriptor descriptor = aec.getDescriptor();
+    Logd("AcousticEchoCanceler " +
+          "name: " + descriptor.name + ", " +
+          "implementor: " + descriptor.implementor + ", " +
+          "uuid: " + descriptor.uuid);
+    Logd("AcousticEchoCanceler.getEnabled: " + aec.getEnabled());
+    return framesPerBuffer;
+  }
+
+  private boolean StartRecording() {
+    Logd("StartRecording");
+    assertTrue(audioRecord != null);
+    assertTrue(audioThread == null);
+    try {
+      audioRecord.startRecording();
+    } catch (IllegalStateException e) {
+      Loge("AudioRecord.startRecording failed: " + e.getMessage());
+      return false;
+    }
+    if (audioRecord.getRecordingState() != AudioRecord.RECORDSTATE_RECORDING) {
+      Loge("AudioRecord.startRecording failed");
+      return false;
+    }
+    audioThread = new AudioRecordThread("AudioRecordJavaThread");
+    audioThread.start();
+    return true;
+  }
+
+  private boolean StopRecording() {
+    Logd("StopRecording");
+    assertTrue(audioThread != null);
+    audioThread.joinThread();
+    audioThread = null;
+    if (aec != null) {
+      aec.release();
+      aec = null;
+    }
+    audioRecord.release();
+    audioRecord = null;
+    return true;
+  }
+
+  // Returns true if built-in AEC is available. Does not take blacklisting
+  // into account.
+  private static boolean builtInAECIsAvailable() {
+    return WebRtcAudioUtils.isAcousticEchoCancelerSupported();
+  }
+
+  // Helper method which throws an exception  when an assertion has failed.
+  private static void assertTrue(boolean condition) {
+    if (!condition) {
+      throw new AssertionError("Expected condition to be true");
+    }
+  }
+
+  private static void Logd(String msg) {
+    Log.d(TAG, msg);
+  }
+
+  private static void Loge(String msg) {
+    Log.e(TAG, msg);
+  }
+
+  private native void nativeCacheDirectBufferAddress(
+      ByteBuffer byteBuffer, long nativeAudioRecord);
+
+  private native void nativeDataIsRecorded(int bytes, long nativeAudioRecord);
+}
diff --git a/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioTrack.java b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioTrack.java
new file mode 100644
index 0000000..da0980e
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioTrack.java
@@ -0,0 +1,262 @@
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.voiceengine;
+
+import java.lang.Thread;
+import java.nio.ByteBuffer;
+
+import android.content.Context;
+import android.media.AudioFormat;
+import android.media.AudioManager;
+import android.media.AudioTrack;
+import android.os.Process;
+import android.util.Log;
+
+class WebRtcAudioTrack {
+  private static final boolean DEBUG = true;
+
+  private static final String TAG = "WebRtcAudioTrack";
+
+  // Default audio data format is PCM 16 bit per sample.
+  // Guaranteed to be supported by all devices.
+  private static final int BITS_PER_SAMPLE = 16;
+
+  // Requested size of each recorded buffer provided to the client.
+  private static final int CALLBACK_BUFFER_SIZE_MS = 10;
+
+  // Average number of callbacks per second.
+  private static final int BUFFERS_PER_SECOND = 1000 / CALLBACK_BUFFER_SIZE_MS;
+
+  private final Context context;
+  private final long nativeAudioTrack;
+  private final AudioManager audioManager;
+
+  private ByteBuffer byteBuffer;
+
+  private AudioTrack audioTrack = null;
+  private AudioTrackThread audioThread = null;
+
+  /**
+   * Audio thread which keeps calling AudioTrack.write() to stream audio.
+   * Data is periodically acquired from the native WebRTC layer using the
+   * nativeGetPlayoutData callback function.
+   * This thread uses a Process.THREAD_PRIORITY_URGENT_AUDIO priority.
+   */
+  private class AudioTrackThread extends Thread {
+    private volatile boolean keepAlive = true;
+
+    public AudioTrackThread(String name) {
+      super(name);
+    }
+
+    @Override
+    public void run() {
+      Process.setThreadPriority(Process.THREAD_PRIORITY_URGENT_AUDIO);
+      Logd("AudioTrackThread" + WebRtcAudioUtils.getThreadInfo());
+
+      try {
+        // In MODE_STREAM mode we can optionally prime the output buffer by
+        // writing up to bufferSizeInBytes (from constructor) before starting.
+        // This priming will avoid an immediate underrun, but is not required.
+        // TODO(henrika): initial tests have shown that priming is not required.
+        audioTrack.play();
+        assertTrue(audioTrack.getPlayState() == AudioTrack.PLAYSTATE_PLAYING);
+      } catch (IllegalStateException e) {
+          Loge("AudioTrack.play failed: " + e.getMessage());
+        return;
+      }
+
+      // Fixed size in bytes of each 10ms block of audio data that we ask for
+      // using callbacks to the native WebRTC client.
+      final int sizeInBytes = byteBuffer.capacity();
+
+      while (keepAlive) {
+        // Get 10ms of PCM data from the native WebRTC client. Audio data is
+        // written into the common ByteBuffer using the address that was
+        // cached at construction.
+        nativeGetPlayoutData(sizeInBytes, nativeAudioTrack);
+        // Write data until all data has been written to the audio sink.
+        // Upon return, the buffer position will have been advanced to reflect
+        // the amount of data that was successfully written to the AudioTrack.
+        assertTrue(sizeInBytes <= byteBuffer.remaining());
+        int bytesWritten = 0;
+        if (WebRtcAudioUtils.runningOnLollipopOrHigher()) {
+          bytesWritten = audioTrack.write(byteBuffer,
+                                          sizeInBytes,
+                                          AudioTrack.WRITE_BLOCKING);
+        } else {
+          bytesWritten = audioTrack.write(byteBuffer.array(),
+                                          byteBuffer.arrayOffset(),
+                                          sizeInBytes);
+        }
+        if (bytesWritten != sizeInBytes) {
+          Loge("AudioTrack.write failed: " + bytesWritten);
+          if (bytesWritten == AudioTrack.ERROR_INVALID_OPERATION) {
+            keepAlive = false;
+          }
+        }
+        // The byte buffer must be rewinded since byteBuffer.position() is
+        // increased at each call to AudioTrack.write(). If we don't do this,
+        // next call to AudioTrack.write() will fail.
+        byteBuffer.rewind();
+
+        // TODO(henrika): it is possible to create a delay estimate here by
+        // counting number of written frames and subtracting the result from
+        // audioTrack.getPlaybackHeadPosition().
+      }
+
+      try {
+        audioTrack.stop();
+      } catch (IllegalStateException e) {
+        Loge("AudioTrack.stop failed: " + e.getMessage());
+      }
+      assertTrue(audioTrack.getPlayState() == AudioTrack.PLAYSTATE_STOPPED);
+      audioTrack.flush();
+    }
+
+    public void joinThread() {
+      keepAlive = false;
+      while (isAlive()) {
+        try {
+          join();
+        } catch (InterruptedException e) {
+          // Ignore.
+        }
+      }
+    }
+  }
+
+  WebRtcAudioTrack(Context context, long nativeAudioTrack) {
+    Logd("ctor" + WebRtcAudioUtils.getThreadInfo());
+    this.context = context;
+    this.nativeAudioTrack = nativeAudioTrack;
+    audioManager = (AudioManager) context.getSystemService(
+        Context.AUDIO_SERVICE);
+    if (DEBUG) {
+      WebRtcAudioUtils.logDeviceInfo(TAG);
+    }
+  }
+
+  private void InitPlayout(int sampleRate, int channels) {
+    Logd("InitPlayout(sampleRate=" + sampleRate + ", channels=" +
+         channels + ")");
+    final int bytesPerFrame = channels * (BITS_PER_SAMPLE / 8);
+    byteBuffer = byteBuffer.allocateDirect(
+        bytesPerFrame * (sampleRate / BUFFERS_PER_SECOND));
+    Logd("byteBuffer.capacity: " + byteBuffer.capacity());
+    // Rather than passing the ByteBuffer with every callback (requiring
+    // the potentially expensive GetDirectBufferAddress) we simply have the
+    // the native class cache the address to the memory once.
+    nativeCacheDirectBufferAddress(byteBuffer, nativeAudioTrack);
+
+    // Get the minimum buffer size required for the successful creation of an
+    // AudioTrack object to be created in the MODE_STREAM mode.
+    // Note that this size doesn't guarantee a smooth playback under load.
+    // TODO(henrika): should we extend the buffer size to avoid glitches?
+    final int minBufferSizeInBytes = AudioTrack.getMinBufferSize(
+        sampleRate,
+        AudioFormat.CHANNEL_OUT_MONO,
+        AudioFormat.ENCODING_PCM_16BIT);
+    Logd("AudioTrack.getMinBufferSize: " + minBufferSizeInBytes);
+    assertTrue(audioTrack == null);
+
+    // For the streaming mode, data must be written to the audio sink in
+    // chunks of size (given by byteBuffer.capacity()) less than or equal
+    // to the total buffer size |minBufferSizeInBytes|.
+    assertTrue(byteBuffer.capacity() < minBufferSizeInBytes);
+    try {
+      // Create an AudioTrack object and initialize its associated audio buffer.
+      // The size of this buffer determines how long an AudioTrack can play
+      // before running out of data.
+      audioTrack = new AudioTrack(AudioManager.STREAM_VOICE_CALL,
+                                  sampleRate,
+                                  AudioFormat.CHANNEL_OUT_MONO,
+                                  AudioFormat.ENCODING_PCM_16BIT,
+                                  minBufferSizeInBytes,
+                                  AudioTrack.MODE_STREAM);
+    } catch (IllegalArgumentException e) {
+      Logd(e.getMessage());
+      return;
+    }
+    assertTrue(audioTrack.getState() == AudioTrack.STATE_INITIALIZED);
+    assertTrue(audioTrack.getPlayState() == AudioTrack.PLAYSTATE_STOPPED);
+    assertTrue(audioTrack.getStreamType() == AudioManager.STREAM_VOICE_CALL);
+  }
+
+  private boolean StartPlayout() {
+    Logd("StartPlayout");
+    assertTrue(audioTrack != null);
+    assertTrue(audioThread == null);
+    audioThread = new AudioTrackThread("AudioTrackJavaThread");
+    audioThread.start();
+    return true;
+  }
+
+  private boolean StopPlayout() {
+    Logd("StopPlayout");
+    assertTrue(audioThread != null);
+    audioThread.joinThread();
+    audioThread = null;
+    if (audioTrack != null) {
+      audioTrack.release();
+      audioTrack = null;
+    }
+    return true;
+  }
+
+  /** Get max possible volume index for a phone call audio stream. */
+  private int GetStreamMaxVolume() {
+    Logd("GetStreamMaxVolume");
+    assertTrue(audioManager != null);
+    return audioManager.getStreamMaxVolume(AudioManager.STREAM_VOICE_CALL);
+  }
+
+  /** Set current volume level for a phone call audio stream. */
+  private boolean SetStreamVolume(int volume) {
+    Logd("SetStreamVolume(" + volume + ")");
+    assertTrue(audioManager != null);
+    if (WebRtcAudioUtils.runningOnLollipopOrHigher()) {
+      if (audioManager.isVolumeFixed()) {
+        Loge("The device implements a fixed volume policy.");
+        return false;
+      }
+    }
+    audioManager.setStreamVolume(AudioManager.STREAM_VOICE_CALL, volume, 0);
+    return true;
+  }
+
+  /** Get current volume level for a phone call audio stream. */
+  private int GetStreamVolume() {
+    Logd("GetStreamVolume");
+    assertTrue(audioManager != null);
+    return audioManager.getStreamVolume(AudioManager.STREAM_VOICE_CALL);
+  }
+
+  /** Helper method which throws an exception  when an assertion has failed. */
+  private static void assertTrue(boolean condition) {
+    if (!condition) {
+      throw new AssertionError("Expected condition to be true");
+    }
+  }
+
+  private static void Logd(String msg) {
+    Log.d(TAG, msg);
+  }
+
+  private static void Loge(String msg) {
+    Log.e(TAG, msg);
+  }
+
+  private native void nativeCacheDirectBufferAddress(
+      ByteBuffer byteBuffer, long nativeAudioRecord);
+
+  private native void nativeGetPlayoutData(int bytes, long nativeAudioRecord);
+}
diff --git a/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioUtils.java b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioUtils.java
new file mode 100644
index 0000000..9aff519
--- /dev/null
+++ b/librtc/src/main/java/org/webrtc/voiceengine/WebRtcAudioUtils.java
@@ -0,0 +1,130 @@
+/*
+ *  Copyright (c) 2015 The WebRTC project authors. All Rights Reserved.
+ *
+ *  Use of this source code is governed by a BSD-style license
+ *  that can be found in the LICENSE file in the root of the source
+ *  tree. An additional intellectual property rights grant can be found
+ *  in the file PATENTS.  All contributing project authors may
+ *  be found in the AUTHORS file in the root of the source tree.
+ */
+
+package org.webrtc.voiceengine;
+
+import android.content.Context;
+import android.content.pm.PackageManager;
+import android.media.audiofx.AcousticEchoCanceler;
+import android.media.audiofx.AudioEffect;
+import android.media.audiofx.AudioEffect.Descriptor;
+import android.media.AudioManager;
+import android.os.Build;
+import android.os.Process;
+import android.util.Log;
+
+import java.lang.Thread;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+public final class WebRtcAudioUtils {
+  // List of devices where it has been verified that the built-in AEC performs
+  // bad and where it makes sense to avoid using it and instead rely on the
+  // native WebRTC AEC instead. The device name is given by Build.MODEL.
+  private static final String[] BLACKLISTED_AEC_MODELS = new String[] {
+      "Nexus 5", // Nexus 5
+      "D6503",   // Sony Xperia Z2 D6503
+  };
+
+  // List of devices where we have seen issues (e.g. bad audio quality) using
+  // the low latency ouput mode in combination with OpenSL ES.
+  // The device name is given by Build.MODEL.
+  private static final String[] BLACKLISTED_OPEN_SL_ES_MODELS = new String[] {
+      "Nexus 6", // Nexus 6
+  };
+
+  // Use 44.1kHz as the default sampling rate.
+  private static final int SAMPLE_RATE_HZ = 44100;
+
+  public static boolean runningOnGingerBreadOrHigher() {
+    // November 2010: Android 2.3, API Level 9.
+    return Build.VERSION.SDK_INT >= Build.VERSION_CODES.GINGERBREAD;
+  }
+
+  public static boolean runningOnJellyBeanOrHigher() {
+    // June 2012: Android 4.1. API Level 16.
+    return Build.VERSION.SDK_INT >= Build.VERSION_CODES.JELLY_BEAN;
+  }
+
+  public static boolean runningOnJellyBeanMR1OrHigher() {
+    // November 2012: Android 4.2. API Level 17.
+    return Build.VERSION.SDK_INT >= Build.VERSION_CODES.JELLY_BEAN_MR1;
+  }
+
+  public static boolean runningOnLollipopOrHigher() {
+    // API Level 21.
+    return Build.VERSION.SDK_INT >= Build.VERSION_CODES.LOLLIPOP;
+  }
+
+  // Helper method for building a string of thread information.
+  public static String getThreadInfo() {
+    return "@[name=" + Thread.currentThread().getName()
+        + ", id=" + Thread.currentThread().getId() + "]";
+  }
+
+  // Returns true if we're running on emulator.
+  public static boolean runningOnEmulator() {
+    return Build.HARDWARE.equals("goldfish") &&
+        Build.BRAND.startsWith("generic_");
+  }
+
+  // Returns true if the device is blacklisted for HW AEC usage.
+  public static boolean deviceIsBlacklistedForHwAecUsage() {
+    List<String> blackListedModels = Arrays.asList(BLACKLISTED_AEC_MODELS);
+    return blackListedModels.contains(Build.MODEL);
+  }
+
+  // Returns true if the device is blacklisted for OpenSL ES usage.
+  public static boolean deviceIsBlacklistedForOpenSLESUsage() {
+    List<String> blackListedModels =
+        Arrays.asList(BLACKLISTED_OPEN_SL_ES_MODELS);
+    return blackListedModels.contains(Build.MODEL);
+  }
+
+  // Returns true if the device supports Acoustic Echo Canceler (AEC).
+  public static boolean isAcousticEchoCancelerSupported() {
+    // AcousticEchoCanceler was added in API level 16 (Jelly Bean).
+    if (!WebRtcAudioUtils.runningOnJellyBeanOrHigher()) {
+      return false;
+    }
+    // Check if the device implements acoustic echo cancellation.
+    return AcousticEchoCanceler.isAvailable();
+  }
+
+  // Returns true if the device supports AEC and it not blacklisted.
+  public static boolean isAcousticEchoCancelerApproved() {
+    if (deviceIsBlacklistedForHwAecUsage())
+      return false;
+    return isAcousticEchoCancelerSupported();
+  }
+
+  // Information about the current build, taken from system properties.
+  public static void logDeviceInfo(String tag) {
+    Log.d(tag, "Android SDK: " + Build.VERSION.SDK_INT + ", "
+        + "Release: " + Build.VERSION.RELEASE + ", "
+        + "Brand: " + Build.BRAND + ", "
+        + "Device: " + Build.DEVICE + ", "
+        + "Id: " + Build.ID + ", "
+        + "Hardware: " + Build.HARDWARE + ", "
+        + "Manufacturer: " + Build.MANUFACTURER + ", "
+        + "Model: " + Build.MODEL + ", "
+        + "Product: " + Build.PRODUCT);
+  }
+
+  // Checks if the process has as specified permission or not.
+  public static boolean hasPermission(Context context, String permission) {
+    return context.checkPermission(
+        permission,
+        Process.myPid(),
+        Process.myUid()) == PackageManager.PERMISSION_GRANTED;
+    }
+}
diff --git a/librtc/src/main/jniLibs/armeabi-v7a/libjingle_peerconnection_so.so b/librtc/src/main/jniLibs/armeabi-v7a/libjingle_peerconnection_so.so
new file mode 100755
index 0000000..a3af0e1
Binary files /dev/null and b/librtc/src/main/jniLibs/armeabi-v7a/libjingle_peerconnection_so.so differ
diff --git a/librtc/src/main/res/values/strings.xml b/librtc/src/main/res/values/strings.xml
new file mode 100644
index 0000000..6c4fb74
--- /dev/null
+++ b/librtc/src/main/res/values/strings.xml
@@ -0,0 +1,3 @@
+<resources>
+    <string name="app_name">LibRTC</string>
+</resources>
diff --git a/settings.gradle b/settings.gradle
index d2197f4..ab42bb1 100644
--- a/settings.gradle
+++ b/settings.gradle
@@ -1 +1 @@
-include ':app', ':webrtc-client'
+include ':app', ':webrtc-client', ':librtc'
diff --git a/webrtc-client/build.gradle b/webrtc-client/build.gradle
index cbcd191..9c0f5b3 100644
--- a/webrtc-client/build.gradle
+++ b/webrtc-client/build.gradle
@@ -3,7 +3,7 @@ apply plugin: 'com.android.library'
 
 android {
     compileSdkVersion 22
-    buildToolsVersion "22.0.0"
+    buildToolsVersion "22.0.1"
 
     defaultConfig {
         minSdkVersion 15
@@ -21,5 +21,6 @@ android {
 
 dependencies {
     compile 'com.github.nkzawa:socket.io-client:0.4.2'
-    compile 'io.pristine:libjingle:9456@aar'
+    //compile 'io.pristine:libjingle:9456@aar'
+    compile project(':librtc')
 }
diff --git a/webrtc-client/src/main/java/fr/pchab/webrtcclient/WebRtcClient.java b/webrtc-client/src/main/java/fr/pchab/webrtcclient/WebRtcClient.java
index 9dad470..8bd1938 100644
--- a/webrtc-client/src/main/java/fr/pchab/webrtcclient/WebRtcClient.java
+++ b/webrtc-client/src/main/java/fr/pchab/webrtcclient/WebRtcClient.java
@@ -245,6 +245,11 @@ public void onSetFailure(String s) {}
         @Override
         public void onSignalingChange(PeerConnection.SignalingState signalingState) {}
 
+        @Override
+        public void onIceConnectionReceivingChange(boolean b) {
+            //TODO provide implementation for this new API
+        }
+
         @Override
         public void onIceConnectionChange(PeerConnection.IceConnectionState iceConnectionState) {
             if(iceConnectionState == PeerConnection.IceConnectionState.DISCONNECTED) {
